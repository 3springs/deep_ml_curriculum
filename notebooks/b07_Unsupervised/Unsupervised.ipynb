{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will learn about unsupervised methods for clustering and dimensionality reduction.\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "- [0. Packages](#0)\n",
    "- [1. Unsupervised Leearning](#1)\n",
    "- [2. Dataset](#2)\n",
    "- [3. Dimensionality Reduction](#3)\n",
    "    - [3.1 PCA](#3-1)\n",
    "    - [3.2 LDA](#3-2)\n",
    "    - [3.3 NMF](#3-3)\n",
    "    - [3.4 Kernel PCA](#3-4)\n",
    "    - [3.5 tSNE](#3-5)\n",
    "    - [3.6 ISOMAP](#3-6)\n",
    "    - [3.7 UMAP](#3-7)\n",
    "- [4. Clustering](#4)\n",
    "    - [4.1. KMeans](#4-1)\n",
    "    - [4.2. DBScan](#4-2)\n",
    "    - [4.3. Hierarchical Clustering](#4-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Packages <a name=\"0\"></a>\n",
    "\n",
    "In this session, we will make use of the following packages:\n",
    "- [PathLib](https://docs.python.org/3/library/pathlib.html) is a convenient library to work with path names.\n",
    "- [NumPy](https://docs.scipy.org/doc/numpy/) is a popular library for scientific computing.\n",
    "- [matplotlib](https://matplotlib.org/3.1.1/contents.html) is a plotting library compatible with numpy.\n",
    "- [pandas](https://pandas.pydata.org/docs/) is what we'll use to manipulate our data.\n",
    "- [sklearn](https://scikit-learn.org/stable/index.html) will be used to measure the performance of our model.\n",
    "\n",
    "Run the next cell to import the necessary packages mentioned before. Besides, we will add more packages as needed while progressing in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good practice to use short but clear aliases for the imported libraries\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# For 3d plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# Magic Function\n",
    "%matplotlib inline\n",
    "# Hide all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1. Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is one of the types of machine learning techniques where the aim is to find patterns in the data without any pre-existent labels (unlike supervised learning).\n",
    "\n",
    "\n",
    "Unsupervised learning is used for two main task:\n",
    "\n",
    "- Dimensionality Reduction: Reduce the number of features trying to retain as much information possible.\n",
    "- Cluster Analysis: Cluster data with similar features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 Dataset\n",
    "\n",
    "In this session, we will be using the GeoLink dataset that used in previous sessions.\n",
    "\n",
    "**Note:** download data from https://drive.google.com/drive/folders/1EgDN57LDuvlZAwr5-eHWB5CTJ7K9HpDP\n",
    "\n",
    "Credit to this repo: https://github.com/LukasMosser/geolink_dataset\n",
    "\n",
    "## Data Disclaimer\n",
    "\n",
    "All the data serving as an input to these notebooks was generously donated by GEOLINK  \n",
    "and is CC-by-SA 4.0 \n",
    "\n",
    "If you use this data please reference the dataset properly to give them credit for their contribution.\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "The geolink dataset we will use in this notebook has been preprocessed. You can find the process of preparation of this dataset in <code>notebook/00 Data Prep/00-mc-prep_geolink_norge_dataset.ipynb</code>\n",
    "\n",
    "## Load Dataset\n",
    "\n",
    "Let's load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_locations = Path(\"../../data/processed/geolink_norge_dataset/\")\n",
    "# Load processed dataset\n",
    "geolink = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    ").set_index([\"Well\", \"DEPT\"])\n",
    "geolink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values for Lithology\n",
    "geolink[\"LITHOLOGY_GEOLINK\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geolink[\"LITHOLOGY_GEOLINK\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all columns\n",
    "print(geolink.columns)\n",
    "# See all columns from CALI\n",
    "print(geolink.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are not missing data in the dataset\n",
    "geolink.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "# 3. Dimensionality Reduction\n",
    "\n",
    "Before talking about dimensionality reduction, it is important to understand the concept of the `curse of dimensionality`. In machine learning, many techniques work by detecting objects or groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data algorithms from being efficient. This phenomenon does not occur in low-dimensional spaces.\n",
    "\n",
    "Hence, dimensionality reduction is an important task in machine learning and can be achieve using many unsupervised learning techniques. We will discuss some of them in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Source: [Kaggle](https://www.kaggle.com/arthurtok/principal-component-analysis-with-kmeans-visuals) License: [Apache 2.0]('http://www.apache.org/licenses/LICENSE-2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-1\"></a>\n",
    "## 3.1 Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) identifies the combination of components (directions in the feature space) that account for the most variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "\n",
    "**NOTE:** <br>\n",
    "\n",
    "PCA has a lot of applications in statistics and machine learning, but one of the most popular ones is for data visualisation.\n",
    "<br/><br/>\n",
    "For Machine Learning applications, it is useful to visualise the data. You can use PCA to reduce any dimensions into 2 or 3 dimensions so that you can plot and hopefully understand the data better.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolink.reset_index()[\"Well\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise the Data\n",
    "PCA is affected by scale so we need to scale the features in our data before applying PCA. In this session, we will use `StandardScaler` to help us standardise the dataset’s features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# In our case, we will take a sample of 1,000 data points from one well\n",
    "#'16_7-4'\n",
    "# 16_9-1\n",
    "# '33_9-11'\n",
    "sample_dataset = geolink.xs(\"15_9-12\").sample(n=1000, replace=False, random_state=2020)\n",
    "# Separating features\n",
    "X = sample_dataset[list(sample_dataset.columns[1:])]\n",
    "# Separating target\n",
    "y = sample_dataset[[\"LITHOLOGY_GEOLINK\"]]\n",
    "\n",
    "# Standardizing the features\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Projection\n",
    "\n",
    "Our data has originally 6 features. In code below, we will project the original data which has 6 dimensions to 2 dimensions.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "\n",
    "**NOTE:** <br>\n",
    "It should be noted that after dimensionality reduction, there is not usually  a particular meaning assigned to each principal component. The new components are just the two main dimensions of variation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the PCA module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We will use 2 components so we can later use a 2d plot.\n",
    "pca = PCA(n_components=2)\n",
    "# We are reducing the dimensions in this step from 6 features to only 2\n",
    "pca_X = pca.fit_transform(X)\n",
    "pca_df = pd.DataFrame(data=pca_X, columns=[\"PCA 1\", \"PCA 2\"])\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two columns\n",
    "final_df = pd.concat([pca_df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column target\n",
    "final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lithologies in our dataset\n",
    "targets = list(final_df[\"target\"].unique())\n",
    "print(targets)\n",
    "print(\"Total:\", len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    \"r\",\n",
    "    \"g\",\n",
    "    \"b\",\n",
    "    \"aqua\",\n",
    "    \"chartreuse\",\n",
    "    \"indigo\",\n",
    "    \"goldenrod\",\n",
    "    \"lightblue\",\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"Principal Component 1\", fontsize=15)\n",
    "ax.set_ylabel(\"Principal Component 2\", fontsize=15)\n",
    "ax.set_title(\"2 component PCA\", fontsize=20)\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = final_df[\"target\"] == target\n",
    "    ax.scatter(\n",
    "        final_df.loc[indicesToKeep, \"PCA 1\"],\n",
    "        final_df.loc[indicesToKeep, \"PCA 2\"],\n",
    "        c=color,\n",
    "        s=50,\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-2\"></a>\n",
    "## 3.2 Linear Discriminar Analysis (LDA)\n",
    "\n",
    "Linear discriminant analysis (LDA) finds a linear combination of features that separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for **dimensionality reduction before later classification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda_X = lda.fit(X, y).transform(X)\n",
    "\n",
    "\n",
    "lda_df = pd.DataFrame(data=lda_X, columns=[\"LDA 1\", \"LDA 2\"])\n",
    "# Concatenate two dataframes\n",
    "final_df = pd.concat([lda_df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "# Rename column\n",
    "final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "# List of lithologies in our dataset\n",
    "targets = list(final_df[\"target\"].unique())\n",
    "\n",
    "# List of colors for the plot\n",
    "colors = [\n",
    "    \"r\",\n",
    "    \"g\",\n",
    "    \"b\",\n",
    "    \"aqua\",\n",
    "    \"chartreuse\",\n",
    "    \"indigo\",\n",
    "    \"goldenrod\",\n",
    "    \"lightblue\",\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "]\n",
    "\n",
    "\n",
    "# Plot Images\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"LDA 1\", fontsize=15)\n",
    "ax.set_ylabel(\"LDA 2\", fontsize=15)\n",
    "ax.set_title(\"2 component LDA\", fontsize=20)\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = final_df[\"target\"] == target\n",
    "    ax.scatter(\n",
    "        final_df.loc[indicesToKeep, \"LDA 1\"],\n",
    "        final_df.loc[indicesToKeep, \"LDA 2\"],\n",
    "        c=color,\n",
    "        s=50,\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-3\"></a>\n",
    "## 3.3 Non-Negative Matrix Factorization (NMF)\n",
    "Source: [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)\n",
    "\n",
    "Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.max())\n",
    "print(X.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use NMF, our data cannot contain negative values. For that reason, we wil use `MinMaxScaler` from sklearn which scales the data in a given range. For example, range (0,1).\n",
    "\n",
    "`MinMaxScaler` is equivalent to the code below:\n",
    "\n",
    "```python\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X)\n",
    "X_transformed = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_transformed.max())\n",
    "print(X_transformed.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=2, init=\"random\", random_state=2020)\n",
    "nmf_X = nmf.fit_transform(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_df = pd.DataFrame(data=nmf_X, columns=[\"NMF 1\", \"NMF 2\"])\n",
    "# Concatenate two dataframes\n",
    "final_df = pd.concat([nmf_df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "# Rename column\n",
    "final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "# List of lithologies in our dataset\n",
    "targets = list(final_df[\"target\"].unique())\n",
    "\n",
    "# List of colors for the plot\n",
    "colors = [\n",
    "    \"r\",\n",
    "    \"g\",\n",
    "    \"b\",\n",
    "    \"aqua\",\n",
    "    \"chartreuse\",\n",
    "    \"indigo\",\n",
    "    \"goldenrod\",\n",
    "    \"lightblue\",\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "]\n",
    "\n",
    "\n",
    "# Plot Images\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"NMF 1\", fontsize=15)\n",
    "ax.set_ylabel(\"NMF 2\", fontsize=15)\n",
    "ax.set_title(\"2 component NMF\", fontsize=20)\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = final_df[\"target\"] == target\n",
    "    ax.scatter(\n",
    "        final_df.loc[indicesToKeep, \"NMF 1\"],\n",
    "        final_df.loc[indicesToKeep, \"NMF 2\"],\n",
    "        c=color,\n",
    "        s=30,\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-4\"></a>\n",
    "## 3.4 Kernel PCA\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)\n",
    "> In the field of multivariate statistics, kernel principal component analysis (kernel PCA) is an extension of principal component analysis (PCA) using techniques of kernel methods.\n",
    "\n",
    "\n",
    "Source: [Sklearn](https://scikit-learn.org/stable/modules/metrics.html#metrics)\n",
    "\n",
    "**Kernels**:\n",
    "> Kernels are measures of similarity, i.e. `s(a, b) > s(a, c)` if objects `a` and `b` are considered “more similar” than objects `a` and `c`. A kernel must also be positive semi-definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "\n",
    "def kernel_pca(X, kernel):\n",
    "    transformer = KernelPCA(n_components=2, kernel=kernel)\n",
    "    X_transformed = transformer.fit_transform(X)\n",
    "\n",
    "    df = pd.DataFrame(data=X_transformed, columns=[\"K_PCA 1\", \"K_PCA 2\"])\n",
    "    # Concatenate two dataframes\n",
    "    final_df = pd.concat([df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "    # Rename column\n",
    "    final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "    # List of lithologies in our dataset\n",
    "    targets = list(final_df[\"target\"].unique())\n",
    "\n",
    "    # List of colors for the plot\n",
    "    colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#2ca02c\",\n",
    "        \"#d62728\",\n",
    "        \"#9467bd\",\n",
    "        \"#8c564b\",\n",
    "        \"#e377c2\",\n",
    "        \"#7f7f7f\",\n",
    "        \"#bcbd22\",\n",
    "        \"#17becf\",\n",
    "    ]\n",
    "\n",
    "    # Plot Images\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_xlabel(\"K_PCA 1\", fontsize=15)\n",
    "    ax.set_ylabel(\"K_PCA 2\", fontsize=15)\n",
    "    ax.set_title('2 component KPCA using kernel={}'.format(kernel), fontsize=20)\n",
    "    for target, color in zip(targets, colors):\n",
    "        indicesToKeep = final_df[\"target\"] == target\n",
    "        ax.scatter(\n",
    "            final_df.loc[indicesToKeep, \"K_PCA 1\"],\n",
    "            final_df.loc[indicesToKeep, \"K_PCA 2\"],\n",
    "            c=color,\n",
    "            s=30,\n",
    "        )\n",
    "    ax.legend(targets)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the dimensions of the data using different kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"rbf\", \"poly\", \"sigmoid\", \"cosine\"]\n",
    "\n",
    "for kernel in kernels:\n",
    "    kernel_pca(X, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-5\"></a>\n",
    "## 3.5 tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised learning developed specificaly for data visualization. This tecnique embedded high-dimensional data in a low-dimensional space of two or three dimensions. Therefore, the embedding or representation of the manifold can be visualize in 2 or 3 dimensions.\n",
    "\n",
    "Let's try something similar to what we did before using now the tSNE technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_X = TSNE(n_components=2).fit_transform(X)\n",
    "tsne_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnse_df = pd.DataFrame(data=tsne_X, columns=[\"TNSE 1\", \"TNSE 2\"])\n",
    "tnse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.reset_index()[\"LITHOLOGY_GEOLINK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat two columns\n",
    "final_df = pd.concat([tnse_df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "# Rename column target\n",
    "final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"TNSE 1\", fontsize=15)\n",
    "ax.set_ylabel(\"TNSE 2\", fontsize=15)\n",
    "ax.set_title(\"2d TNSE\", fontsize=20)\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = final_df[\"target\"] == target\n",
    "    ax.scatter(\n",
    "        final_df.loc[indicesToKeep, \"TNSE 1\"],\n",
    "        final_df.loc[indicesToKeep, \"TNSE 2\"],\n",
    "        c=color,\n",
    "        s=50,\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-6\"></a>\n",
    "## 3.6 ISOMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isomap is a popular technique for dimensionality reduction. Isomap is a highly efficient algorithm and it can be generally applied to a variety of data sources and dimensionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "isomap = Isomap(n_components=2)\n",
    "isomap_X = isomap.fit_transform(X)\n",
    "\n",
    "# Isomap Dataframe\n",
    "isomap_df = pd.DataFrame(data=isomap_X, columns=[\"ISOMAP 1\", \"ISOMAP 2\"])\n",
    "# Concat two columns\n",
    "final_df = pd.concat([isomap_df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "# Rename column target\n",
    "final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"ISOMAP 1\", fontsize=15)\n",
    "ax.set_ylabel(\"ISOMAP 2\", fontsize=15)\n",
    "ax.set_title(\"2d ISOMAP\", fontsize=20)\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = final_df[\"target\"] == target\n",
    "    ax.scatter(\n",
    "        final_df.loc[indicesToKeep, \"ISOMAP 1\"],\n",
    "        final_df.loc[indicesToKeep, \"ISOMAP 2\"],\n",
    "        c=color,\n",
    "        s=50,\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "\n",
    "**Exercise 1:** <br>\n",
    "Plot a 2d PCA, Kernel PCA, tsne and ISOMAP for a different well log. Compare and analyse which method worked better for the specific well log selected.\n",
    "    \n",
    "**Exercise 2 (Optional):** <br>\n",
    "Use a different dataset such us the `Titanic` dataset or `iris` dataset and apply different techniques of dimensionality reduction to visualise 2d plots of the data points.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 UMAP\n",
    "\n",
    "From the [offcial documentation](https://umap-learn.readthedocs.io/en/latest/):\n",
    "\n",
    "> Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data\n",
    "- The data is uniformly distributed on Riemannian manifold\n",
    "- The Riemannian metric is locally constant (or can be approximated as such)\n",
    "- The manifold is locally connected. <br/><br/>\n",
    "From these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import umap\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=2020)\n",
    "umap_X = isomap.fit_transform(X)\n",
    "\n",
    "# Isomap Dataframe\n",
    "umap_df = pd.DataFrame(data=umap_X, columns=[\"UMAP 1\", \"UMAP 2\"])\n",
    "# Concat two columns\n",
    "final_df = pd.concat([umap_df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "# Rename column target\n",
    "final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"UMAP 1\", fontsize=15)\n",
    "ax.set_ylabel(\"UMAP 2\", fontsize=15)\n",
    "ax.set_title(\"2d UMAP\", fontsize=20)\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = final_df[\"target\"] == target\n",
    "    ax.scatter(\n",
    "        final_df.loc[indicesToKeep, \"UMAP 1\"],\n",
    "        final_df.loc[indicesToKeep, \"UMAP 2\"],\n",
    "        c=color,\n",
    "        s=50,\n",
    "    )\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a name=\"4\"></a>\n",
    "# 4. Clustering\n",
    "\n",
    "> Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Cluster analysis itself is not one specific algorithm, but the general task to be solved...\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "\n",
    "**NOTE:** <br>\n",
    "Different clustering algorithms have different properties. The example below (Source: [Sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)) shows characteristics of different clustering algorithms on datasets that are “interesting” but still in 2D. With the exception of the last dataset, the parameters of each of these dataset-algorithm pairs has been tuned to produce good clustering results. Some algorithms are more sensitive to parameter values than others.\n",
    "    \n",
    "**Warning:** While these examples give some intuition about the algorithms, this intuition might not apply to very high dimensional data.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png' width=800 heigh=800/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore many different clustering algorithm. They usually have a very similar API in `sklearn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4-1\"></a>\n",
    "## 4.1 KMeans\n",
    "\n",
    "Kmeans is a clustering algorithm where given a k number of clusters, it tries to find the centroid with the nearest mean.\n",
    "\n",
    "In the example below, we have some data in a 2d space with 3 clusters. The algorithm will find the best centroids for the 3 clusters in a iterative process:\n",
    "\n",
    "![kmeans-convergence](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif)\n",
    "\n",
    "[Image Source](https://en.wikipedia.org/wiki/K-means_clustering#/media/File:K-means_convergence.gif) License Image: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to choose the right number of clusters?**\n",
    "\n",
    "We will use the **Elbow Method**:\n",
    "\n",
    "Using the \"elbow\" as a cutoff point is a common heuristic to choose a number of cluster where that adding another cluster doesn't give much better modeling of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the dimensions first using TSNE and then we will apply the Kmeans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_X = TSNE(n_components=2).fit_transform(X)\n",
    "tnse_df = pd.DataFrame(data=tsne_X, columns=[\"TNSE 1\", \"TNSE 2\"])\n",
    "# Concat two columns\n",
    "final_df = pd.concat([tnse_df, y.reset_index()[\"LITHOLOGY_GEOLINK\"]], axis=1)\n",
    "# Rename column target\n",
    "final_df = final_df.rename(columns={\"LITHOLOGY_GEOLINK\": \"target\"})\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Manifold\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(final_df[\"TNSE 1\"], final_df[\"TNSE 2\"], s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia = []\n",
    "# we will say that the number of clusters is 10 (total number of different lithologies)\n",
    "### Static code to get max no of clusters\n",
    "for i in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=i, init=\"k-means++\", random_state=2020)\n",
    "    kmeans.fit(final_df[[\"TNSE 1\", \"TNSE 2\"]])\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    # inertia_ is the formula used to segregate the data points into clusters\n",
    "\n",
    "# Visualizing the ELBOW method to get the optimal value of K\n",
    "plt.plot(range(1, 10), inertia)\n",
    "plt.title(\"The Elbow Method\")\n",
    "plt.xlabel(\"no of clusters\")\n",
    "plt.ylabel(\"wcss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Build\n",
    "kmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=2020)\n",
    "y_kmeans = kmeans.fit_predict(final_df[[\"TNSE 2\", \"TNSE 1\"]])\n",
    "# Plot the different clusters\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(final_df[\"TNSE 1\"], final_df[\"TNSE 2\"], c=y_kmeans, s=10, cmap=\"viridis\")\n",
    "centers = kmeans.cluster_centers_\n",
    "# Plot the centers\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c=\"gray\", s=200, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4-2\"></a>\n",
    "## 4.2 DBScan\n",
    "\n",
    "Density-based spatial clustering of applications with noise (DBSCAN) is a very popular clustering algorithm, which has been reconognized both in the academy and industry as one of the most used methods for industrial applications and most cited clustering algorithms in scientific literature.\n",
    "\n",
    "One of the reasons that makes DBSCAN a great method for clustering is that also helps to estimate the optimal number of cluster in the data.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/1920px-DBSCAN-density-data.svg.png' width=400 heigh=400>\n",
    "\n",
    "License Image: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)\n",
    "\n",
    "\n",
    "For the next example we will use the sample data. Source: [Sklearn](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Helpers to generate data around the centers\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(\n",
    "    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n",
    ")\n",
    "\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute DBScan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBScan assigns a cluster (positive numeric) label to each data point and -1 if it is considered noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBScan will help us to determine the number of clusters in our data manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=14,\n",
    "    )\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=6,\n",
    "    )\n",
    "\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "\n",
    "**Exercise 3:** <br>\n",
    "1. Select a Well log from the geolink dataset.\n",
    "2. Apply DBScan to determine the ideal number of clusters\n",
    "3. Plot the clusters using the DBScan labels\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Hierarchical Clustering\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "\n",
    "> In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. \n",
    "- Agglomerative: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "- Divisive: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "Examples from: [Agglomerative Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering), [Feature Agglomerative](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration)\n",
    "\n",
    "\n",
    "**Example Agglomerative Clustering:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "clustering = AgglomerativeClustering().fit(X)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Feature Agglomerative:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, cluster\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "images = digits.images\n",
    "X = np.reshape(images, (len(images), -1))\n",
    "print(X.shape)\n",
    "agglo = cluster.FeatureAgglomeration(n_clusters=32)\n",
    "agglo.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = agglo.transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "\n",
    "**Exercise 4 (Optional):** <br>\n",
    "\n",
    "1. Apply Hierarchical Clustering to the `iris` or`geolink` dataset.\n",
    "2. Reduce the number of features to 2 and plot a 2-dimensional plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and further reading\n",
    "\n",
    "[Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning)\n",
    "\n",
    "[Dimensionality Reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)\n",
    "\n",
    "Clustering:\n",
    "- [K-means](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "- [DBScan](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "- [Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
