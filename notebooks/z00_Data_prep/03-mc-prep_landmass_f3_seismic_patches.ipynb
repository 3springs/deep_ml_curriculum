{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: https://dataunderground.org/dataset/landmass-f3\n",
    "- Credits to researchers at Georgia Tech, Agile Geoscience\n",
    "- License CCbySA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import torch\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = Path(\"../../data/raw/landmass-f3/\")\n",
    "data_out = Path(\"../../data/processed/landmass-f3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(data_in / \"Landmass_CC-BY-SA.h5\", \"r\") as f:\n",
    "    print(list(f.attrs))\n",
    "    print(list(f))\n",
    "    for k in [\"LANDMASS-1\", \"LANDMASS-2\"]:\n",
    "        print(k, list(f[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(data_in / \"Landmass_CC-BY-SA.h5\", \"r\")\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "for key in f[\"LANDMASS-1\"].keys():\n",
    "    d = f[\"LANDMASS-1\"][key][: nrows * ncols, :, :, 0]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(8, 4))\n",
    "\n",
    "    for i in range(ncols * nrows):\n",
    "\n",
    "        ax = axs[i // ncols][i % ncols]\n",
    "        ax.imshow(d[i])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    #     plt.tight_layout()\n",
    "    plt.suptitle('{}'.format(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(data_in / \"Landmass_CC-BY-SA.h5\", \"r\")\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "for key in f[\"LANDMASS-2\"].keys():\n",
    "    d = f[\"LANDMASS-2\"][key][: nrows * ncols, :, :, 0]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 3))\n",
    "\n",
    "    for i in range(ncols * nrows):\n",
    "\n",
    "        ax = axs[i // ncols][i % ncols]\n",
    "        ax.imshow(d[i])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    #     plt.tight_layout()\n",
    "    plt.suptitle('{}'.format(key))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(data_in / \"Landmass_CC-BY-SA.h5\", \"r\")\n",
    "classes = [\"Chaotic Horizon\", \"Fault\", \"Horizon\", \"Salt Dome\"]\n",
    "X = []\n",
    "Y = []\n",
    "for i, key in enumerate(tqdm(classes)):\n",
    "    x = np.array(f[\"LANDMASS-1\"][key].value)\n",
    "\n",
    "    # convert to uint8\n",
    "    x = (x + 1) / 2\n",
    "    x *= 255\n",
    "    x = x.astype(np.uint8)\n",
    "\n",
    "    y = np.ones(x.shape[0]) * i\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    print(i, key, x.shape, x.min(), x.max())\n",
    "\n",
    "X = np.concatenate(X, 0).astype(np.uint8)\n",
    "Y = np.concatenate(Y, 0).astype(np.uint8)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)\n",
    "\n",
    "with (data_out / \"LandmassF3Patches/processed/training.pt\").open(\"wb\") as fo:\n",
    "    torch.save((torch.from_numpy(X_train)[:, :, :, 0], torch.from_numpy(y_train)), fo)\n",
    "    print(fo)\n",
    "\n",
    "with (data_out / \"LandmassF3Patches/processed/test.pt\").open(\"wb\") as fo:\n",
    "    torch.save((torch.from_numpy(X_test)[:, :, :, 0], torch.from_numpy(y_test)), fo)\n",
    "    print(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.split(X_train, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)\n",
    "X_train = np.concatenate(\n",
    "    [\n",
    "        X_train[:, :26, :26, 0],\n",
    "        X_train[:, 26 * 1 : 26 * 2, 26 * 1 : 26 * 2, 0],\n",
    "        X_train[:, 26 * 2 : 26 * 3, 26 * 2 : 26 * 3, 0],\n",
    "    ]\n",
    ")\n",
    "y_train = np.concatenate([y_train, y_train, y_train])\n",
    "y_test = np.concatenate([y_test, y_test, y_test])\n",
    "X_test = np.concatenate(\n",
    "    [\n",
    "        X_test[:, :26, :26, 0],\n",
    "        X_test[:, 26 * 1 : 26 * 2, 26 * 1 : 26 * 2, 0],\n",
    "        X_test[:, 26 * 2 : 26 * 3, 26 * 2 : 26 * 3, 0],\n",
    "    ]\n",
    ")\n",
    "print(X_train.shape, y_train.shape)\n",
    "with (data_out / \"LandmassF3PatchesMini/processed/training.pt\").open(\"wb\") as fo:\n",
    "    torch.save((torch.from_numpy(X_train), torch.from_numpy(y_train)), fo)\n",
    "    print(fo)\n",
    "\n",
    "with (data_out / \"LandmassF3PatchesMini/processed/test.pt\").open(\"wb\") as fo:\n",
    "    torch.save((torch.from_numpy(X_test), torch.from_numpy(y_test)), fo)\n",
    "    print(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(data_out / \"LandmassF3PatchesMini/processed/training.pt\")[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "mnist = MNIST(\"/tmp/mnist\", download=True)\n",
    "x, y = mnist[5]\n",
    "np.array(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_ml_curriculum.data.landmass_f3 import LandmassF3Patches, LandmassF3PatchesMini\n",
    "from deep_ml_curriculum.config import project_dir\n",
    "\n",
    "\n",
    "landmassf3 = LandmassF3Patches(project_dir / \"data/processed/landmass-f3\")\n",
    "print(landmassf3)\n",
    "x, y = landmassf3[4]\n",
    "print(landmassf3.classes[y])\n",
    "print(np.array(x).shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING This may be too small to see patterns, which may lower accuracy a lot on an already hard task\n",
    "landmassf3mini = LandmassF3PatchesMini(project_dir / \"data/processed/landmass-f3\")\n",
    "print(landmassf3mini)\n",
    "x, y = landmassf3mini[40]\n",
    "print(landmassf3.classes[y])\n",
    "print(np.array(x).shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('jup3.7.3': venv)",
   "language": "python",
   "name": "python37364bitjup373venv303c3eb08efe4501baa24424ed3eb0f3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
