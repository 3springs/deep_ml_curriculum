{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from https://github.com/pangeo-data/WeatherBench/blob/master/README.md\n",
    "\n",
    "see https://github.com/pangeo-data/WeatherBench/blob/master/notebooks/3-cnn-example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = Path(\"../../data/raw/weatherbench/\")\n",
    "data_out = Path(\"../../data/processed/weatherbench/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(path, var, years=slice(\"2017\", \"2018\")):\n",
    "    \"\"\"\n",
    "    Load the test dataset. If z return z500, if t return t850.\n",
    "    Args:\n",
    "        path: Path to nc files\n",
    "        var: variable. Geopotential = 'z', Temperature = 't'\n",
    "        years: slice for time window\n",
    "    Returns:\n",
    "        dataset: Concatenated dataset for 2017 and 2018\n",
    "    \"\"\"\n",
    "    ds = xr.open_mfdataset('{}/*.nc'.format(path), combine=\"by_coords\")[var]\n",
    "    if var in [\"z\", \"t\"]:\n",
    "        try:\n",
    "            ds = ds.sel(level=500 if var == \"z\" else 850).drop(\"level\")\n",
    "        except ValueError:\n",
    "            ds = ds.drop(\"level\")\n",
    "    return ds.sel(time=years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldBench3D(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        var_dict,\n",
    "        lead_time,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        load=True,\n",
    "        mean=None,\n",
    "        std=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={\"level\": [1]}, dims=[\"level\"])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({\"level\": generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, \"level\").transpose(\"time\", \"lat\", \"lon\", \"level\")\n",
    "        self.mean = (\n",
    "            self.data.mean((\"time\", \"lat\", \"lon\")).compute() if mean is None else mean\n",
    "        )\n",
    "        self.std = (\n",
    "            self.data.std(\"time\").mean((\"lat\", \"lon\")).compute() if std is None else std\n",
    "        )\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load:\n",
    "            print(\"Loading data into RAM\")\n",
    "            self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the number of batches per epoch\"\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"Generate one batch of data\"\n",
    "        idxs = self.idxs[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"Updates indexes after each epoch\"\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "DATADIR = \"../../data/processed/weatherbench/5.625deg\"\n",
    "\n",
    "# Load the validation subset of the data: 2017 and 2018\n",
    "z500_valid = load_test_data('{}/geopotential_500'.format(DATADIR), \"z\")\n",
    "t850_valid = load_test_data('{}/temperature_850'.format(DATADIR), \"t\")\n",
    "valid = xr.merge([z500_valid, t850_valid])\n",
    "\n",
    "z = xr.open_mfdataset('{}/geopotential_500/*.nc'.format(DATADIR), combine=\"by_coords\")\n",
    "t = xr.open_mfdataset('{}/temperature_850/*.nc'.format(DATADIR), combine=\"by_coords\").drop(\n",
    "    \"level\"\n",
    ")\n",
    "\n",
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "datasets = [z, t]\n",
    "ds = xr.merge(datasets)\n",
    "\n",
    "# In this notebook let's only load a subset of the training data\n",
    "ds_train = ds.sel(time=slice(\"2015\", \"2016\"))\n",
    "ds_test = ds.sel(time=slice(\"2017\", \"2018\"))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({\"z\": None, \"t\": None})\n",
    "\n",
    "bs = 32\n",
    "lead_time = 6\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = WorldBench3D(\n",
    "    ds_train.sel(time=slice(\"2015\", \"2015\")), dic, lead_time, batch_size=bs, load=True\n",
    ")\n",
    "dg_valid = WorldBench3D(\n",
    "    ds_train.sel(time=slice(\"2016\", \"2016\")),\n",
    "    dic,\n",
    "    lead_time,\n",
    "    batch_size=bs,\n",
    "    mean=dg_train.mean,\n",
    "    std=dg_train.std,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(dg_train.mean, dg_train.std)\n",
    "\n",
    "# Now also a generator for testing. Impartant: Shuffle must be False!\n",
    "dg_test = WorldBench3D(\n",
    "    ds_test,\n",
    "    dic,\n",
    "    lead_time,\n",
    "    batch_size=bs,\n",
    "    mean=dg_train.mean,\n",
    "    std=dg_train.std,\n",
    "    shuffle=False,\n",
    ")\n",
    "x, y = dg_test[0]\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.isel(time=100)[\"t\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.isel(time=4200)[\"z\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make smaller 1d ds\n",
    "DATADIR = data_in / \"5.625deg\"\n",
    "\n",
    "# Load the validation subset of the data: 2017 and 2018\n",
    "z500_valid = load_test_data('{}/geopotential_500'.format(DATADIR), \"z\")\n",
    "t850_valid = load_test_data('{}/temperature_850'.format(DATADIR), \"t\")\n",
    "valid = xr.merge([z500_valid, t850_valid]).isel()\n",
    "\n",
    "\n",
    "z = xr.open_mfdataset('{}/geopotential_500/*.nc'.format(DATADIR), combine=\"by_coords\")\n",
    "t = xr.open_mfdataset('{}/temperature_850/*.nc'.format(DATADIR), combine=\"by_coords\").drop(\n",
    "    \"level\"\n",
    ")\n",
    "\n",
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "datasets = [z, t]\n",
    "\n",
    "# Choose grid cell Inland of Perth 31.9505° S, 115.8605° E\n",
    "# https://www.google.com/maps/@-31.9784233,118.1256945,10.25z\n",
    "ds_perth = xr.merge(datasets).sel(lat=-30.9375, lon=118.125)\n",
    "\n",
    "# In this notebook let's only load a subset of the training data\n",
    "ds_train_perth = ds_perth  # .sel(time=slice('2015', '2016'))\n",
    "ds_test_perth = ds_perth  # .sel(time=slice('2017', '2018'))\n",
    "ds_perth\n",
    "\n",
    "\n",
    "data_out.mkdir(exist_ok=True)\n",
    "ds_perth.to_netcdf(data_out / \"perth_5.625deg_z_t.nc\")\n",
    "data_out / \"perth_5.625deg_z_t.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# load 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldBench1D(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        var_dict,\n",
    "        lead_time,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        load=True,\n",
    "        mean=None,\n",
    "        std=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={\"level\": [1]}, dims=[\"level\"])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({\"level\": generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, \"level\").transpose(\"time\", \"level\")\n",
    "        self.mean = self.data.mean((\"time\")).compute() if mean is None else mean\n",
    "        self.std = self.data.std(\"time\").compute() if std is None else std\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load:\n",
    "            print(\"Loading data into RAM\")\n",
    "            self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the number of batches per epoch\"\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"Generate one batch of data\"\n",
    "        idxs = self.idxs[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"Updates indexes after each epoch\"\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = data_out / \"5.625deg\"\n",
    "\n",
    "ds_perth = xr.open_dataset(\n",
    "    \"../../data/processed/weatherbench/perth_5.625deg_z_t.nc\"\n",
    ").drop([\"lon\", \"lat\"])\n",
    "# In this notebook let's only load a subset of the training data\n",
    "ds_train_perth = ds_perth.sel(time=slice(\"1900\", \"2016\"))\n",
    "ds_test_perth = ds_perth.sel(time=slice(\"2017\", \"2018\"))\n",
    "\n",
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({\"z\": None, \"t\": None})\n",
    "\n",
    "bs = 32\n",
    "lead_time = 6\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = WorldBench1D(\n",
    "    ds_train_perth.sel(time=slice(\"1900\", \"2015\")),\n",
    "    dic,\n",
    "    lead_time,\n",
    "    batch_size=bs,\n",
    "    load=True,\n",
    ")\n",
    "dg_valid = WorldBench1D(\n",
    "    ds_train_perth.sel(time=slice(\"2016\", \"2016\")),\n",
    "    dic,\n",
    "    lead_time,\n",
    "    batch_size=bs,\n",
    "    mean=dg_train.mean,\n",
    "    std=dg_train.std,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(dg_train.mean, dg_train.std)\n",
    "\n",
    "# Now also a generator for testing. Impartant: Shuffle must be False!\n",
    "dg_test = WorldBench1D(\n",
    "    ds_test_perth,\n",
    "    dic,\n",
    "    lead_time,\n",
    "    batch_size=bs,\n",
    "    mean=dg_train.mean,\n",
    "    std=dg_train.std,\n",
    "    shuffle=False,\n",
    ")\n",
    "x, y = dg_test[0]\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "ds_perth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_perth.sel(time=slice(\"2015\", \"2015-03\"))[\"z\"].plot()\n",
    "plt.show()\n",
    "ds_perth.sel(time=slice(\"2015\", \"2015-03\"))[\"t\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "jup3.7.3",
   "language": "python",
   "name": "jup3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
