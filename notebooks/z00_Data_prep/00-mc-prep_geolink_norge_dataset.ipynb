{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note download data from https://drive.google.com/drive/folders/1EgDN57LDuvlZAwr5-eHWB5CTJ7K9HpDP\n",
    "\n",
    "Credit to this repo: https://github.com/LukasMosser/geolink_dataset\n",
    "\n",
    "## Data Disclaimer\n",
    "\n",
    "All the data serving as an input to these notebooks was generously donated by GEOLINK  \n",
    "and is CC-by-SA 4.0 \n",
    "\n",
    "If you use their data please reference their dataset properly to give them credit for their contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn import preprocessing\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in and our directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_locations = Path(\n",
    "    \"../../data/raw/geolink_dataset/GEOLINK North sea wells with Lithology interpretation/GEOLINK_Lithology and wells NORTH SEA\"\n",
    ")\n",
    "data_locations_wellheads = Path(\"../../data/raw/geolink_dataset/norge_well_heads\")\n",
    "interim_locations = Path(\"../../data/processed/geolink_norge_dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithology = pd.read_excel(data_locations / \"../Lithology code data.xlsx\", header=1)[\n",
    "    :-1\n",
    "]\n",
    "df_lithology[\"Abbreviation\"] = pd.to_numeric(df_lithology[\"Abbreviation\"])\n",
    "df_lithology.to_parquet(\n",
    "    interim_locations / \"geolink_norge_lithology.parquet\", compression=\"gzip\"\n",
    ")\n",
    "df_lithology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df_well_tops = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(data_locations_wellheads / \"wellbore_exploration_all.csv\"),\n",
    "        pd.read_csv(data_locations_wellheads / \"wellbore_development_all.csv\"),\n",
    "        pd.read_csv(data_locations_wellheads / \"wellbore_other_all.csv\"),\n",
    "    ]\n",
    ")\n",
    "df_well_tops[\"wlbWellboreName_geolink\"] = df_well_tops[\"wlbWellboreName\"].str.replace(\n",
    "    \"/\", \"_\"\n",
    ")\n",
    "\n",
    "\n",
    "# add dates\n",
    "date_cols = [\"wlbEntryDate\", \"wlbCompletionDate\"]\n",
    "for c in date_cols:\n",
    "    df_well_tops[c] = pd.to_datetime(df_well_tops[c])  # .astype('str')\n",
    "\n",
    "df_well_tops[\"wlbNsDecDeg\"] = df_well_tops[\"wlbNsDecDeg\"].replace(0, np.nan)\n",
    "df_well_tops[\"wlbEwDesDeg\"] = df_well_tops[\"wlbEwDesDeg\"].replace(0, np.nan)\n",
    "\n",
    "a = set(df_well_tops.columns)\n",
    "df_well_tops = df_well_tops.dropna(axis=1, thresh=0.9 * len(df_well_tops))\n",
    "b = set(df_well_tops.columns)\n",
    "print(\"removed\", a - b)\n",
    "\n",
    "# make into geodataframe\n",
    "df_well_tops = gpd.GeoDataFrame(\n",
    "    df_well_tops,\n",
    "    geometry=gpd.points_from_xy(df_well_tops.wlbEwDesDeg, df_well_tops.wlbNsDecDeg),\n",
    ")\n",
    "df_well_tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to import these files as las files and get their dataframes and hopefully put them into a data format that is more suited for ML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (interim_locations / \"geolink_norge_well_logs_raw.parquet\").exists():\n",
    "\n",
    "    # load las files\n",
    "    well_dataframes = []\n",
    "    files = sorted(data_locations.glob(\"*.las\"))\n",
    "    for f in tqdm(files):\n",
    "        df = lasio.read(f).df()\n",
    "        df[\"Well\"] = f.stem\n",
    "        well_dataframes.append(df)\n",
    "\n",
    "    df_all = pd.concat(well_dataframes)\n",
    "\n",
    "    df_all[\"Well\"] = df_all[\"Well\"].astype(\"category\")\n",
    "\n",
    "    # Name lithology\n",
    "    litho_dict = df_lithology.set_index(\"Abbreviation\")[\"Lithology\"].to_dict()\n",
    "    df_all[\"LITHOLOGY_GEOLINK\"] = (\n",
    "        df_all[\"LITHOLOGY_GEOLINK\"].replace(litho_dict).astype(\"category\")\n",
    "    )\n",
    "\n",
    "    # unique index\n",
    "    df_all = df_all.reset_index()  # .set_index(['Well', 'DEPT'])\n",
    "\n",
    "    df_all.to_parquet(\n",
    "        interim_locations / \"geolink_norge_well_logs_raw.parquet\", compression=\"gzip\"\n",
    "    )\n",
    "\n",
    "df_all = pd.read_parquet(interim_locations / \"geolink_norge_well_logs_raw.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean las files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean.\n",
    "\n",
    "# must have well head\n",
    "df_all_clean2 = df_all[\n",
    "    df_all.Well.apply(lambda s: s in set(df_well_tops[\"wlbWellboreName_geolink\"]))\n",
    "]\n",
    "\n",
    "# must have lithology\n",
    "df_all_clean2 = df_all_clean2.dropna(subset=[\"LITHOLOGY_GEOLINK\"])\n",
    "print(\"nans\", df_all_clean2.isna().mean().sort_values())\n",
    "# Remove logs which are present less than half the time\n",
    "df_all_clean1 = df_all_clean2.dropna(axis=1, thresh=0.9 * len(df_all_clean2))\n",
    "print('kept {:%} cols'.format(len(df_all_clean1.columns) / len(df_all_clean2.columns)))\n",
    "# Drop columns with Nan's\n",
    "df_all_clean = df_all_clean1.dropna(axis=0)\n",
    "print('kept {:%} rows'.format(len(df_all_clean) / len(df_all_clean2)))\n",
    "df_all_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by well name\n",
    "wells_val = [\n",
    "    \"35_11-1\",\n",
    "    \"35_11-10\",\n",
    "    \"35_11-11\",\n",
    "    \"35_11-12\",\n",
    "    \"35_11-13\",\n",
    "    \"35_11-15 S\",\n",
    "    \"35_11-2\",\n",
    "    \"35_11-5\",\n",
    "    \"35_11-6\",\n",
    "    \"35_11-7\",\n",
    "    \"35_12-1\",\n",
    "]\n",
    "\n",
    "wells_test = [\n",
    "    \"34_10-12\",\n",
    "    \"34_10-16 R\",\n",
    "    \"34_10-17\",\n",
    "    \"34_10-19\",\n",
    "    \"34_10-21\",\n",
    "    \"34_10-23\",\n",
    "    \"34_10-33\",\n",
    "    \"34_10-35\",\n",
    "    \"34_10-5\",\n",
    "    \"34_10-7\",\n",
    "    \"34_11-1\",\n",
    "    \"34_11-2 S\",\n",
    "    \"34_11-3 T2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean_test = df_all_clean[df_all_clean.Well.apply(lambda s: s in wells_test)]\n",
    "df_all_clean_train = df_all_clean[\n",
    "    df_all_clean.Well.apply(lambda s: (s not in wells_test) and (s not in wells_val))\n",
    "]\n",
    "assert len(set(df_all_clean_val.Well).intersection(set(df_all_clean_train))) == 0\n",
    "assert len(set(df_all_clean_test.Well).intersection(set(df_all_clean_train))) == 0\n",
    "assert len(set(df_all_clean_test.Well).intersection(set(df_all_clean_val))) == 0\n",
    "len(df_all_clean_train), len(df_all_clean_val), len(df_all_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean_train.to_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\", compression=\"gzip\"\n",
    ")\n",
    "df_all_clean_test.to_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_test.parquet\", compression=\"gzip\"\n",
    ")\n",
    "df_all_clean_val.to_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_val.parquet\", compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picks = pd.read_excel(\n",
    "    data_locations / \"../NPD stratigraphic picks north sea.xlsx\", header=0\n",
    ")\n",
    "df_picks.to_parquet(\n",
    "    interim_locations / \"geolink_norge_picks.parquet\", compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well heads part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only wells we use\n",
    "a = sorted(df_all.Well.unique())\n",
    "df_well_tops = df_well_tops[\n",
    "    df_well_tops[\"wlbWellboreName_geolink\"].apply(lambda s: s in a)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_well_tops.to_file(interim_locations / \"norge_well_tops.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test load\n",
    "df_all_clean2 = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    ")  # .set_index(['Well', 'DEPT'])\n",
    "\n",
    "df_well_tops = gpd.read_file(interim_locations / \"norge_well_tops.gpkg\")\n",
    "df_well_tops_minimal = df_well_tops[\n",
    "    [\n",
    "        \"wlbWellboreName_geolink\",\n",
    "        \"wlbCompletionYear\",\n",
    "        \"wlbKellyBushElevation\",\n",
    "        \"wlbCompletionDate\",\n",
    "        \"wlbTotalDepth\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "]\n",
    "df_well_tops.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge well tops and well logs, a selection\n",
    "df_all_clean3 = pd.merge(\n",
    "    left=df_all_clean2.sample(1000),\n",
    "    right=df_well_tops_minimal,\n",
    "    left_on=\"Well\",\n",
    "    right_on=\"wlbWellboreName_geolink\",\n",
    "    how=\"left\",\n",
    ").drop(columns=\"wlbWellboreName_geolink\")\n",
    "df_all_clean3 = df_all_clean3.set_index(['Well', 'DEPT'])\n",
    "df_all_clean3 = gpd.GeoDataFrame(df_all_clean3, geometry=df_all_clean3['geometry'])\n",
    "df_all_clean3.plot()\n",
    "# df_all_clean3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picks = pd.read_parquet(interim_locations / \"geolink_norge_picks.parquet\")\n",
    "df_picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    ").set_index([\"Well\", \"DEPT\"])\n",
    "df_all_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_clean = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    ").set_index([\"Well\", \"DEPT\"])\n",
    "df_all_clean['DEPT'] = df_all_clean.index.get_level_values(1)\n",
    "df_all_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_ml_curriculum.visualization.well_log import plot_facies, plot_well\n",
    "well_name=\"30_4-1\"\n",
    "logs = df_all_clean.xs(well_name)\n",
    "facies = logs['LITHOLOGY_GEOLINK'].astype('category').values\n",
    "plot_well(well_name, \n",
    "          logs, \n",
    "          facies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1,8))\n",
    "plot_facies(facies, plt.gca(), colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reindex depth and to Xarray\n",
    "\n",
    "This lets us includes location easily without using much more space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Load some\n",
    "df_all_clean1 = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_test.parquet\"\n",
    ").set_index(['Well', 'DEPT'])\n",
    "df_all_clean1['Depth'] = df_all_clean1.index.get_level_values(1)\n",
    "df_all_clean1['split'] = 'test'\n",
    "\n",
    "# Load some\n",
    "df_all_clean2 = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    ").set_index(['Well', 'DEPT'])\n",
    "df_all_clean2['Depth'] = df_all_clean2.index.get_level_values(1)\n",
    "df_all_clean2['split'] = 'train'\n",
    "\n",
    "# Load some\n",
    "df_all_clean3 = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_val.parquet\"\n",
    ").set_index(['Well', 'DEPT'])\n",
    "df_all_clean3['Depth'] = df_all_clean3.index.get_level_values(1)\n",
    "df_all_clean3['split'] = 'val'\n",
    "\n",
    "df_all = pd.concat([df_all_clean1, df_all_clean2, df_all_clean3])\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_well_tops = gpd.read_file(interim_locations / \"norge_well_tops.gpkg\")\n",
    "df_well_tops_minimal = df_well_tops[\n",
    "    [\n",
    "        \"wlbWellboreName_geolink\",\n",
    "        \"wlbCompletionYear\",\n",
    "        \"wlbKellyBushElevation\",\n",
    "        \"wlbCompletionDate\",\n",
    "        \"wlbTotalDepth\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "].copy()\n",
    "df_well_tops_minimal['xc'] = df_well_tops_minimal.geometry.x\n",
    "df_well_tops_minimal['yc'] = df_well_tops_minimal.geometry.y\n",
    "df_well_tops_minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nidx = np.arange(0, 6000, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(x):\n",
    "    \"\"\"Reindex each well to 15cm\"\"\"\n",
    "    x = x.reset_index().set_index('DEPT')\n",
    "    x = x.reindex(nidx, method='nearest', limit=1).drop(columns=['Well']).sort_index()\n",
    "    return x\n",
    "#     return x.reset_index().set_index(['Well', 'DEPT'])\n",
    "\n",
    "df_all3 = df_all.groupby(level=0).apply(reindex).dropna()\n",
    "df_all3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "xr_all_clean2 = df_all3.to_xarray()\n",
    "xr_all_clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_wells = df_well_tops_minimal.rename(columns={'wlbWellboreName_geolink':'Well'}).set_index('Well').to_xarray()\n",
    "xr_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "xr_all = xr.merge(\n",
    "    [xr_all_clean2, xr_wells],\n",
    "    join='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_all2 = xr_all.sortby(['Well', 'DEPT'])\n",
    "xr_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from deep_ml_curriculum.visualization.well_log import plot_well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_name=\"30_4-1\"\n",
    "logs = xr_all2.sel(Well=well_name).to_dataframe().dropna()\n",
    "logs['DEPT'] = logs['Depth']\n",
    "facies = logs['LITHOLOGY_GEOLINK'].astype('category').values\n",
    "plot_well(well_name, logs, facies)\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_ml_curriculum.visualization.well_log import plot_facies, plot_well\n",
    "well_name=\"30_4-1\"\n",
    "logs = df_all_clean.xs(well_name)\n",
    "facies = logs['LITHOLOGY_GEOLINK'].astype('category').values\n",
    "plot_well(well_name, \n",
    "          logs, \n",
    "          facies)\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dset_to_nc(dset, f, engine=\"netcdf4\", compression={\"zlib\": True}):\n",
    "    if isinstance(dset, xr.DataArray):\n",
    "        dset = dset.to_dataset(name=\"data\")\n",
    "    encoding = {k: {\"zlib\": True} for k in dset.data_vars}\n",
    "    print('saving to {}'.format(f))\n",
    "    dset.to_netcdf(f, engine=engine, encoding=encoding)\n",
    "    print('Wrote {}.nc size={} M'.format(f.stem, f.stat().st_size / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_to_nc(dset=xr_all.drop(['geometry']),\n",
    "          f=interim_locations/'geolink_norge_well_logs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "def get_dir_size(start_path=\".\"):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def dset_to_zarr(dset, f):\n",
    "    if isinstance(dset, xr.DataArray):\n",
    "        dset = dset.to_dataset(name=\"data\")\n",
    "    encoding = {k: {\"zlib\": True} for k in dset.data_vars}\n",
    "    print('saving to {}'.format(f))\n",
    "    if f.exists():\n",
    "        try:\n",
    "            return xr.open_zarr(f)\n",
    "        except:\n",
    "            shutil.rmtree(f)\n",
    "    dset.to_zarr(str(f))\n",
    "    print('{}.zarr size={} M'.format(f.stem, get_dir_size(str(f)) / 1000000.0))\n",
    "    \n",
    "dset_to_zarr(dset=xr_all.drop(['geometry']),\n",
    "          f=interim_locations/'geolink_norge_well_logs.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "jup3.7.3",
   "language": "python",
   "name": "jup3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
