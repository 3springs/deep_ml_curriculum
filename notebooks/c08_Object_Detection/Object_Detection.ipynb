{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook provides examples on to use pre-trained deep neural networks for the object recognition task the notebook. All images used for testing were downloaded from unplash.com under license [free-usable images](https://unsplash.com/license).\n",
    "\n",
    "## Table of Content\n",
    "0. [Libraries](#libraries)\n",
    "1. [Object Recognition](#object)\n",
    "2. [COCO Dataset Recognition](#coco)\n",
    "3. [Faster R-CNN in Pytorch](#frcnn)\n",
    "4. [Mask R-CNN in Pytorch](#frcnn)\n",
    "5. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import urllib.request\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Recognition\n",
    "\n",
    "A more complex problem than the classic image recognition problem is the object recognition task. In image classification we predict the category of an image, however, in object recognition not only we classify multiple object in an image but we can also predict the location of those objects. \n",
    "In this notebook we will discuss **Faster RCNN** model for object recognition and **Mask RCNN** for instance segmentation.\n",
    "\n",
    "**Object Recognition:**\n",
    "\n",
    "Objects detected with OpenCV's Deep Neural Network module (dnn) by using a YOLOv3 model trained on **COCO dataset** capable to detect objects of 80 common classes.\n",
    "\n",
    "<img width=500 heigh=500 src='https://upload.wikimedia.org/wikipedia/commons/3/38/Detected-with-YOLO--Schreibtisch-mit-Objekten.jpg?download'>\n",
    "\n",
    "Image Credits: [MTheiler](https://en.wikipedia.org/wiki/Object_detection#/media/File:Detected-with-YOLO--Schreibtisch-mit-Objekten.jpg) License: [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "\n",
    "# Instance Segmentation\n",
    "\n",
    "In instance segmentation we will combine object recognition and semantic segmentation.\n",
    "\n",
    "<img width=800 heigh=500 src='maskrcnn.png'>\n",
    "\n",
    "Credits: [He et al](https://arxiv.org/pdf/1703.06870.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Dataset\n",
    "\n",
    "[Oficial Website](https://cocodataset.org/#home)\n",
    "\n",
    "COCO is one of the most popular datasets for object detection. Currently, the COCO dataset offers a dataset for object detection, keypoints in faces, and even segmentation. Below, we will be using a list with the category names corresponding to the COCO dataset for object recognition and keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    \"__background__\",\n",
    "    \"person\",\n",
    "    \"bicycle\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"airplane\",\n",
    "    \"bus\",\n",
    "    \"train\",\n",
    "    \"truck\",\n",
    "    \"boat\",\n",
    "    \"traffic light\",\n",
    "    \"fire hydrant\",\n",
    "    \"N/A\",\n",
    "    \"stop sign\",\n",
    "    \"parking meter\",\n",
    "    \"bench\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"sheep\",\n",
    "    \"cow\",\n",
    "    \"elephant\",\n",
    "    \"bear\",\n",
    "    \"zebra\",\n",
    "    \"giraffe\",\n",
    "    \"N/A\",\n",
    "    \"backpack\",\n",
    "    \"umbrella\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"handbag\",\n",
    "    \"tie\",\n",
    "    \"suitcase\",\n",
    "    \"frisbee\",\n",
    "    \"skis\",\n",
    "    \"snowboard\",\n",
    "    \"sports ball\",\n",
    "    \"kite\",\n",
    "    \"baseball bat\",\n",
    "    \"baseball glove\",\n",
    "    \"skateboard\",\n",
    "    \"surfboard\",\n",
    "    \"tennis racket\",\n",
    "    \"bottle\",\n",
    "    \"N/A\",\n",
    "    \"wine glass\",\n",
    "    \"cup\",\n",
    "    \"fork\",\n",
    "    \"knife\",\n",
    "    \"spoon\",\n",
    "    \"bowl\",\n",
    "    \"banana\",\n",
    "    \"apple\",\n",
    "    \"sandwich\",\n",
    "    \"orange\",\n",
    "    \"broccoli\",\n",
    "    \"carrot\",\n",
    "    \"hot dog\",\n",
    "    \"pizza\",\n",
    "    \"donut\",\n",
    "    \"cake\",\n",
    "    \"chair\",\n",
    "    \"couch\",\n",
    "    \"potted plant\",\n",
    "    \"bed\",\n",
    "    \"N/A\",\n",
    "    \"dining table\",\n",
    "    \"N/A\",\n",
    "    \"N/A\",\n",
    "    \"toilet\",\n",
    "    \"N/A\",\n",
    "    \"tv\",\n",
    "    \"laptop\",\n",
    "    \"mouse\",\n",
    "    \"remote\",\n",
    "    \"keyboard\",\n",
    "    \"cell phone\",\n",
    "    \"microwave\",\n",
    "    \"oven\",\n",
    "    \"toaster\",\n",
    "    \"sink\",\n",
    "    \"refrigerator\",\n",
    "    \"N/A\",\n",
    "    \"book\",\n",
    "    \"clock\",\n",
    "    \"vase\",\n",
    "    \"scissors\",\n",
    "    \"teddy bear\",\n",
    "    \"hair drier\",\n",
    "    \"toothbrush\",\n",
    "]\n",
    "\n",
    "COCO_PERSON_KEYPOINT_NAMES = [\n",
    "    \"nose\",\n",
    "    \"left_eye\",\n",
    "    \"right_eye\",\n",
    "    \"left_ear\",\n",
    "    \"right_ear\",\n",
    "    \"left_shoulder\",\n",
    "    \"right_shoulder\",\n",
    "    \"left_elbow\",\n",
    "    \"right_elbow\",\n",
    "    \"left_wrist\",\n",
    "    \"right_wrist\",\n",
    "    \"left_hip\",\n",
    "    \"right_hip\",\n",
    "    \"left_knee\",\n",
    "    \"right_knee\",\n",
    "    \"left_ankle\",\n",
    "    \"right_ankle\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region based CNNs\n",
    "\n",
    "Region-based convolutional neural networks or regions with CNN features (R-CNNs) are a pioneering approach that applies deep models to object detection. R-CNN models first select several proposed regions from an image and then they use a CNN to perform forward computation to extract features from each proposed area. Afterwards, we use the features of each proposed region to predict their categories and bounding boxes.\n",
    "\n",
    "Specifically, R-CNNs are composed of four main parts:\n",
    "\n",
    "- Selective search is performed on the input image to select multiple high-quality proposed regions. These proposed regions are generally selected on multiple scales and have different shapes and sizes. The category and ground-truth bounding box of each proposed region is labeled.\n",
    "\n",
    "- A pre-trained CNN is selected and placed, in truncated form, before the output layer. It transforms each proposed region into the input dimensions required by the network and uses forward computation to output the features extracted from the proposed regions.\n",
    "\n",
    "- The features and labeled category of each proposed region are combined as an example to train multiple support vector machines for object classification. Here, each support vector machine is used to determine whether an example belongs to a certain category.\n",
    "\n",
    "- The features and labeled bounding box of each proposed region are combined as an example to train a linear regression model for ground-truth bounding box prediction.\n",
    "\n",
    "Although R-CNN models use pre-trained CNNs to effectively extract image features, the main downside is the slow speed. As you can imagine, we can select thousands of proposed regions from a single image, requiring thousands of forward computations from the CNN to perform object detection. This massive computing load means that R-CNNs are not widely used in actual applications.\n",
    "\n",
    "Source: [R-CNNs](https://d2l.ai/chapter_computer-vision/rcnn.html)\n",
    "\n",
    " In this section, we will discuss R-CNNs and a series of improvements made to them: [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf), [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), and [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf). We will focus our discussion to the designs of these models and the implementation of the models **Fast R-CNN** and **Mask R-CNN** in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast RCNN\n",
    "\n",
    "**Fast R-CNN architecture:**\n",
    " An input image and multiple regions of interest (RoIs) are input into a fully convolutional network. Each RoI is pooled into a fixed-size feature map and then mapped to a feature vector by fully connected layers (FCs).\n",
    "The network has two output vectors per RoI: softmax probabilities and per-class bounding-box regression offsets. The architecture is trained end-to-end with a multi-task loss. <br/><br/>\n",
    "<img width=400 heigh=400 src='fastrcnn.png'/>\n",
    "\n",
    "Source: [Ross Girshick](https://arxiv.org/pdf/1504.08083.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster RCNN in Pytorch\n",
    "\n",
    "Faster RCNN is a popular object detection network that combine Region Proposal Networks (RPN) and Fast RCNN for a faster inference. RPN are 'attention' mechanism that tells the unified network where to look.\n",
    "\n",
    "<img width=400 heigh=400 src='rpn.png'/>\n",
    "\n",
    "Source: [Ren et al.](https://arxiv.org/pdf/1506.01497.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original implementation of Faster RCNN, the backbone used was VGG19. However, Pytorch offers a pretrained version (COCO Dataset) using ResNet50 as the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"example.jpg\"\n",
    "Image.open(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some functions for the inference. Read the comments in every line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = T.ToTensor()\n",
    "\n",
    "\n",
    "def get_prediction(img, threshold=0.5):\n",
    "    # Defing PyTorch Transform\n",
    "    transform = T.Compose([T.ToTensor()])  # Convert PIL Image to tensor\n",
    "    img = transform(img)  # Apply the transform to the image\n",
    "    pred = FRCNN([img])  # Pass the image to the model\n",
    "    pred_class = [\n",
    "        COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0][\"labels\"].numpy())\n",
    "    ]  # Get the Prediction Score\n",
    "    pred_boxes = [\n",
    "        [(i[0], i[1]), (i[2], i[3])] for i in list(pred[0][\"boxes\"].detach().numpy())\n",
    "    ]  # Bounding boxes\n",
    "    pred_score = list(pred[0][\"scores\"].detach().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][\n",
    "        -1\n",
    "    ]  # Get list of index with score greater than threshold.\n",
    "    pred_boxes = pred_boxes[: pred_t + 1]\n",
    "    pred_class = pred_class[: pred_t + 1]\n",
    "    return pred_boxes, pred_class, pred_score\n",
    "\n",
    "\n",
    "def object_detection_api(img_path, threshold=0.5):\n",
    "    # Open Image\n",
    "    im = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    boxes, pred_cls, pred_score = get_prediction(im, threshold)  # Get predictions\n",
    "    # Print predictions\n",
    "    print(dict(zip(pred_cls, pred_score)))\n",
    "\n",
    "    # Loop over bounding boxes\n",
    "    for i in range(len(boxes)):\n",
    "        # Change color according to prediction score\n",
    "        color = (0, 255, 0)\n",
    "        if pred_score[i] < 0.4:\n",
    "            color = (255, 0, 0)\n",
    "        if pred_score[i] > 0.4 and pred_score[i] < 0.7:\n",
    "            color = (255, 100, 0)\n",
    "\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        # Add Rectangle\n",
    "        draw.rectangle([boxes[i][0], boxes[i][1]], outline=color)\n",
    "        # Add text\n",
    "        draw.text(boxes[i][0], pred_cls[i], align=\"left\", fill=color)\n",
    "    plt.figure(figsize=(10, 10))  # display the output image\n",
    "    plt.imshow(im)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `object_detecion_api` we created some logic to change the color of the bounding boxes according to the confidence in the prediction. We can also pass the parameter `threshold` which will filter the results below the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we set the threshold to zero it will show all results\n",
    "object_detection_api(filename, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter the results for those prediction with confidence lower than 0.5\n",
    "object_detection_api(filename, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also take only those predictions above 0.9 (Strong confidence)\n",
    "object_detection_api(filename, threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "\n",
    "**Exercise 1:** <br>\n",
    "    \n",
    "1. Use the code below to get the file names of the images in the example folder.\n",
    "    \n",
    "```python\n",
    "import os\n",
    "images = os.listdir('./exercise1')\n",
    "```\n",
    "    \n",
    "2. Write a code that allows you take `images` as a parameter and a `threshold`. Then, show the `images` with the bounding box predictions.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 1.\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 1</b></font>\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "import os\n",
    "DIR = './exercise1'\n",
    "images = os.listdir(DIR)\n",
    "\n",
    "for image in images:\n",
    "    object_detection_api(f'{DIR}/{image}', threshold=0.5)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask RCNN in Pytorch\n",
    "\n",
    "If training data is labeled with the pixel-level positions of each object in an image, a Mask R-CNN model can effectively use these detailed labels to further improve the precision of object detection.\n",
    "\n",
    "Mask R-CNN is a modification to the Faster R-CNN model. Mask R-CNN models replace the RoI pooling layer with an RoI alignment layer. This allows the use of bilinear interpolation to retain spatial information on feature maps, making Mask R-CNN better suited for pixel-level predictions. The RoI alignment layer outputs feature maps of the same shape for all RoIs. This not only predicts the categories and bounding boxes of RoIs, but allows us to use an additional fully convolutional network to predict the pixel-level positions of objects. We will describe how to use fully convolutional networks to predict pixel-level semantics in images later in this chapter.\n",
    "\n",
    "Source: [R-CNN](https://d2l.ai/chapter_computer-vision/rcnn.html)\n",
    "\n",
    "Architecture:\n",
    "<img height=600 width=600 src='mask_arch.png'/>\n",
    "\n",
    "Source: [He et al.](https://arxiv.org/pdf/1703.06870.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRCNN = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_mask(img_path, threshold):\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img = transform(Image.open(img_path))\n",
    "    pred = MRCNN([img])\n",
    "    pred_score = list(pred[0][\"scores\"].detach().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
    "    masks = (pred[0][\"masks\"] > 0.5).squeeze().detach().cpu().numpy()\n",
    "    pred_class = [\n",
    "        COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0][\"labels\"].numpy())\n",
    "    ]\n",
    "    pred_boxes = [\n",
    "        [(i[0], i[1]), (i[2], i[3])] for i in list(pred[0][\"boxes\"].detach().numpy())\n",
    "    ]\n",
    "    masks = masks[: pred_t + 1]\n",
    "    pred_boxes = pred_boxes[: pred_t + 1]\n",
    "    pred_class = pred_class[: pred_t + 1]\n",
    "    return masks, pred_boxes, pred_class\n",
    "\n",
    "\n",
    "def random_colour_masks(image):\n",
    "    colors = [\n",
    "        [0, 255, 0],\n",
    "        [0, 0, 255],\n",
    "        [255, 0, 0],\n",
    "        [0, 255, 255],\n",
    "        [255, 255, 0],\n",
    "        [255, 0, 255],\n",
    "        [80, 70, 180],\n",
    "        [250, 80, 190],\n",
    "        [245, 145, 50],\n",
    "        [70, 150, 250],\n",
    "        [50, 190, 190],\n",
    "    ]\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    r[image == 1], g[image == 1], b[image == 1] = colors[random.randrange(0, 10)]\n",
    "    coloured_mask = np.stack([r, g, b], axis=2)\n",
    "    return coloured_mask\n",
    "\n",
    "\n",
    "def instance_segmentation_api(img_path, threshold=0.5, rect_th=1, text_size=1):\n",
    "    masks, boxes, pred_cls = get_prediction_mask(img_path, threshold)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for i in range(len(masks)):\n",
    "        rgb_mask = random_colour_masks(masks[i])\n",
    "        img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "        cv2.rectangle(\n",
    "            img, boxes[i][0], boxes[i][1], color=(0, 255, 0), thickness=rect_th\n",
    "        )\n",
    "        # Resize font according to image size\n",
    "        scale = 1  # this value can be from 0 to 1 (0,1] to change the size of the text relative to the image\n",
    "        imageHeight, imageWidth, _ = img.shape\n",
    "        fontScale = min(imageWidth, imageHeight) / (25 / scale)\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            pred_cls[i],\n",
    "            boxes[i][0],\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            text_size,\n",
    "            (0, 255, 0),\n",
    "        )\n",
    "    plt.figure(figsize=(20, 30))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation_api('./exercise2/mask1.jpg'.format(), threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "\n",
    "**Exercise 2:** <br>\n",
    "    \n",
    "1. Repeat the steps for Exercise 1 using images in `./exercise1` and `./exercise2` this time the Mask R-CNN model.\n",
    "2. In addition, modify the function `instance_segmentation_api` so it will be optional to get the semantic segmentation, the bouding boxes with prediction class or both (instance segmentation).\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 2\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>Hints for Exercise 2</b></font>\n",
    "</summary>\n",
    "    \n",
    "1. There are many ways of getting the paths using python. We will use an advanced function using python functional programming paradign like: `lambda`, `map`.\n",
    "    \n",
    "```python\n",
    "get_list_images = lambda path: list(map(lambda x,y: f'{y}/{x}', os.listdir(path), [path]*len(os.listdir(path))))\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "combine_lists = lambda paths: flatten(list(map(get_list_images, paths)))\n",
    "combine_lists(['./exercise1', './exercise2'])\n",
    "    \n",
    "```\n",
    "2. You can always pass some boolean variables and create some conditional inside your function.\n",
    "```python\n",
    "    instance_segmentation_api(img_path, threshold=0.5, rect_th=1, text_size=1, segmantic_segmentation=False, bounding_boxes=False, both=True)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Source: [RCNN](https://d2l.ai/chapter_computer-vision/rcnn.html)\n",
    "\n",
    "- An R-CNN model selects several proposed regions and uses a CNN to perform forward computation and extract the features from each proposed region. It then uses these features to predict the categories and bounding boxes of proposed regions.\n",
    "\n",
    "- Fast R-CNN improves on the R-CNN by only performing CNN forward computation on the image as a whole. It introduces an RoI pooling layer to extract features of the same shape from RoIs of different shapes.\n",
    "\n",
    "- Faster R-CNN replaces the selective search used in Fast R-CNN with a region proposal network. This reduces the number of proposed regions generated, while ensuring precise object detection.\n",
    "\n",
    "- Mask R-CNN uses the same basic structure as Faster R-CNN, but adds a fully convolution layer to help locate objects at the pixel level and further improve the precision of object detection."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
