{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# Hide all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will learn about Neural Networks and how to implement them using Scikit-learn. Also, we will start exploring some basic concepts for the Pytorch library.\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "- [0. Video. Neural networks in the world around us](#0)\n",
    "- [1. Deep Nueral Networks](#1)\n",
    "    - [1.1 Backpropagation](#1-1)\n",
    "    - [1.2 Important Concepts](#1-2)\n",
    "- [2. Nueral networks in Scikit-learn](#2)\n",
    "- [3. Introduction to Pytorch](#3)\n",
    "    - [3.1 Tensors](#3-1)\n",
    "    - [3.2 Operations](#3-2)\n",
    "    - [3.3 Converting Numpy arrays and Tensors](#3-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "# 0. Neural Networks in the world around us\n",
    "\n",
    "The following video shows a nicely animated video explaining neural networks in the world around us. This video was provided by the Perth Machine Learning Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube Video\n",
    "HTML(\n",
    "    '<iframe width=\"800\" height=\"600\" src=\"https://www.youtube.com/embed/JxZul0fsw1k\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1. Deep Neural Networks\n",
    "\n",
    "A Neural Network is a computing system inspired by the biological neural networks that constitute animal brains. Each neuron can connect (edges) with other neurons. Those neurons and edges usually have weights that are adjusted in the training phase. The sum of the inputs is then passed to a non-linear function. The same neurons are agregated in layers into layers. Different layers will apply different transformations.\n",
    "\n",
    "In the example below, we can observe a neural network with 3 inputs and 2 outputs. The layers in the middle are called hidden layers. The term 'Deep' from Deep Learning is given for the number of hidden layers in a neural net.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1920px-Colored_neural_network.svg.png' width=300 heigh=300/>\n",
    "\n",
    "[Source Image](https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg) License Image: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)\n",
    "\n",
    "Example of a simple neural network with two input units (each with a single input) and one output unit (with two inputs).\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/4/42/A_simple_neural_network_with_two_input_units_and_one_output_unit.png' width=300 heigh=300/>\n",
    "\n",
    "[Source Image](https://commons.wikimedia.org/wiki/File:A_simple_neural_network_with_two_input_units_and_one_output_unit.png) License Image: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en)\n",
    "\n",
    "\n",
    "\n",
    "The advantages of DNNs are:\n",
    "\n",
    "- Capability to learn non-linear models.\n",
    "\n",
    "- Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "Some disadvantages of DNNs include:\n",
    "\n",
    "- DNNs with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "\n",
    "- DNNs requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "\n",
    "- DNNs are sensitive to feature scaling.\n",
    "\n",
    "In the upcoming notebooks, we will learn how to overcome and deal with some of those problems.\n",
    "\n",
    "Source: [Scikit-learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mlp-tips)\n",
    "\n",
    "<a name=\"1-1\"></a>\n",
    "\n",
    "## 1.1 Backpropagation\n",
    "\n",
    "The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\n",
    "    \n",
    "<a name=\"1-2\"></a>    \n",
    "## 1.2 Important Concepts\n",
    "    \n",
    "**Optimizers:** are algorithms that updates the neural network parameters such as the weights to reduce the losses. Popular optimizers are : \n",
    "    \n",
    "- `Gradient Descent`: Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient of the function at the current point.\n",
    "\n",
    "- `Stochastic Gradient Descent`: Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof.\n",
    "    \n",
    "- `Adam`: an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\n",
    "\n",
    "Source: [Paper](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "    \n",
    "The methods mentioned before are iterative processes that take steps to advance towards a solution. The step size at each iteration towards the minimum of loss is the **learning rate**.\n",
    "    \n",
    "    \n",
    "**Activation Functions:** Activation functions are important to add non-linearity into the models. This will allow the model to represent more complex data.\n",
    "    \n",
    "**Regularization** is the process of adding information in order to prevent overfitting.\n",
    "\n",
    "    \n",
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b>Exercise 1</b>: <br>\n",
    "In the next example, you will be able to use the tensorflow playground to test some of the concepts of neural networks.\n",
    "    \n",
    "1. Read the concepts below\n",
    "2. Explore in the iterative playground how changing some hyperparameters will affect the prediction, the number of epochs and the test loss in the output.\n",
    "    - Test different values for `learning rate` and different `activations`.\n",
    "    - Add different number of neurons for the hidden layers and compare results.\n",
    "3. Add 6 hidden layers and perform same tests from step 2. \n",
    "4. Use the `Spiral` data, and for every hidden layer (6 in total) add 8 neurons. Compare results of the test loss with and without regularization `L1` and `L2`.\n",
    "4. Add some noise and perform and repeat steps 2 to 4.\n",
    "    \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\n",
    "    '<iframe src=\"https://playground.tensorflow.org\" width=\"1000\" height=\"800\"></iframe>'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "# 2. Neural Networks in Scikit-learn\n",
    "Source: [MLP](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "> Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers.\n",
    "\n",
    "\n",
    "Let's try a classiffication problem with an MLP model. But first let's import the dataset.\n",
    "For this example, we will be using the digits dataset which contains handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "# Let's first see one of the images\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "print('Target: {}'.format(digits.target[0]))\n",
    "plt.imshow(digits.images[0], cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.images[0])\n",
    "print(\"No. Images:\", len(digits.images))\n",
    "print(\"Shape:\", digits.images[0].shape)\n",
    "print(\"Max value:\", digits.images[0].max())\n",
    "print(\"Min value:\", digits.images[0].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have a total of 1797 images of 8x8 pixels. The values ranges between 0 and 15. As mentioned before, one of the problems with NNs is that they are very sensitive to feature scaling. To solve this problem, we will scale the images in the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MinMaxScaler with a range of [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0, 1))\n",
    "scaler.fit(digits.images[0])\n",
    "images = [scaler.transform(image) for image in digits.images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the max and min of an image to test it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images[0].max())\n",
    "print(images[0].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's separate features from the target and split them between train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently `X` has 3 dimensions. However, `MLPClassifier` only accepts arrays with 1 or 2 dimensions. We will need to reshape `X` to have only 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's flatten one dimension\n",
    "X = np.resize(X, (X.shape[0], 8 * 8))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pick some hyperparameters before the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate is by default 0.001, we will use the default value\n",
    "def train(X_train, y_train, hidden_layers, activation):\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        random_state=2020,\n",
    "        max_iter=100,\n",
    "    ).fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 2\n",
    "activation = \"relu\"  # ReLU Activation function\n",
    "solver = \"sgd\"  # Stochastic Gradient Descent optimizer\n",
    "\n",
    "# Let's train the model using our custom hyperparameters\n",
    "clf = train(X_train, y_train, hidden_layers, activation)\n",
    "# Let's evaluate the accuracy of the model using the test data\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 28.3%, which is not really great. Let's try again and train a model changing the number of hidden layers to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = 10\n",
    "\n",
    "# Let's train the model using our custom hyperparameters\n",
    "clf = train(X_train, y_train, hidden_layers, activation)\n",
    "# Let's evaluate the accuracy of the model using the test data\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10 hidden layers our model's accuracy improved significantly from 28.3% to 71.48%. Much better than before !\n",
    "\n",
    "The more hidden layers a Neural Networks has, it is say that the model is \"Deeper\". Usually deeper models achieve better accuracy, however they are also more prompt to overfitting and regularization techniques are needed to avoid that. The interest of researchers to create models with more layers is what helped created the term \"Deep Learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b>Exercise 2</b>: <br>\n",
    "Create a function to test all possible combinations of the hyperparameters below and compare the accuracy to select the best hyperparameters. The process of selecting the best hyperparameters is also called hyperparameter optimisation. We will talk more about it in the coming lessons.\n",
    "\n",
    "`Hidden Layers`: 2,4,6,8,10 <br/>\n",
    "`Activation`: identity, logistic, tanh, relu <br/>\n",
    "`Solver`: lbfgs, sgd, adam <br/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 2\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 2</b></font>\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "hyperparameters = {\n",
    "    'hidden_layers': [2,4,6,8,10],\n",
    "    'activations': ['identity','logistic','tanh','relu'],\n",
    "    'solver': ['lbfgs','sgd','adam']\n",
    "}\n",
    "\n",
    "for solver in hyperparameters['solver']:\n",
    "    for activation in hyperparameters['activations']:\n",
    "        for num_hidden_layers in hyperparameters['hidden_layers']:\n",
    "            print(f'Train NN with solver:{solver}, activation:{activation} and Number of hidden layers:{num_hidden_layers}')\n",
    "            # Let's train the model using our custom hyperparameters\n",
    "            clf = train(X_train, y_train, num_hidden_layers, activation)\n",
    "            # Let's evaluate the accuracy of the model using the test data\n",
    "            print(clf.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the same model using Pytorch ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "# 3. Introduction to Pytorch\n",
    "\n",
    "PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab (FAIR).\n",
    "\n",
    "A number of pieces of Deep Learning software are built on top of PyTorch, including `Tesla Autopilot`, `Uber's Pyro`, `HuggingFace's Transformers`, `PyTorch Lightning`, and `Catalyst`.\n",
    "\n",
    "\n",
    "According to the [Pytorch's official website](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py) Pytorch is:\n",
    "\n",
    "> It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "\n",
    "We will refer to the equivalent of Numpy arrays as tensors when using Pytorch.\n",
    "\n",
    "Source: [Pytorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-1\"></a>\n",
    "## 3.1 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import pytorch library\n",
    "import torch\n",
    "\n",
    "# Let's create a random tensor with size [5,3]\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor directly from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's creste a random tensor with the same size as the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)  # override dtype!\n",
    "print(x)  # result has the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-2\"></a>\n",
    "## 3.2 Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take look at the addition operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.rand(2)\n",
    "print(x, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use a different syntax for addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.add(x, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x * x2)\n",
    "print(x - x2)\n",
    "print(x / x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-3\"></a>\n",
    "## 3.3 Converting Numpy Arrays and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's convert a Pytorch tensor to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the numpy array changed in value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_(1)  # In-place adding\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to convert a numpy array to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have learnt the basics about Neural Networks and how to implement one NN for classification using Scikit-learn.\n",
    "We also introduced Pytorch and some basic operations with tensors. In the next lesson, we will dive deeper into the some important and concep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and further reading\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "[Pytorch](https://pytorch.org/)\n",
    "\n",
    "[NN Playground](https://playground.tensorflow.org/)\n",
    "\n",
    "[Multi-layered perceptron](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "[Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "[Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "[Adam](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
