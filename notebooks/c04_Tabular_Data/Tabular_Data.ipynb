{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "When we talk about tabular data we mean any data that can be put in a table. The table will have rows and columns. Each row shows an instance of data (aka data point) and each column is a feature of the data. Usually at least one of the columns would be the target value. If the target is a continuous variable, then the problem is a regression, and if it is a discrete variable or categorical variable the problem is a classification.<br>\n",
    "A neural network is very well suited for this type of problems, whether it is a classification or a regression problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn import metrics, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Let's start we a classification problem. In this case we will be using well log data to determine the lithology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"../../data/processed/geolink_norge_dataset/geolink_norge_well_logs_train.parquet\"\n",
    ").set_index([\"Well\", \"DEPT\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a list of wells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "v4wy8aXrFkoK",
    "outputId": "7fa90839-9af9-4d7f-e427-e1b214740312"
   },
   "outputs": [],
   "source": [
    "wells = np.unique(np.array([w for w, d in df.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add depth as a feature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Stb4LhYoFsOt"
   },
   "outputs": [],
   "source": [
    "depths = np.array([d for w, d in df.index])\n",
    "df[\"Depths\"] = depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify how many wells are used for training and how many for test. we are splitting training and test set based on the wells since there is correlation between datapoints in each well which will result in higher accuracy. But this high accuracy is deceiving and not real. As soon as we start testing the model on a new well the accuracy will drop. Splitting data based on wells avoid this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n_wells = 40\n",
    "selected_wells = np.random.choice(wells, n_wells * 2, replace=False)\n",
    "training_wells = selected_wells[:n_wells]\n",
    "test_wells = selected_wells[n_wells:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to process the input and target data. The input data needs to be normalised with a standard scaler, and the output data needs to be converted from text to numbers. To convert text to numbers we use `LabelEncoder` from Scikit Learn. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[\"LITHOLOGY_GEOLINK\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to decide which columns from the data we are going to use as feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [\"CALI\", \"RHOB\", \"GR\", \"DTC\", \"RDEP\", \"RMED\", \"Depths\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabelEncoder` converts each type to a value. It is like creating a list of types and instead of using the names we use the their index in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.transform([\"Shaly Silt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the types at various depths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "idx = 0\n",
    "x = df.loc[wells[idx], \"Depths\"]\n",
    "y = encoder.transform(df.loc[wells[idx], \"LITHOLOGY_GEOLINK\"])\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply transformation to training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(df.loc[training_wells, feat_cols].values)\n",
    "y_train = encoder.transform(df.loc[training_wells, \"LITHOLOGY_GEOLINK\"])\n",
    "x_test = scaler.transform(df.loc[test_wells, feat_cols].values)\n",
    "y_test = encoder.transform(df.loc[test_wells, \"LITHOLOGY_GEOLINK\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a classification model is a value for each type. The type with the highest value is the one the model thinks is most likely to be associated with the input data. Therefore, the output size of the model should be the number of types.<br>\n",
    "Input size will be the number of features (columns) in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(feat_cols)\n",
    "output_size = len(df[\"LITHOLOGY_GEOLINK\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with neural networks, let's see how would a random forest classifier would perform on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    200, max_depth=5, max_features=\"sqrt\", n_jobs=-1, random_state=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(x_train)\n",
    "metrics.accuracy_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(x_test)\n",
    "metrics.accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a random forest is giving us an accuracy of about 35%. Let's see if a neural network can beat that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "A linear layer (sometimes called a fully-connected layer) is a simple matrix multiplication. Let's say we have an input data with $n$ features and we are trying to predict $m$ outputs. In this case the input of size $n$ is multiplied by a $n\\times m$ matrix (called weight) and is summed with a $m\\times 1$ matrix (called bias). The result of this operation would be an output of size $m$.\n",
    "$$ \\left( \\begin{array}{cc}\n",
    "x_1 & x_2 & ... & x_n \\\\\n",
    "\\end{array} \\right)\n",
    "%\n",
    "\\left( \\begin{array}{cc}\n",
    "w_{1,1} & w_{1,2} & ... & w_{1,m} \\\\\n",
    "w_{2,1} & w_{2,2} & ... & w_{2,m} \\\\\n",
    " & \\vdots\\\\\n",
    "w_{n,1} & w_{n,2} & ... & w_{n,m}\n",
    "\\end{array} \\right)\n",
    "+\n",
    "\\left( \\begin{array}{cc}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots\\\\\n",
    "b_m\n",
    "\\end{array} \\right)\n",
    "=\n",
    "\\left( \\begin{array}{cc}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\\n",
    "y_m\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Every time we perform this operation, it is called a linear layer. However, a simple linear layer is not enough to model complex relationship. Therefore, the output of the layers are passed through a non-linear function called _activation function_. <br>\n",
    "There are many activation functions, but one of the most common ones is Rectified Linear Unit (ReLU). ReLU simply keeps the input value if it is positive and replaces it with zero if it is negative. Mathematically you can also write it as:<br>\n",
    "$$ReLU(x) = max(0,x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the start, we create a network that has two linear layers separated by a ReLU activation function. It will takes $n$ inputs and turns it into $h$ values. After passing it through the activation function we turn it into $m$ values as output. You can see the details of the model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(SimpleLinear, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we are using GPU if we have access to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(x_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "x_test = torch.Tensor(x_test).to(device)\n",
    "y_test = torch.LongTensor(y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the model. We are using 7 features, and trying to predict a value for each class in the data. Let's choose a hidden size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLinear(7, output_size, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ADAM` optimiser and Cross Entropy as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training loop and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, bs=256):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = []\n",
    "        for i in range(0, len(x_train), bs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x_train[i : i + bs, :])\n",
    "            loss = loss_func(preds, y_train[i : i + bs])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "            if i % 1000 == 0:\n",
    "                print('Loss: {:.3f}'.format(loss.item()), end=\"\\r\", flush=True)\n",
    "        preds = model(x_test)\n",
    "        loss = loss_func(preds, y_test)\n",
    "        print('Epoch#{} Test Loss = {:.3f}'.format(epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how good a model is based on loss value can be difficult. Let's check the accuracy of the model on training and test set so we can compare it to the random forest classifier we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, y):\n",
    "    m = model.to(\"cpu\")\n",
    "    m.eval()\n",
    "    preds = m(x.to(\"cpu\")).argmax(-1)\n",
    "    preds = preds.detach().numpy()\n",
    "    return metrics.accuracy_score(y.to(\"cpu\").numpy(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is lower than the random forest. Note that our judgement will be based on accuracy on the test set. This shows how good the model is on the data it hasn't seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Linear Layers\n",
    "We discussed what linear layers are, and created a model with two linear layers separated with an activation function. This is called stacking and we cann do it with more than two layers. By adding more layers we create a deep model (hence deep learning) which can understand more complex relationship. However, it must be noted that making the model deeper can also make more difficult to train at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new class which can have as many linear layers as we want. In the example below, we can pass in a list for the hidden sizes (intermediate sizes). If we only pass in one value, it would be similar to the last example, but if we put more than one value in the list we can create a deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[10]):\n",
    "        super(StackedLinear, self).__init__()\n",
    "        sizes = [input_size] + hidden_sizes\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Sequential(nn.Linear(sizes[i], sizes[i + 1]), nn.ReLU()))\n",
    "        layers.append(nn.Linear(sizes[-1], output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an instance of the model with two hidden layers of size $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedLinear(input_size, output_size, [50, 50]).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the model and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, epochs=10, bs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the accuracy should be close to the random forest classifier and maybe even slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "Regularisation is a group of techniques used to stop the model from overfitting the training data and help the model to generalise the patterns in the data without memorising the actual values. Notice in the last example when we created an instance of optimiser we passed in a value for `weight_decay`. Weight decay is a regularisation technique which adds the sum of all the weights to the loss function. As a result, the optimiser tries to avoid large values for model weights which makes it less likely for the model to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise1\n",
    "Retrain the model above by changing the weight decay value and see how it affects the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Another type of regularisation in neural networks is called dropout. Dropout layer is positioned after the activation function and during the training phase it randommly zeros the output of neurons. By doing so, it forces the model to use smaller amount of information at each step and therefore the model wouldn't have too much information to overfit.<br>\n",
    "Now let's recreate the model we used before but this time with dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLinearDO(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[10], dropout=0.5):\n",
    "        super(StackedLinearDO, self).__init__()\n",
    "        sizes = [input_size] + hidden_sizes\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(sizes[i], sizes[i + 1]), nn.ReLU(), nn.Dropout(p=dropout)\n",
    "                )\n",
    "            )\n",
    "        layers.append(nn.Linear(sizes[-1], output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ The value that we pass in as dropout is the probability of each neurons output is being zeroed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackedLinearDO(input_size, output_size, [50, 50], dropout=0.2).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, epochs=20, bs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are lucky, we should be able to get a better accuracy compared to random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Try changing the value of dropout and number of layers and their size and observe how they will effect the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "As we discussed a regression is a problem where the target is a continuous value. For a neural netwrok the classification and regression problems are almost the same. the main difference is usually in the loss function we use. While for classification Cross Entropy is a common loss, for regression we will use Mean Square Errors in the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to use well logs dataset again. But for the sake of practice we will switch the target from the lithology to one of the measurements, in this case Bulk Density (RHOB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"../../data/processed/geolink_norge_dataset/geolink_norge_well_logs_train.parquet\"\n",
    ").set_index([\"Well\"])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to go through the same preprocessing steps as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler_rho = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[\"LITHOLOGY_GEOLINK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [\"CALI\", \"GR\", \"DTC\", \"RDEP\", \"RMED\", \"DEPT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example now we face an interesting problem. We have categorical values as input. Note that neural networks are a series of mathematical operations; therefore cannot deal with non-numerical variables.<br>\n",
    "But actually we converted the classes into values using a label encoder, so we should be alright? The problem with using the encoder output is that they are indices not values. They don't have mathematical significance. When index of one type is $4$ and the other is $2$, it doesn't mean some characteristics of the first type has twice the value of the second. We need a better way of dealing with categorical values. One common method is One Hot Encoding. One hot encoding adds a column to features for each type. The value of one of the columns will be one depending on the type.<br>\n",
    "Let's understand this through an example. Let's say the data has feature that only has three types __A__, __B__, and __C__. By one hot encoding we replace the column with three columns. Each column will contain either one or zero. \n",
    "- The first column would be one if the type is __A__ otherwise zero.\n",
    "- The second column would be one if the type is __B__ otherwise zero.\n",
    "- The third column would be one if the type is __C__ otherwise zero.\n",
    "\n",
    "Therefore, if the type was A, the value for the one hot encoded columns would be 1, 0, and , 0.<br>\n",
    "This method despite being useful, has some drawbacks. One issue is that when you have multiple classes you need to add many columns to data which makes processing more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Embedding\n",
    "The flexibility of neural networks allows us to use a technique much better that one hot encoding. This method is called feature embedding. In this method we create a table which assigns a number of features to each class. Then, when the data is passed through the network, instead of the categorical variables their features from the embedding table will be used. What is intersting about this method is that we do not know the values in the table when we start the training process. They will be learned during the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have one categorical variable, and the rest are numerical, we can separate the inputs into a categorical inputs and numerical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnum_train = scaler.fit_transform(df.loc[training_wells, feat_cols].values)\n",
    "xcat_train = encoder.transform(df.loc[training_wells, \"LITHOLOGY_GEOLINK\"])\n",
    "y_train = scaler_rho.fit_transform(df.loc[training_wells, [\"RHOB\"]].values)\n",
    "\n",
    "xnum_test = scaler.transform(df.loc[test_wells, feat_cols].values)\n",
    "xcat_test = encoder.transform(df.loc[test_wells, \"LITHOLOGY_GEOLINK\"])\n",
    "y_test = scaler_rho.transform(df.loc[test_wells, [\"RHOB\"]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we need to create a new model with embedding tables for categorical variables. We need to specify how many types there are in our categorical columns (num_classes), and how many values will be used to represent each type (emb_vec_size). Then, in the model we create a stacked linear layers with dropout (as defined before) using the right number of inputs (numerical inputs + embedding features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEmbed(nn.Module):\n",
    "    def __init__(self, hidden_sizes, num_classes, emb_vec_size, dropout=0.5):\n",
    "        super(FeatureEmbed, self).__init__()\n",
    "        self.emb = nn.Embedding(num_classes, emb_vec_size)\n",
    "        self.net = StackedLinearDO(\n",
    "            input_size=6 + emb_vec_size,\n",
    "            output_size=1,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, xnum, xcat):\n",
    "        x1 = self.emb(xcat)\n",
    "        x_in = torch.cat((x1, xnum), dim=1)\n",
    "        return self.net(x_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 2\n",
    "dropout = 0.3\n",
    "model = FeatureEmbed(\n",
    "    hidden_sizes=[50, 50],\n",
    "    num_classes=num_classes,\n",
    "    emb_vec_size=vec_size,\n",
    "    dropout=dropout,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnum_train = torch.Tensor(xnum_train).to(device)\n",
    "xcat_train = torch.LongTensor(xcat_train).to(device)\n",
    "y_train = torch.Tensor(y_train).to(device)\n",
    "\n",
    "xnum_test = torch.Tensor(xnum_test).to(device)\n",
    "xcat_test = torch.LongTensor(xcat_test).to(device)\n",
    "y_test = torch.Tensor(y_test).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Mean Square Error as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a training loop. The training loop is slightly different, since we have a numerical input and a categorical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emb_model(model, epochs=10, bs=256):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = []\n",
    "        for i in range(0, len(x_train), bs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xnum_train[i : i + bs, :], xcat_train[i : i + bs])\n",
    "            loss = loss_func(preds, y_train[i : i + bs])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "            if i % 1000 == 0:\n",
    "                print('Loss: {:.3f}'.format(loss.item()), end=\"\\r\", flush=True)\n",
    "        preds = model(xnum_test, xcat_test)\n",
    "        loss = loss_func(preds, y_test)\n",
    "        print('Epoch#{} Test Loss = {:.3f}'.format(epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb_model(model, epochs=10, bs=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use $R^2$ as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rsquare(model, xnum, xcat, y):\n",
    "    m = model.to(\"cpu\")\n",
    "    m.eval()\n",
    "    preds = m(xnum.to(\"cpu\"), xcat.to(\"cpu\"))\n",
    "    preds = preds.detach().numpy()\n",
    "    return metrics.r2_score(y.to(\"cpu\").numpy(), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquare(model, xnum_train, xcat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquare(model, xnum_test, xcat_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained, which means we should be able to have a look at embedding table and see the values in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.emb.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "cmap = cm.get_cmap(\"brg\", num_classes)\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(num_classes):\n",
    "    x = embed[i, 0]\n",
    "    y = embed[i, 1]\n",
    "    color = cmap(i)\n",
    "    litho = encoder.inverse_transform([i])\n",
    "    plt.scatter(x, y, color=color)\n",
    "    plt.text(x + 0.05, y - 0.05, litho[0], color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the two values we used for each type. If the model is trained well, we should be able to see types with closer characteristics should be placed closer to each other in the plot above. This is another interesting feature of embedding tables. It allows us to not only to convert categorical variables to numericals, but also we can see the behaviour of each class and compare them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try increasing the number of embedding features to $5$ and see how it would affect $R^2$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Solutions  \n",
    "<details><summary>See solutions</summary>\n",
    "\n",
    "<details><summary>Exercise 1</summary>\n",
    "<b>No weight decay</b>\n",
    "\n",
    "```Python\n",
    "model = StackedLinear(input_size, output_size, [50, 50]).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0)\n",
    "train_model(model, epochs=10, bs=256)\n",
    "accuracy(model, x_test, y_test)\n",
    "```\n",
    "<b>Large weight decay</b>\n",
    "```Python\n",
    "model = StackedLinear(input_size, output_size, [50, 50]).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "train_model(model, epochs=10, bs=256)\n",
    "accuracy(model, x_test, y_test)\n",
    "```\n",
    "\n",
    "</details>\n",
    "<details><summary>Exercise 2</summary>\n",
    "    <b>Larger dropout</b>\n",
    "    \n",
    "```Python\n",
    "model = StackedLinearDO(input_size, output_size, [50, 50], dropout=0.6).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "train_model(model, epochs=20, bs=256)\n",
    "accuracy(model, x_test, y_test)\n",
    "```\n",
    "\n",
    "    <b>More layers</b>\n",
    "    \n",
    "```Python\n",
    "model = StackedLinearDO(input_size, output_size, [50, 50, 50], dropout=0.2).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "train_model(model, epochs=20, bs=256)\n",
    "accuracy(model, x_test, y_test)\n",
    "```\n",
    "    \n",
    "    <b>Larger Layers</b>\n",
    "    \n",
    "```Python\n",
    "model = StackedLinearDO(input_size, output_size, [100, 100], dropout=0.2).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "train_model(model, epochs=20, bs=256)\n",
    "accuracy(model, x_test, y_test)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "<details><summary>Exercise 3</summary>\n",
    "\n",
    "```Python\n",
    "vec_size = 5\n",
    "dropout = 0.3\n",
    "model = FeatureEmbed(\n",
    "    hidden_sizes=[50, 50],\n",
    "    num_classes=num_classes,\n",
    "    emb_vec_size=vec_size,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "train_emb_model(model, epochs=10, bs=256)\n",
    "Rsquare(model, xnum_test, xcat_test, y_test)\n",
    "```\n",
    "\n",
    "</details>\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "- [Regularisation](https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd)\n",
    "- [Dropout in (Deep) Machine learning](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5#:~:text=According%20to%20Wikipedia%20%E2%80%94,which%20is%20chosen%20at%20random.)\n",
    "- [Why One-Hot Encode Data in Machine Learning?](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)\n",
    "- [Neural Network Embeddings Explained](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
