{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:10:58.797160Z",
     "start_time": "2020-07-14T11:10:58.791454Z"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will learn about evaluation metrics and different techniques to optimised Machine Learning models.\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "- [0. Packages](#0)\n",
    "- [1. Evaluation Metrics](#1)\n",
    "    - [1.1 Regression Metrics](#1.1)\n",
    "        - [Mean Absolute Error (MAE)](#1.1.1)\n",
    "        - [Mean Squared Error (MSE)](#1.1.2)\n",
    "        - [Root Mean Squared Error (RMSE)](#1.1.3)\n",
    "    - [1.2 Classification Metrics](#1.2)\n",
    "        - [True Positives, True Negatives, False Positives, False Negatives](#1.2.1)\n",
    "        - [Accuracy, Recall & Precision](#1.2.2)\n",
    "        - [Confusion Matrix](#1.2.3)\n",
    "        - [AUC and ROC curves](#1.2.4)\n",
    "- [2. Hyperparameter Optimisation](#2)\n",
    "- [3. Cross Validation](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:12:24.941132Z",
     "start_time": "2020-07-14T11:12:24.934546Z"
    }
   },
   "source": [
    "# 0. Packages <a name=\"0\"></a>\n",
    "\n",
    "In this session, we will make use of the following packages:\n",
    "- [numpy](https://docs.scipy.org/doc/numpy/) is a popular library for scientific computing.\n",
    "- [matplotlib](https://matplotlib.org/3.1.1/contents.html) is a plotting library compatible with numpy.\n",
    "- [pandas](https://pandas.pydata.org/docs/) is what we'll use to manipulate our data.\n",
    "- [sklearn](https://scikit-learn.org/stable/index.html) will be used to measure the performance of our model.\n",
    "\n",
    "Run the next cell to import the necessary packages mentioned before. Besides, we will add more packages as needed while progressing in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:09.886472Z",
     "start_time": "2020-07-16T12:02:09.098444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Good practice to use short but clear aliases for the imported libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# Magic Function\n",
    "%matplotlib inline\n",
    "# Hide all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluation Metrics and Scoring <a name=\"1\"></a>\n",
    "\n",
    "In Machine Learning (ML), evaluating the trained models is one of the most important task in the model creation cycle. \n",
    "\n",
    "A typical workflow for supervised learning looks like this:\n",
    "\n",
    "- 1. Select a model with an initial configuration, train the model and then predict on unseem data (Test Dataset a.k.a `X_test` and `y_test`).\n",
    "- 2. Compare the target `y_test` with the results from `y_predicted`. Then measure the performance using an evaluation metrics.\n",
    "- 3. The hyperparameters of the models are finetuned to reach the optimal for the evaluation metric.\n",
    "\n",
    "\n",
    "Important to know:\n",
    "\n",
    "- Evaluating a model is an important step to build effective ML models.\n",
    "- There are many different evaluation metrics with some pro's and con's. Some popular metrics are `Accuracy`, `Confusion Matrix`, `AUC and ROC Curves`, and `Mean Squared Error`. \n",
    "- Depending on the ML task, there are different metrics for different ML tasks. For example, `Mean Squared Error` is usually used for regression problems, where as `Confusion Matrix` for classification. \n",
    "\n",
    "\n",
    "In supervised learning, we can perform `classification` and `regression` tasks. In a nutshell, the tasks of `classification` corresponds to the task of assigning classes or labels to the sampled data. For example, the problem of classifiying images of handwritting digits from 0 to 9 will have 10 classes. On the other hand, regression, tries to predict continous values. For example, predicting the price of a house given the neighborhood, number of rooms, etc. (Example: [Kaggle Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques))\n",
    "\n",
    "In this section, we will focus on `classification` and `regression` metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T10:56:22.985101Z",
     "start_time": "2020-07-16T10:56:22.979583Z"
    }
   },
   "source": [
    "<a name='1-1'></a>\n",
    "## 1.1 Regression Metrics\n",
    "\n",
    "To evaluate the performance on regression problems, we will use metrics that meausure the distance between the target values and the predictions made by our model. It is common to hear that a regression metric is called `loss` or `error` functions. Depending on the context, some loss functions are refered with different names. For example, Mean Absolute Error is often called `L1 loss` or `Manhattan distance` in academic papers.\n",
    "\n",
    "In a regression problem, the objective is to minimize the error, cost or loss function.\n",
    "\n",
    "For the regression examples, suppose that we already trained a model and we got `y` and the predictions for `y_pred`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:15.452389Z",
     "start_time": "2020-07-16T12:02:15.445447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([98.62768289, 87.33919458, 50.9745525 , 27.18357143, 33.69187277,\n",
       "       21.6954265 , 27.64771432, 34.33155928, 86.21589355, 15.669967  ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a seed for random numbers\n",
    "np.random.seed(2020)\n",
    "# Generate random numbers \n",
    "y = np.random.uniform(low=0.0, high=100.0, size=(10,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:21.212777Z",
     "start_time": "2020-07-16T12:02:21.207738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90, 85, 45, 34, 33, 22, 28, 33, 89, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred = np.array([90,85,45,34,33,22,28,33,89,16])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1-1'></a>\n",
    "## Mean Absolute Error (L1 loss)\n",
    "\n",
    "According to [wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_error):\n",
    "\n",
    "> In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as:\n",
    "\n",
    "\n",
    "$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|$\n",
    "\n",
    "Let's break the equation:\n",
    "\n",
    "- $\\frac{1}{n_{\\text{samples}}}$: Divide everything by the total number of data points.\n",
    "- $y$: Target output values\n",
    "- $\\hat{y}$: Predicted output values\n",
    "- $\\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|$: Sum of the absolute value of the difference between $y$ and $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:26.501002Z",
     "start_time": "2020-07-16T12:02:26.497278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9552289225711506\n"
     ]
    }
   ],
   "source": [
    "# Let's implement MAE using numpy\n",
    "def MAE(y, y_pred):   \n",
    "    n_samples = len(y)\n",
    "    mean_abs_err = np.sum(np.abs(y-y_pred))/n_samples\n",
    "    return mean_abs_err\n",
    "\n",
    "print(MAE(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:28.590521Z",
     "start_time": "2020-07-16T12:02:28.562717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9552289225711506"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-learn has an implementation for MAE\n",
    "\n",
    "# Let's import it first\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T11:17:59.720252Z",
     "start_time": "2020-07-16T11:17:59.710521Z"
    }
   },
   "source": [
    "<a name='1-1-2'></a>\n",
    "## Mean Squared Error (L2 loss, also called Euclidean Distance)\n",
    "\n",
    "Mean Squared Error is probably one of the most popular and used metrics for regression tasks. As it squares the differences, it penalizes more large errors. Being a differentiable function, it can be optimised better.\n",
    "\n",
    "$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:31.374154Z",
     "start_time": "2020-07-16T12:02:31.369664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.239649808666712\n"
     ]
    }
   ],
   "source": [
    "# Let's implement MSE using numpy\n",
    "def MSE(y, y_pred):   \n",
    "    n_samples = len(y)\n",
    "    mean_sq_err = np.sum((y-y_pred)**2)/n_samples\n",
    "    return mean_sq_err\n",
    "\n",
    "print(MSE(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:32.929241Z",
     "start_time": "2020-07-16T12:02:32.924106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.239649808666712"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-learn has an implementation for MSE as well\n",
    "# Let's import it first\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)\n",
    "\n",
    "$\\text{RMSE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:38.497349Z",
     "start_time": "2020-07-16T12:02:38.492879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.152065727883738\n"
     ]
    }
   ],
   "source": [
    "# Let's implement MAE using numpy\n",
    "def RMSE(y, y_pred):   \n",
    "    n_samples = len(y)\n",
    "    mean_abs_err = np.sqrt(np.sum((y-y_pred)**2)/n_samples)\n",
    "    return mean_abs_err\n",
    "\n",
    "print(RMSE(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:02:41.142004Z",
     "start_time": "2020-07-16T12:02:41.137173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.152065727883738"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-learn has an implementation for MSE as well\n",
    "# Let's import it first MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# We could do something like this for this implementation\n",
    "np.sqrt(mean_squared_error(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T11:30:21.125648Z",
     "start_time": "2020-07-16T11:30:21.120661Z"
    }
   },
   "source": [
    "There are many implementations of regression metrics in Scikit-learn. Each metric has its advantages and drawbacks.\n",
    "\n",
    "For a completed list of metrics check the official API:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T11:38:38.032897Z",
     "start_time": "2020-07-14T11:38:38.023231Z"
    }
   },
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 Classification Metrics\n",
    "<a name='1-2-1'></a>\n",
    "### True Positives, False Positives, True Negatives, and False Negatives\n",
    "\n",
    "The most basic statistics to compute from the model predictions are the true positives, true negatives, false positives and false negatives:\n",
    "\n",
    "- true positive (TP): The model classifies the example as positive, and the actual label also positive.\n",
    "- false positive (FP): The model classifies the example as positive, **but** the actual label is negative.\n",
    "- true negative (TN): The model classifies the example as negative, and the actual label is also negative.\n",
    "- false negative (FN): The model classifies the example as negative, **but** the label is actually positive.\n",
    "\n",
    "We will count the number of TP, FP, TN and FN in the given data.  All of our metrics can be built off of these four statistics. \n",
    "\n",
    "Recall that the model outputs real numbers between 0 and 1.\n",
    "* To compute binary class predictions, we need to convert these to either 0 or 1. \n",
    "* We'll do this using a threshold value $th$.\n",
    "* Any model outputs above $th$ are set to 1, and below $th$ are set to 0. \n",
    "\n",
    "All of the metrics studies in this section (except for ROC-AUC Curves) will depend on the choice of this threshold. \n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1\n",
    "\n",
    "The first two metrics, `TP` and `FP` are implemented below. Based on that code, create two more functions for `TN` and `TP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:03:07.872266Z",
     "start_time": "2020-07-16T12:03:07.864627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's set the threshold as 0.5 by default\n",
    "def true_positives(y, pred, th=0.5):\n",
    "    TP = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = (pred > th).astype(int) # astype(int) converts an array of booleans to integers\n",
    "    # compute True Positives. Use numpy to calculate TP  \n",
    "    TP = np.sum((y == 1) & (th_preds == 1))\n",
    "    return TP\n",
    "\n",
    "def true_negatives(y, pred, th=0.5):\n",
    "    TN = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = (pred > th).astype(int) # astype(int) converts an array of booleans to integers\n",
    "    # compute True Negatives. Use numpy to calculate TN\n",
    "    TN = np.sum((y == 0) & (th_preds == 0))\n",
    "    return TN\n",
    "\n",
    "def false_positives(y, pred, th=0.5):\n",
    "    FP = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = pred > th\n",
    "    # COMPLETE CODE HERE\n",
    "    return FP\n",
    "\n",
    "def false_negatives(y, pred, th=0.5):\n",
    "    FN = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = pred > th\n",
    "    # COMPLETE CODE HERE\n",
    "    return FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 1\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 1</b></font>\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "    def false_positives(y, pred, th=0.5):\n",
    "        FP = 0\n",
    "        # get thresholded predictions\n",
    "        th_preds = (pred > th).astype(int)\n",
    "        # compute FP\n",
    "        FP = np.sum(((y == 0) & (th_preds == 1)).astype(int))\n",
    "        return FP\n",
    "\n",
    "    def false_negatives(y, pred, th=0.5):\n",
    "        FN = 0\n",
    "        # get thresholded predictions\n",
    "        th_preds = (pred > th).astype(int)\n",
    "        FN = np.sum(((y == 1) & (th_preds == 0)).astype(int))\n",
    "        return FN\n",
    " ```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T09:24:19.781972Z",
     "start_time": "2020-07-15T09:24:19.770566Z"
    }
   },
   "source": [
    "## <a name='1-2-2'></a>\n",
    "## Accuracy, Precision, Recall\n",
    "\n",
    "From the basic statistics we can calculate metrics such as accuracy, precision and recall.\n",
    "\n",
    "A few definitions from Wikipedia below:\n",
    "\n",
    "- **Accuracy**: Accuracy is the closeness of the measurements to a specific value (Source: [Wikipedia](https://en.wikipedia.org/wiki/Accuracy_and_precision)). \n",
    "\n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}$\n",
    "\n",
    "Accuracy is one of the most popular metrics for classification. However, accuracy alone might not be the best metric to benchmark the performance of a ML model.\n",
    "\n",
    "\n",
    "In the following scenario, imagine that you trained an ML binary classification model and you got $y$ and $\\hat{y}$ like this. You also want calculate a threshold of 0,5:\n",
    "\n",
    "**Model 1:**\n",
    "\n",
    "<table>\n",
    "<tr><th>Original </th><th>With Threshold 0,5</th></tr>\n",
    "<tr><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: | \n",
    "| 0 | 0,2 |\n",
    "| 1 | 0,6 |\n",
    "| 0 | 0,44 |\n",
    "| 1 | 0,8 |\n",
    "| 1 | 0,75 |\n",
    "| 0 | 0,55 |\n",
    "| 0 | 0,25 |\n",
    "| 1 | 0,67 |\n",
    "| 1 | 0,32 |\n",
    "| 1 | 0,15 |\n",
    "    \n",
    "</td><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: | \n",
    "| 0 | 0 |\n",
    "| 1 | 1 |\n",
    "| 0 | 0 |\n",
    "| 1 | 1 |\n",
    "| 1 | 1 |\n",
    "| 0 | 1 |\n",
    "| 0 | 0 |\n",
    "| 1 | 1 |\n",
    "| 1 | 0 |\n",
    "| 1 | 0 |\n",
    "\n",
    "</td></tr></table>\n",
    "\n",
    "From those results, let's calculate TP, TN, FP, FN:\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkgreen\"><b>Help...</b></font>\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "    TP = np.sum(((y == 1) & (th_preds == 1)).astype(int))\n",
    "    TN = np.sum(((y == 0) & (th_preds == 0)).astype(int))\n",
    "    FP = np.sum(((y == 0) & (th_preds == 1)).astype(int))\n",
    "    FN = np.sum(((y == 1) & (th_preds == 0)).astype(int))\n",
    " ```\n",
    "</details>\n",
    "\n",
    "- $\\mathrm{TP = 4}$\n",
    "- $\\mathrm{TN = 3}$\n",
    "- $\\mathrm{FP = 1}$\n",
    "- $\\mathrm{FN = 2}$\n",
    "\n",
    "To calculate accuracy:\n",
    "\n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}$\n",
    "\n",
    "\n",
    "So in our case:\n",
    "\n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {4} +\\mathrm {3} }{\\mathrm {4} +\\mathrm {3} +\\mathrm {1} +\\mathrm {2} }} = {\\frac{7}{10}} = 0,7 $\n",
    "\n",
    "So we got an accuracy of 0,7. No bad right? Now let's compare with the next example:\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Imagine that you have trained another ML algorithm and you get the results below:\n",
    "1. Calculate results with a threshold of 0,5\n",
    "2. Calculate TP, TN, FP, FN\n",
    "3. Calculate accuracy\n",
    "\n",
    "**Model 2:**\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr><th>Original </th><th>With Threshold 0,5</th></tr>\n",
    "<tr><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 1 | 0,01 |\n",
    "| 1 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "    \n",
    "</td><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 1 | 0 |\n",
    "| 1 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "\n",
    "</td></tr></table>\n",
    "\n",
    "You can click in the button below the reveal the solution for exercise 2\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 2</b></font>\n",
    "</summary>\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr><th>Original </th><th>With Threshold 0,5</th><th>Results</th></tr>\n",
    "<tr><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 1 | 0,01 |\n",
    "| 1 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "    \n",
    "</td><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: | \n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 1 | 0 |\n",
    "| 1 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "\n",
    "</td><td>\n",
    "    \n",
    "- $TP=0$\n",
    "- $TN=8$\n",
    "- $FP=0$\n",
    "- $FN=2$    \n",
    "    \n",
    "    \n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {0} +\\mathrm {8} }{\\mathrm {0} +\\mathrm {8} +\\mathrm {0} +\\mathrm {2} }} = {\\frac{8}{10}} = 0,8 $\n",
    "    \n",
    "</td></tr> </table>\n",
    "    \n",
    "**IMPORTANT (READ THIS):** <br/>\n",
    "So our accuracy is 80% now ! Does this mean that our second model is better?\n",
    "    \n",
    "Actually, our first intuition would tell us that the first model is better, even if we are getting a better accuracy from the second model. Why is that?\n",
    "    \n",
    "If we check the results for the second model, we will notice that the second model always predicts 0.01. This is not a very useful model and that's why we should not rely only in the accuracy metric.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk now about recall and precision.\n",
    "\n",
    "Some few definitions from Wikipedia below:\n",
    "\n",
    "- **recall**: measures the proportion of actual positives that are correctly identified as such (Source: [Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)). <br/>\n",
    "$\\displaystyle \\mathrm {Recall} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN}}}$\n",
    "\n",
    "- **precision**: are the proportions of positive results that are true positives (Source: [Wikipedia](https://en.wikipedia.org/wiki/Accuracy_and_precision)).  <br/>\n",
    "$\\displaystyle \\mathrm {Precision} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP}}}$\n",
    "\n",
    "\n",
    "From the last exercise we had:\n",
    "\n",
    "- $TP=0$\n",
    "- $TN=8$\n",
    "- $FP=0$\n",
    "- $FN=2$    \n",
    "    \n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {0} +\\mathrm {8} }{\\mathrm {0} +\\mathrm {8} +\\mathrm {0} +\\mathrm {2} }} = {\\frac{8}{10}} = 0,8 $\n",
    "\n",
    "Now, let's calculate the Recall and Precision for the same example:\n",
    "\n",
    "$\\displaystyle \\mathrm {Recall} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN}}}={\\frac {\\mathrm {0}}{\\mathrm {0} +\\mathrm {2}}}=0$\n",
    "    \n",
    "$\\displaystyle \\mathrm {Precision} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP}}}={\\frac {\\mathrm {0} }{\\mathrm {0} +\\mathrm {0}}}=$Error (Do not divide by zero)\n",
    "\n",
    "**Note:** We can modify the formula and add a small value $\\epsilon$ to avoid problems of division by zero.\n",
    "Then, the new formula could be:\n",
    "\n",
    "$\\displaystyle \\mathrm {Precision} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} + \\epsilon}}$\n",
    "\n",
    "where $\\epsilon$ is equal to small number such as 0,000001\n",
    "\n",
    "Then:\n",
    "\n",
    "$\\displaystyle \\mathrm {Precision} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} + \\epsilon}}={\\frac {\\mathrm {0} }{\\mathrm {0} +\\mathrm {0} + 0,000001}}=0$\n",
    "\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "We already calculated the `precision` and `recall` for the second model of our example. We noticed that in both cases we got a value of zero. Despite of having an accuracy of 80%, a precision and recall of 0 is very suspicious. This would usually mean that there is something wrong either with our model or the test data set we are using to evaluate.\n",
    "\n",
    "- Calculate the precision and recall for the first model\n",
    "- Compare how those accuracy, precision and recall differ between model 1 and model 2 \n",
    "\n",
    "Model 1:\n",
    "<table>\n",
    "<tr><th>Original </th><th>With Threshold 0,5</th></tr>\n",
    "<tr><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: | \n",
    "| 0 | 0,2 |\n",
    "| 1 | 0,6 |\n",
    "| 0 | 0,44 |\n",
    "| 1 | 0,8 |\n",
    "| 1 | 0,75 |\n",
    "| 0 | 0,55 |\n",
    "| 0 | 0,25 |\n",
    "| 1 | 0,67 |\n",
    "| 1 | 0,32 |\n",
    "| 1 | 0,15 |\n",
    "    \n",
    "</td><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: | \n",
    "| 0 | 0 |\n",
    "| 1 | 1 |\n",
    "| 0 | 0 |\n",
    "| 1 | 1 |\n",
    "| 1 | 1 |\n",
    "| 0 | 1 |\n",
    "| 0 | 0 |\n",
    "| 1 | 1 |\n",
    "| 1 | 0 |\n",
    "| 1 | 0 |\n",
    "\n",
    "</td></tr></table>\n",
    "\n",
    "Model 2:\n",
    "<table>\n",
    "<tr><th>Original </th><th>With Threshold 0,5</th></tr>\n",
    "<tr><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 1 | 0,01 |\n",
    "| 1 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "| 0 | 0,01 |\n",
    "    \n",
    "</td><td>\n",
    "    \n",
    "| $y$ | $\\hat{y}$ |\n",
    "| :-: | :-: |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 1 | 0 |\n",
    "| 1 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "| 0 | 0 |\n",
    "\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 1\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 3</b></font>\n",
    "</summary>\n",
    "    \n",
    "**Model 1:**\n",
    "    \n",
    "- $\\mathrm{TP = 4}$\n",
    "- $\\mathrm{TN = 3}$\n",
    "- $\\mathrm{FP = 1}$\n",
    "- $\\mathrm{FN = 2}$\n",
    "\n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {4} +\\mathrm {3} }{\\mathrm {4} +\\mathrm {3} +\\mathrm {1} +\\mathrm {2} }} = {\\frac{7}{10}} = 0,7 $\n",
    "    \n",
    "$\\displaystyle \\mathrm {Recall} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN}}}={\\frac {\\mathrm {4}}{\\mathrm {4} +\\mathrm {2}}}=\\frac {\\mathrm {4}}{\\mathrm {6}} = 0.67$\n",
    "    \n",
    "$\\displaystyle \\mathrm {Precision} ={\\frac {\\mathrm {TP}}{\\mathrm {TP} +\\mathrm {FP}}}={\\frac{\\mathrm {4}}{\\mathrm {4} +\\mathrm {1}}}=\\frac {\\mathrm {4}}{\\mathrm {5}} = 0.8$\n",
    "    \n",
    "**Model 2:**\n",
    "    \n",
    "- $\\mathrm{TP = 0}$\n",
    "- $\\mathrm{TN = 8}$\n",
    "- $\\mathrm{FP = 0}$\n",
    "- $\\mathrm{FN = 2}$\n",
    "    \n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {0} +\\mathrm {8} }{\\mathrm {0} +\\mathrm {8} +\\mathrm {0} +\\mathrm {2} }} = {\\frac{8}{10}} = 0,8 $\n",
    "    \n",
    "$\\displaystyle \\mathrm {Recall} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} +\\epsilon}}={\\frac {\\mathrm {0}}{\\mathrm {0} +\\mathrm {2}+\\epsilon}}=\\frac {\\mathrm {0}}{\\mathrm {2}+\\epsilon} = 0$\n",
    "    \n",
    "$\\displaystyle \\mathrm {Precision} ={\\frac {\\mathrm {TP}}{\\mathrm {TP} +\\mathrm {FP}+\\epsilon}}={\\frac{\\mathrm {0}}{\\mathrm {0} +\\mathrm {1}+\\epsilon}}=\\frac {\\mathrm {0}}{\\mathrm {0}+\\epsilon} = 0$\n",
    "    \n",
    "</details>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics in Python\n",
    "Let's implement the metrics for accuracy, recall, and precision in Python.\n",
    "\n",
    "But first, let's check again the functions to calculate TP,TN,FP and FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:24:30.981575Z",
     "start_time": "2020-07-16T12:24:30.973007Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you implemented correctly the functions in exercise 1. It should look like this code below:\n",
    "# Let's set the threshold as 0.5 by default\n",
    "def true_positives(y, pred, th=0.5):\n",
    "    TP = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = (pred > th).astype(int) # astype(int) will convert an array of booleans to integers\n",
    "    # compute True Positives. Use numpy to calculate TP  \n",
    "    TP = np.sum(((y == 1) & (th_preds == 1)).astype(int))\n",
    "    return TP\n",
    "\n",
    "def true_negatives(y, pred, th=0.5):\n",
    "    TN = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = (pred > th).astype(int)\n",
    "    # compute True Negatives. Use numpy to calculate TN\n",
    "    TN = np.sum(((y == 0) & (th_preds == 0)).astype(int))\n",
    "    return TN\n",
    "\n",
    "def false_positives(y, pred, th=0.5):\n",
    "    FP = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = (pred > th).astype(int)\n",
    "    # compute FP\n",
    "    FP = np.sum(((y == 0) & (th_preds == 1)).astype(int))\n",
    "    return FP\n",
    "\n",
    "def false_negatives(y, pred, th=0.5):\n",
    "    FN = 0\n",
    "    # get thresholded predictions\n",
    "    th_preds = (pred > th).astype(int)\n",
    "    FN = np.sum(((y == 1) & (th_preds == 0)).astype(int))\n",
    "    return FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the accuracy is defined by:\n",
    "\n",
    "$\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:39:37.873525Z",
     "start_time": "2020-07-16T12:39:37.869211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implementation of Accuracy in Python\n",
    "def Accuracy(y,y_hat):\n",
    "    TP = true_positives(y,y_hat)\n",
    "    TN = true_negatives(y,y_hat)\n",
    "    FP = false_positives(y,y_hat)\n",
    "    FN = false_negatives(y,y_hat)\n",
    "    \n",
    "    return (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Implement the metrics for precision and recall in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 4</b></font>\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "# Implementation of Recall in Python\n",
    "def recall(y,y_hat):\n",
    "    TP = true_positives(y,y_hat)\n",
    "    FN = false_negatives(y,y_hat)\n",
    "\n",
    "    return (TP)/(TP+FN)\n",
    "\n",
    "print(recall(y,y_hat))\n",
    "# Output: 0.666666666\n",
    "# Implementation of Precision in Python\n",
    "def precision(y,y_hat):\n",
    "    TP = true_positives(y,y_hat)\n",
    "    FP = false_positives(y,y_hat)\n",
    "\n",
    "    return (TP)/(TP+FP)\n",
    "\n",
    "print(precision(y,y_hat))\n",
    "# Output: 0.8\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and F1-Score\n",
    "\n",
    "> In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix,[8] is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).[9] The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).\n",
    "\n",
    "Confusion Matrices is a useful tool that we could use while evaluation classification models. Scikit-learn has an implementation of this.\n",
    "\n",
    "\n",
    "Let's check an example of the Confusion Matrix using the data for the **model 1:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:43:57.313519Z",
     "start_time": "2020-07-16T12:43:57.307796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import module\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "c_matrix = cm(y, (y_hat > 0.5).astype(int))\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:45:24.204440Z",
     "start_time": "2020-07-16T12:45:24.198823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 2 4\n"
     ]
    }
   ],
   "source": [
    "# You can use ravel to extract the statistics TP,TN,FP,FN\n",
    "tn, fp, fn, tp = cm(y, (y_hat > 0.5).astype(int)).ravel()\n",
    "print(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T12:52:23.519780Z",
     "start_time": "2020-07-16T12:52:23.259389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's plot the confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "c_matrix = cm(y, (y_hat > 0.5).astype(int))\n",
    "f = sns.heatmap(c_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
