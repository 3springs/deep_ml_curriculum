{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook provides examples of some machine learning methods used in supervised learning. Please refer to the table below to navigate through the notebook.\n",
    "\n",
    "## Table of Content\n",
    "0. [Supervised Learning](#supervised)\n",
    "1. [Libraries](#libraries)\n",
    "2. [Dataset](#dataset)\n",
    "1. [KNN](#knn)\n",
    "2. [SVM](#svm)\n",
    "3. [Decision Trees](#decision-trees)\n",
    "4. [Random Forest](#random-forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Supervised Learning <a name=\"supervised\"></a>\n",
    "\n",
    "Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs [1]. It infers a function from labelled training data consisting of a set of training examples [2].\n",
    "\n",
    "![ML](ml2.png)\n",
    "![ML](ml1.png)\n",
    "\n",
    "Sources:\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Supervised_learning#:~:text=Supervised%20learning%20is%20the%20machine,a%20set%20of%20training%20examples.)\n",
    "-  1. Stuart J. Russell, Peter Norvig (2010) Artificial Intelligence: A Modern Approach, Third Edition, Prentice Hall ISBN 9780136042594.\n",
    "-  2. Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012) Foundations of Machine Learning, The MIT Press ISBN 9780262018258."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries <a name=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "#Classifiers\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "# Hide all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. About the Geolink Dataset <a name=\"dataset\"></a>\n",
    "\n",
    "This is a well dataset of Well's Petroleum wells drilled on the Norwegian continental shelf. It was released by the Norweigen Government, cleaned by Geolink, and loaded by LukasMosser. The full dataset has 221 wells, over 150 lithologies, with around 500 MB of data.\n",
    "\n",
    "\n",
    "You don't need to understand the data fully for this course, but here's a brief overview\n",
    "\n",
    "- Well - The well name\n",
    "- DEPT - Depth below the ground in meters\n",
    "- LITHOLOGY_GEOLINK - This is the facies or lithology, which means rock type. This is a label, made by multiple human experts by looking at the context, the well, maps of the area, and often picture of rock samples extracted from the well.\n",
    "- [Well logs](https://en.wikipedia.org/wiki/Well_logging): These are specialised measurements by instruments lowered down the well hole\n",
    "    - CALI - [Caliper log](https://en.wikipedia.org/wiki/Caliper_log), this measures the size of the well bore\n",
    "    - GR - [Gamma Ray](https://en.wikipedia.org/wiki/Gamma_ray_logging): Measure passive amount of high energy electromagnetic radiation naturally emitted from the rock\n",
    "    - RHOB - [Bulk Density](https://en.wikipedia.org/wiki/Density_logging): Measured active amount high energy electromagnetic radiation. This has a transmitting source of gamma rays\n",
    "    - DTC - [Compressional wave](https://en.wikipedia.org/wiki/Longitudinal_wave) travel time: This measure the how long a compressional wave takes to travel through the formationation\n",
    "    - RDEP - [Resistivity](https://en.wikipedia.org/wiki/Resistivity_logging) Deep: Electrical resistivity through the rock with a deep penetration\n",
    "    - RMED - Resistivity Medium: Electrical resistivity through the rock with a nedium penetration\n",
    "    - *Many other well logs were removed as they were not present in all wells*\n",
    "    \n",
    "Interpreting lithology from well logs is a very hard problem for machine learning because:\n",
    "\n",
    "- It's usually done by expert humans (Petrophysicists) with years to decades of experience, not an random human\n",
    "- it takes into account context in the form of prior knowledge, geology, nearby wells, rock samples, and many more. Many of these are forms of information the machine doesn't have access to\n",
    "- The data is unbalanced with important rocks like sandstone sometimes appearing as very this layers\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img width=\"480\" src=\"../../reports/figures/30-4_1.png\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "<img width=\"320\" src=\"../../data/processed/geolink_norge_dataset/location of geolink wells.png\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Data Disclaimer\n",
    "\n",
    "All the data serving as an input to these notebooks was generously donated by GEOLINK  \n",
    "and is CC-by-SA 4.0 \n",
    "\n",
    "If you use this data please reference the dataset properly to give them credit for their contribution.\n",
    "\n",
    "**Note:** download data from https://drive.google.com/drive/folders/1EgDN57LDuvlZAwr5-eHWB5CTJ7K9HpDP\n",
    "\n",
    "Credit to this repo: https://github.com/LukasMosser/geolink_dataset\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The geolink dataset we will use in this notebook has been preprocessed. You can find the process of preparation of this dataset in [Data Preparation](../z00_Data_prep/00-mc-prep_geolink_norge_dataset.ipynb)\n",
    "\n",
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_locations = Path(\"../../data/processed/geolink_norge_dataset/\")\n",
    "# Load processed dataset\n",
    "geolink = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    ").set_index([\"Well\", \"DEPT\"])\n",
    "# Add Depth as column\n",
    "geolink['DEPT'] = geolink.index.get_level_values(1)\n",
    "\n",
    "# Work with one well\n",
    "geolink = geolink.xs(\"30_4-1\")\n",
    "geolink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolink['LITHOLOGY_GEOLINK'].astype('category')\n",
    "geolink['LITHOLOGY_GEOLINK'] = geolink['LITHOLOGY_GEOLINK'].values.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_ml_curriculum.visualization.well_log import plot_facies, plot_well\n",
    "plot_well(\"30_4-1\", geolink, facies=geolink['LITHOLOGY_GEOLINK'].astype('category').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only take the data from CALI  onward for X.\n",
    "X = geolink.iloc[:, 1:]\n",
    "# LITHOLOGY_GEOLINK will be our class y.\n",
    "y = geolink[\"LITHOLOGY_GEOLINK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Nearest Neighbors (KNN) <a name=\"knn\"></a>\n",
    "**Example of decision boundaries using KNN:**\n",
    "<br/>\n",
    "<div>\n",
    "    Original Dataset. The dataset which consists of three classes (red,green and blue points). (For KNN classification example.)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Data3classes.png\" width=300 height=300/>\n",
    "    Decision Boundaries using 1NN algorithm\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/Map1NN.png\" width=300 height=300/>\n",
    "</div>\n",
    "\n",
    "Images sources: [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#/media/File:Data3classes.png)\n",
    " distributed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) license.\n",
    "          \n",
    "The code presented in this section is inspired from the official documentation [here](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py).\n",
    "\n",
    "According to Wikipedia:\n",
    ">k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, normalizing the training data can improve its accuracy dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get types of Lithology\n",
    "classes = list(geolink[\"LITHOLOGY_GEOLINK\"].unique())\n",
    "print('Classes: {}'.format(classes))\n",
    "print('Total Classes: {}'.format(len(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the classes\n",
    "y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a total of 11 classes. However, like in the majority of machine learning algorithms, it is recommended to encode target values.\n",
    "\n",
    "\n",
    "There are different encoder preprocessing functions available in the scikit learn library. For this example we will use the [<code>LabelEncoder</code>](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html):\n",
    "\n",
    "> Encode target labels with a value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y, and not the input X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# This will help the LabelEncoder to map the classes to a corresponding value between 0 and n_classes-1\n",
    "le.fit(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of `neighbors` is a hyperparameter that must be set for this algorithm. We will arbitrarily select a value of 15 for this hyperparameter.\n",
    "\n",
    "> In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)#:~:text=In%20machine%20learning%2C%20a%20hyperparameter,weights%20are%20derived%20via%20training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Neighbors around our datapoint to be classified\n",
    "n_neighbors = 15\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors)\n",
    "# Now we will use the y values and transform the labels.\n",
    "transformed_y_train = le.transform(y_train.to_numpy())\n",
    "print(transformed_y_train)\n",
    "# Let's fit the data\n",
    "knn_classifier.fit(X_train, transformed_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Time\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "y_true = le.transform(y_test.to_numpy())\n",
    "print('Accuracy: {}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to predict the 11 different classes with 93.2% accuracy using new data. This is slightly better than random, however the accuracy is still very low. Let's train using the same algorithm and hyperparameters but this time we will normalise the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalised Data\n",
    "\n",
    "\n",
    "### Why normalise the data?\n",
    "\n",
    "Many machine learning (ML) algorithms attempt to find trends in the data by comparing the features of data points. However, machine learning algorithms usually struggle more in the training phase when the features are on different scales. Normalise the data will almost always improve the results for ML algorithms.\n",
    "\n",
    "There are different ways to normalised the data and usually, each method has pros and cons. For example, some methods are better at dealing with outliers than others.\n",
    "\n",
    "Two of the most popular methods for normalising the data are:\n",
    "\n",
    "* Standard Score:\n",
    "$\\frac {X-\\mu }{\\sigma }$\n",
    "\n",
    "* Min-Max Feature Scaling:\n",
    "$X'={\\frac {X-X_{\\min }}{X_{\\max }-X_{\\min }}}$\n",
    "\n",
    "More information about normalisation [here](https://en.wikipedia.org/wiki/Normalization_(statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Standard Score for this example\n",
    "normalized_df = (X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the original max values of the dataset\n",
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the original max values after normalising\n",
    "normalized_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's used the normalized data for both for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    normalized_df, y.to_numpy(), test_size=0.2, random_state=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# This will help the LabelEncoder to map the classes to a corresponding value between 0 and n_classes-1\n",
    "le.fit(y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized knn\n",
    "knn_classifier_norm = KNeighborsClassifier(n_neighbors)\n",
    "# Now we will use the y values and transform the labels.\n",
    "transformed_y_train = le.transform(y_train)\n",
    "# Let's fit the data\n",
    "knn_classifier_norm.fit(X_train, transformed_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Time\n",
    "y_pred = knn_classifier_norm.predict(X_test)\n",
    "y_true = le.transform(y_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we got an accuracy of 90%. Normalising will usually help in the training process. So it's a good practice to preprocess the data before the training phase. However, some machine learning algorithms such as KNN are robust enough to work well with different scales so normalization might not be necessary in some cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So... What if we change the hyperparameters of a classifier such as the number of `neighbors`?\n",
    "\n",
    "We can see in the examples below how changing <code>n_neighbors</code> affect the performance of the final model.\n",
    "\n",
    "The accuracy for the same normalised model are:\n",
    "\n",
    "- n_neighbors = 10, accuracy: 90.9%\n",
    "- n_neighbors = 5,  accuracy: 91.6%\n",
    "- n_neighbors = 1,  accuracy: 92.2%\n",
    "We could  manually try different values until we find the best hyperparameters. Of course, this approach could be very time consuming, in particular, when we have a big feature space and want to optimise many hyperparameters. One way to solve this problem is automating the search of hyperparameters. This is also called Hyperparameter Optimisation. We will get deeper into this concept in the next sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 10\n",
    "# Normalized knn\n",
    "knn_classifier_norm = KNeighborsClassifier(n_neighbors)\n",
    "# Now we will use the y values and transform the labels.\n",
    "transformed_y_train = le.transform(y_train)\n",
    "# Let's fit the data\n",
    "knn_classifier_norm.fit(X_train, transformed_y_train)\n",
    "# Evaluation Time\n",
    "y_pred = knn_classifier_norm.predict(X_test)\n",
    "y_true = le.transform(y_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5\n",
    "# Normalized knn\n",
    "knn_classifier_norm = KNeighborsClassifier(n_neighbors)\n",
    "# Now we will use the y values and transform the labels.\n",
    "transformed_y_train = le.transform(y_train)\n",
    "# Let's fit the data\n",
    "knn_classifier_norm.fit(X_train, transformed_y_train)\n",
    "# Evaluation Time\n",
    "y_pred = knn_classifier_norm.predict(X_test)\n",
    "y_true = le.transform(y_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 1\n",
    "# Normalized knn\n",
    "knn_classifier_norm = KNeighborsClassifier(n_neighbors)\n",
    "# Now we will use the y values and transform the labels.\n",
    "transformed_y_train = le.transform(y_train)\n",
    "# Let's fit the data\n",
    "knn_classifier_norm.fit(X_train, transformed_y_train)\n",
    "# Evaluation Time\n",
    "y_pred = knn_classifier_norm.predict(X_test)\n",
    "y_true = le.transform(y_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <h2>Exercise 1</h2>\n",
    "\n",
    "  Let's practice some of the key concepts we have learned so far. \n",
    "    \n",
    "  0. Choose the data for well log '30_6-14'. \n",
    "  1. Normalised and split the dataset provided (Use 85\\% of the data for training and 15\\% for testing)\n",
    "  2. Transform the data using label encoding (This is done automatically by scikit-learn for some ML methods. However, it is good to get used to this concept).\n",
    "  3. Train a KNN model using <code>n_neighbors</code>= 1,5,10,15\n",
    "  4. Compare the accuracy of the different models\n",
    "\n",
    "  <details>\n",
    "  <summary><b>â†’ Hints</b></summary>\n",
    "   - In this notebook we have already covered all the steps necessary to complete this exercise. You need to load again the dataset but selecting this time the well log '30_6-14'. Copy and paste the code and modify it as needed.\n",
    "      \n",
    "  </details>\n",
    "\n",
    "  <br/>\n",
    "  <br/>\n",
    "  <details>\n",
    "  <summary>\n",
    "    <b>â†’ Solution</b>\n",
    "  </summary>\n",
    "    \n",
    "  ```python\n",
    "    # Exercise\n",
    "\n",
    "    # Write your code below:\n",
    "\n",
    "    # 0. Select a different well\n",
    "    interim_locations = Path(\"../../data/processed/geolink_norge_dataset/\")\n",
    "    # Load processed dataset\n",
    "    geolink = pd.read_parquet(\n",
    "        interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    "    ).set_index([\"Well\", \"DEPT\"])\n",
    "    # Add Depth as column\n",
    "    geolink['DEPT'] = geolink.index.get_level_values(1)\n",
    "    # Work with one well\n",
    "    geolink = geolink.xs('30_6-14')\n",
    "\n",
    "    # 1. Normalise dataset and Split well log dataset here\n",
    "\n",
    "    # We only take the data from CALI  onward for X.\n",
    "    X = geolink.iloc[:, 1:]\n",
    "    # LITHOLOGY_GEOLINK will be our class y.\n",
    "    y = geolink[\"LITHOLOGY_GEOLINK\"]\n",
    "    # Normalize data\n",
    "    normalized_X = (X - X.mean()) / X.std()\n",
    "    # Let's used the normalized data for both for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        normalized_X, y.to_numpy(), test_size=0.15, random_state=1\n",
    "    )\n",
    "\n",
    "    # 2. Transform the data using label encoding\n",
    "\n",
    "    # Label Encoder\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    classes = list(geolink[\"LITHOLOGY_GEOLINK\"].unique())\n",
    "    # This will help the LabelEncoder to map the classes to a corresponding value between 0 and n_classes-1\n",
    "    le.fit(classes)\n",
    "    # Now we will use the y values and transform the labels.\n",
    "    transformed_y_train = le.transform(y_train)\n",
    "    y_test_true = le.transform(y_test)\n",
    "    # 3. Train your models 15NN\n",
    "\n",
    "    # Note: In this example code we are just showing 15NN\n",
    "    # Number of Neighbors around our datapoint to be classified\n",
    "\n",
    "    # 1NN\n",
    "    n_neighbors = 1\n",
    "    cls_1nn = KNeighborsClassifier(n_neighbors)\n",
    "    # Let's fit the data\n",
    "    cls_1nn .fit(X_train, transformed_y_train)\n",
    "\n",
    "    # 5NN\n",
    "    n_neighbors = 5\n",
    "    cls_5nn = KNeighborsClassifier(n_neighbors)\n",
    "    # Let's fit the data\n",
    "    cls_5nn .fit(X_train, transformed_y_train)\n",
    "\n",
    "    # 10NN\n",
    "    n_neighbors = 10\n",
    "    cls_10nn = KNeighborsClassifier(n_neighbors)\n",
    "    # Let's fit the data\n",
    "    cls_10nn .fit(X_train, transformed_y_train)\n",
    "\n",
    "    # 15NN\n",
    "    n_neighbors = 15\n",
    "    cls_15nn = KNeighborsClassifier(n_neighbors)\n",
    "    # Let's fit the data\n",
    "    cls_15nn .fit(X_train, transformed_y_train)\n",
    "\n",
    "    # 4. Compare the accuracy of your models\n",
    "\n",
    "    # Evaluation Time\n",
    "    y_pred = cls_1nn.predict(X_test)\n",
    "    print(f\"Accuracy 1NN: {accuracy_score(y_test_true, y_pred)}\")\n",
    "\n",
    "    # Evaluation Time\n",
    "    y_pred = cls_5nn.predict(X_test)\n",
    "    print(f\"Accuracy 5NN: {accuracy_score(y_test_true, y_pred)}\")\n",
    "\n",
    "    # Evaluation Time\n",
    "    y_pred = cls_10nn.predict(X_test)\n",
    "    print(f\"Accuracy 10NN: {accuracy_score(y_test_true, y_pred)}\")\n",
    "\n",
    "    # Evaluation Time\n",
    "    y_pred = cls_15nn.predict(X_test)\n",
    "    print(f\"Accuracy 15NN: {accuracy_score(y_test_true, y_pred)}\")     \n",
    "  ```\n",
    "\n",
    "  </details>\n",
    "\n",
    "  </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "\n",
    "# Write your code below:\n",
    "\n",
    "# 0. Select a different well\n",
    "\n",
    "# 1. Normalise dataset and Split well log dataset here\n",
    "\n",
    "# 2. Transform the data using label encoding\n",
    "\n",
    "# 3. Train your models 1NN, 5NN, 10NN, 15NN\n",
    "\n",
    "# 4. Compare the accuracy of your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Support Vector Machines (SVM) <a name=\"svm\"></a>\n",
    "\n",
    "**A bit of history:**\n",
    "\n",
    "> The Support Vector Machine (SVM) algorithm is a popular machine learning tool that offers solutions for both classification and regression problems. Developed at AT&T Bell Laboratories by Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Vapnik et al., 1997), it presents one of the most robust prediction methods, based on the statistical learning framework.\n",
    "\n",
    "**How does it works?**\n",
    "> An SVM performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. You can use an SVM when your data has exactly two classes, e.g. binary classification problems\n",
    "\n",
    "\n",
    "The graphic below shows how a support vector machine would choose a separating hyperplane for two classes of points in 2D. H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximum margin.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/1024px-Svm_separating_hyperplanes_%28SVG%29.svg.png\" width=300 height=300/>\n",
    "\n",
    "[Source Image](https://en.wikipedia.org/wiki/Support_vector_machine#/media/File:Svm_separating_hyperplanes_(SVG).svg)\n",
    "\n",
    "One of the limitations of SVMs is that they inherently do binary classification. However, there are different methods to extend SVMs and use them in multiclass problems. The most common methods involve transforming the problem into a set of binary classification problems, by one of two strategies:\n",
    "\n",
    "- One vs. the rest. For ð‘˜ classes, ð‘˜ binary classifiers are trained. Each determines whether an example belongs to its 'own' class versus any other class. The classifier with the largest output is taken to be the class of the example.\n",
    "- One vs. one. A binary classifier is trained for each pair of classes. A voting procedure is used to combine the outputs.\n",
    "\n",
    "Sources: [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine), [SVMs](https://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html), [Multiclass SVM](https://stats.stackexchange.com/questions/215342/can-a-linear-svm-only-have-2-classes)\n",
    "\n",
    "\n",
    "**Scikit-learn Implementation:**\n",
    "\n",
    "Multiclass and Multilabel algorithms:\n",
    "\n",
    "> **Warning:** All classifiers in scikit-learn do multiclass classification out-of-the-box. You donâ€™t need to use the sklearn.multiclass module unless you want to experiment with different multiclass strategies.\n",
    "\n",
    "More information about Multiclass classification in scikit-learn [here](https://scikit-learn.org/stable/modules/multiclass.html)\n",
    "\n",
    "The SVC implementation is not efficient for large datasets. Instead we will use the LinearSVC implementation. Find more information in the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "\n",
    "> The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and might be impractical beyond tens of thousands of samples. For large datasets consider using sklearn.svm.LinearSVC or sklearn.linear_model.SGDClassifier instead, possibly after a sklearn.kernel_approximation.Nystroem transformer.\n",
    "\n",
    "Sometimes, we will want to train and test our algorithm in a sample fo the data instead of the completed dataset. Some ML methods do not deal well with large datasets so taking a sample from the dataset would be preferable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_locations = Path(\"../../data/processed/geolink_norge_dataset/\")\n",
    "# Load processed dataset\n",
    "geolink = pd.read_parquet(\n",
    "    interim_locations / \"geolink_norge_well_logs_train.parquet\"\n",
    ").set_index([\"Well\", \"DEPT\"])\n",
    "# Add Depth as column\n",
    "geolink['DEPT'] = geolink.index.get_level_values(1)\n",
    "\n",
    "# Work with one well\n",
    "geolink = geolink.xs(\"30_4-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total rows in Geolink dataset for the selected well:\", len(geolink))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms will take significantly more time to train than others. In the case of SVMs the process of training can get slower as the number of data points is increased.\n",
    "\n",
    "Note: For the example below, we will sample the data to get only 10,000 data points from the original geolink dataset instead of 22,921. This reduction in datapoints may affect the accuracy of the model. But it will be done for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case, we will take a sample of 10,000 data points from one well\n",
    "sample_dataset = geolink.sample(n=10000, replace=False, random_state=2020)\n",
    "\n",
    "X_sample = sample_dataset.iloc[:, 1:]\n",
    "y_sample = sample_dataset[\"LITHOLOGY_GEOLINK\"]\n",
    "# Normalise data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_sample)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_sample, test_size=0.3, random_state=2020\n",
    ")\n",
    "\n",
    "clf = LinearSVC(\n",
    "    penalty=\"l2\",\n",
    "    loss=\"squared_hinge\",\n",
    "    dual=True,\n",
    "    tol=0.0001,\n",
    "    C=100,\n",
    "    multi_class=\"ovr\",\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    class_weight=None,\n",
    "    verbose=0,\n",
    "    random_state=2020,\n",
    "    max_iter=10000,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "categories = y_sample.unique()\n",
    "# Evaluation Time\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = pd.Categorical(y_pred, categories=categories)\n",
    "y_true = y_test\n",
    "print('Accuracy: {}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results as well logs\n",
    "X_sample = geolink.iloc[:, 1:]\n",
    "y_true = geolink[\"LITHOLOGY_GEOLINK\"]\n",
    "X_scaled = scaler.fit_transform(X_sample)\n",
    "y_pred = clf.predict(X_scaled)\n",
    "\n",
    "true = pd.Categorical(y_true)\n",
    "pred = pd.Categorical(y_pred, categories=true.categories)\n",
    "\n",
    "f, ax = plt.subplots(nrows=1, ncols=2, figsize=(4, 12))\n",
    "plot_facies(pred, ax=ax[0], colorbar=False, xlabel='Pred')\n",
    "plot_facies(true, ax=ax[1], xlabel='True')\n",
    "ax[0].set_xticklabels([])\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results could be better with a deeper network and more data. We will revit this again in the Recurrent Neural Networks notebook.\n",
    "\n",
    "We could also:\n",
    "- give it a sequence of a logs, so it can see the context up an down the well\n",
    "- give it the X and Y location of the well\n",
    "- look at more logs, while using the GPU\n",
    "- provide some human examples for each well\n",
    "- project a geological model or geological maps onto the wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Decision Trees <a name=\"decision-trees\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. \n",
    "Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.\n",
    "\n",
    "Some **advantages** of using decision trees are:\n",
    "\n",
    "- Simple to understand and interpret: People can understand decision tree models after a brief explanation. Trees can also be displayed graphically in a way that is easy for non-experts to interpret.\n",
    "\n",
    "- Able to handle both numerical and categorical data. [1]\n",
    "\n",
    "- Requires little data preparation: Other techniques often require data normalization. Since trees can handle qualitative predictors, there is no need to create dummy variables. [1]\n",
    "\n",
    "- Performs well with large datasets. Large amounts of data can be analyzed using standard computing resources in a reasonable time.\n",
    "\n",
    "**Limitations:** \n",
    "- Trees can be very non-robust. A small change in the training data can result in a large change in the tree and consequently the final predictions. [1]\n",
    "- Decision-tree learners can create over-complex trees that do not generalize well from the training data.\n",
    "\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "\n",
    "Source: [1] Gareth, James; Witten, Daniela; Hastie, Trevor; Tibshirani, Robert (2015). An Introduction to Statistical Learning. New York: Springer. pp. 315. ISBN 978-1-4614-7137-0.\n",
    "\n",
    "**Note:** Decision trees can be used for classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example, we will train a decision tree on the iris dataset.\n",
    "\n",
    "We can use the function <code>plot_tree</code> to show the flowchart of the trained model. Find more information about plotting the decision surface of a decision tree [here](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py). As can be seen in the next plot, the maximum [depth](http://typeocaml.com/2014/11/26/height-depth-and-level-of-a-tree/) of the tree is 5. Usually decision trees with more levels (deeper trees) will be able to discriminate better and take more finetuned decisions, however, deeper trees are also likely to [overfit](https://en.wikipedia.org/wiki/Overfitting) easily. \n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "\n",
    "**NOTE:** <br>\n",
    "\n",
    "Use the hyperparameters `max_depth` and `max_features` carefully. Incresing those parameters might increase the [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) of the model but it is also likely to overfit the models at some point.\n",
    "    <br/>\n",
    "Using the correct metrics of evaluation is key to correctly assess the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset geolink. test_size =0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=2020\n",
    ")\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "# X0 and y0 from iris_dataset\n",
    "tree.plot_tree(clf.fit(X_train, y_train), max_depth=3, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a decision tree (DT) with our geolink dataset. For the next example, we will set the hyperparater max_depth to 100. Notice how the accuracy in the model changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=2020)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation Time\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 93%~ just with the default hyperparameters. Let's train the DT again with a different hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = tree.DecisionTreeClassifier(random_state=2020, max_depth=10)\n",
    "clf2 = clf2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation Time\n",
    "y_pred = clf2.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got now an accuracy of 92%~ just with the hyperparameter max_depth=10. Let's train the DT again with a different hyperparameter. In appearance, the first model would be better, however, there are other metrics besides accuracy that should be taken into account. There are also other methods to avoid overfitting. We will go deeper into this topic in the next sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Random Forest <a name=\"random-forest\"></a>\n",
    "\n",
    "> A random forest is a meta estimator that fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "\n",
    "Random Forest is an ensemble method:\n",
    ">The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm to improve generalizability/robustness over a single estimator.\n",
    "\n",
    "We will learn more about ensemble methods in the next session.\n",
    "\n",
    "Scikit-learn support Random Forest for classification and regression (<code>RandomForestClassifier</code> and <code>RandomForestRegressor</code>)\n",
    "\n",
    "> In random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features. (See the parameter tuning guidelines for more details). \n",
    "The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yields decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice, the variance reduction is often significant hence yielding an overall better model.\n",
    "\n",
    "This implementation of RandomForest only accepts numerical values. You should always encode categorical values.\n",
    "\n",
    "Source: [Official Documentation Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "**Note:** The next 2 pieces of code might take several minutes to complete the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train again the model with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=100, n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and further reading\n",
    "The following sources have been used in the creation of this notebook:\n",
    "- [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree)\n",
    "- [SVMs](https://nlp.stanford.edu/IR-book/html/htmledition/multiclass-svms-1.html)\n",
    "- [Normalisation](https://en.wikipedia.org/wiki/Normalization_(statistics))\n",
    "- [Seaborn Datasets](https://seaborn.pydata.org/generated/seaborn.load_dataset.html)\n",
    "- [Scikit-learn Datasets](https://scikit-learn.org/stable/datasets/index.html)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "deep_ml_curriculum",
   "language": "python",
   "name": "deep_ml_curriculum"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
