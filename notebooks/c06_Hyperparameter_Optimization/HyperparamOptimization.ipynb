{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "Hyperparameters refer to those parameters used in training. Given that the optimization algorithms in a Neural Network such as `SGD` and `Adam` are stochastic in nature, it also means that a DNN trained with different hyperparameters might have different solutions (landing in a different local minima).\n",
    "\n",
    "Finding the best parameters possible for training our neural network is what we call `hyperparameter optimization`.\n",
    "\n",
    "One could just simply try different variations of hyperparameters manually. Fortunately, there are better ways of doing this. In this lesson, we will be using Pytorch to create our neural network and sklearn modules `sklearn.model_selection.GridSearchCV` and `sklearn.model_selection.RandomizedSearchCV` for the hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:45.890585Z",
     "start_time": "2020-10-12T05:32:45.000957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8e3b3a9db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:45.909164Z",
     "start_time": "2020-10-12T05:32:45.893935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"Name device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a pipeline of data transformations.\n",
    "\n",
    "For the train\n",
    "- We transform an Image to a Pytorch tensor\n",
    "- Normalize data using only one channel (gray scale image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:45.917050Z",
     "start_time": "2020-10-12T05:32:45.912884Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:45.926592Z",
     "start_time": "2020-10-12T05:32:45.919151Z"
    }
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    # For the data training dataset\n",
    "    \"train\": T.Compose([T.ToTensor(), T.Normalize((0.0,), (1.0,))]),\n",
    "    # For the validation dataset\n",
    "    \"val\": T.Compose([T.ToTensor(), T.Normalize((0.0,), (1.0,))]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a data loader for the training and testing dataset. `MNIST` is a dataset that contains images of handwritten digits from 0 to 9. We will train a classifier using hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:45.976068Z",
     "start_time": "2020-10-12T05:32:45.928821Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"../../data/processed/MNIST\", train=True, download=False, transform=data_transforms[\"train\"]\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"../../data/processed/MNIST\", train=False, download=False, transform=data_transforms[\"val\"]\n",
    "    ),\n",
    "    batch_size=batch_size + 64,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "\n",
    "`GridSearch` tries all possible combinations over specified parameter values for the model. We will implement a Grid Search method from scratch that allow us to try different parameters in our model and pick the best ones for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's create a CNN model. We will start by creating a function that returns a block of layers given some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:45.985654Z",
     "start_time": "2020-10-12T05:32:45.978381Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_block(input_dim, output_dim, kernel, padding, activation=\"relu\"):\n",
    "    activations = {\n",
    "        \"relu\": nn.ReLU(inplace=True),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"selu\": nn.SELU(inplace=True),\n",
    "        \"sigmoid\": nn.Sigmoid(),\n",
    "    }\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=output_dim,\n",
    "            kernel_size=kernel,\n",
    "            padding=padding,\n",
    "        ),\n",
    "        nn.BatchNorm2d(output_dim),\n",
    "        activations[activation],\n",
    "        nn.MaxPool2d(2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the architecture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:46.006848Z",
     "start_time": "2020-10-12T05:32:45.989875Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTCNN(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(MNISTCNN, self).__init__()\n",
    "\n",
    "        self.layer1 = create_block(1, 32, 3, 1, activation)\n",
    "        self.layer2 = create_block(32, 64, 3, 1, activation)\n",
    "        self.layer3 = create_block(64, 128, 3, 1, activation)\n",
    "        self.fc1 = nn.Linear(in_features=128 * 3 * 3, out_features=256)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a typical function for the training process in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:46.031035Z",
     "start_time": "2020-10-12T05:32:46.012668Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, n_epochs=1, bs=64, device=\"cpu\"):\n",
    "    # Set model in train mode\n",
    "    model.train().to(device)\n",
    "    running_loss = 0.0\n",
    "    for epoch in tqdm(range(n_epochs), desc='train', unit='epoch'):\n",
    "        for (x, y) in tqdm(dataloader, leave=False, desc='training'):\n",
    "            y = y.to(device)\n",
    "            x = x.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(x)  # Get the prediction here\n",
    "            loss = criterion(outputs, y)  # Calculate loss\n",
    "            loss.backward()  # Do backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        running_loss = 0.0\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:46.052711Z",
     "start_time": "2020-10-12T05:32:46.036067Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, device=\"cpu\"):\n",
    "    print(\"Evaluation...\")\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    for (x, y) in tqdm(dataloader, desc='testing', leave=False):\n",
    "\n",
    "        y = y.to(device)\n",
    "        x = x.to(device)\n",
    "        # Make prediction\n",
    "        pred = model(x).argmax(1)\n",
    "\n",
    "        # Count correct answers\n",
    "        correct += int(sum(y == pred))\n",
    "        # Count total number of images\n",
    "        total += pred.shape[0]\n",
    "\n",
    "    # Return accuracy metric\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a method that allow us to try different parameters such as the number of `epochs`, the `optimizer`, and the `activation functions` used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:46.086001Z",
     "start_time": "2020-10-12T05:32:46.056109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a criterion given a name\n",
    "def get_criterion(criterion_name):\n",
    "    criterions = {\n",
    "        \"cross_entropy\": nn.CrossEntropyLoss()\n",
    "        # You can add more criterions here if you need to\n",
    "    }\n",
    "\n",
    "    return criterions[criterion_name]\n",
    "\n",
    "\n",
    "# Get an optimizer given a name, model and learning rate\n",
    "def get_optimizer(optimizer_name, model, learning_rate=1e-3):\n",
    "    optimizers = {\n",
    "        \"sgd\": optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "        \"adam\": optim.Adam(model.parameters(), lr=learning_rate),\n",
    "        \"adagrad\": optim.Adagrad(model.parameters(), lr=learning_rate),\n",
    "    }\n",
    "\n",
    "    return optimizers[optimizer_name]\n",
    "\n",
    "\n",
    "def GridSearch(train_data, test_data, hyperparameters):\n",
    "    best_accuracy = 0\n",
    "    best_parameters = {\n",
    "        \"epoch\": 0,\n",
    "        \"activation\": None,\n",
    "        \"optimizer\": None,\n",
    "        \"learning_rate\": 0.0,\n",
    "    }\n",
    "    \n",
    "    possible_values = list(itertools.product(*hyperparameters.values()))\n",
    "    \n",
    "    for values in tqdm(possible_values, desc='gridsearch'):\n",
    "        hparams = dict(zip(hyperparameters.keys(), values))\n",
    "        \n",
    "        activation = hparams['activations']\n",
    "        epochs = hparams['epochs']\n",
    "        epochs = hparams['epochs']\n",
    "        opt = hparams['optimizers']\n",
    "        learning_rate = hparams['learning_rate']\n",
    "\n",
    "        model = MNISTCNN(activation).to(device)\n",
    "        criterion = get_criterion(\"cross_entropy\")\n",
    "        optimizer = get_optimizer(opt, model, learning_rate)\n",
    "        print(\n",
    "            'Training NN...optimizer:{}, activation:{}, epochs:{}, learning rate:{}'.format(opt, activation, epochs, learning_rate)\n",
    "        )\n",
    "        # Let's train the model using our custom hyperparameters\n",
    "        clf = train(\n",
    "            model,\n",
    "            train_data,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            n_epochs=epochs,\n",
    "            bs=64,\n",
    "            device=device,\n",
    "        )\n",
    "        accuracy = evaluation(clf, test_data, device)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            # Update parameters\n",
    "            best_accuracy = accuracy\n",
    "            best_parameters[\"epoch\"] = epochs\n",
    "            best_parameters[\"activation\"] = activation\n",
    "            best_parameters[\"optimizer\"] = opt\n",
    "            best_parameters[\"learning_rate\"] = learning_rate\n",
    "\n",
    "        print('Accuracy Testing: {}'.format(accuracy))\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b>Exercise 1</b>: <br>\n",
    "    \n",
    "**Warning:** GridSearch performs an exhaustive search with the parameters given. It might take around 12 minutes to complete the optimization in a GPU for the hyperparameters given below.\n",
    "    \n",
    "1. Pass the following hyperparameters to the function `GridSearch`.\n",
    "    \n",
    "```python\n",
    "hyperparameters = {\n",
    "    'epochs': [1],\n",
    "    'activations': ['selu','tanh','relu'],\n",
    "    'optimizers': ['adagrad','sgd','adam'],\n",
    "    'learning_rate': [1e-2, 1e-3]\n",
    "}\n",
    "```\n",
    "2. Execute the function and find the best hyperparameter for training.\n",
    "    \n",
    "Note it may take 15 minutes on a GPU\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 1\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 1</b></font>\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "hyperparameters = {\n",
    "    'epochs': [1],\n",
    "    'activations': ['selu','tanh','relu'],\n",
    "    'optimizers': ['adagrad','sgd','adam'],\n",
    "    'learning_rate': [1e-2, 1e-3]\n",
    "}\n",
    "\n",
    "import time\n",
    "# This line of code is to measure the executation time\n",
    "start_time = time.time()\n",
    "######################################################\n",
    "# YOU CODE GOES HERE\n",
    "best_params = GridSearch(train_loader, test_loader, hyperparameters)\n",
    "print(best_params)\n",
    "######################################################\n",
    "print(f'--- {(time.time() - start_time)/60.0} minutes ---')\n",
    "```\n",
    "\n",
    "    \n",
    "You should get this as a result:\n",
    "    \n",
    "```python\n",
    "{'epoch': 1,\n",
    " 'activation': 'relu',\n",
    " 'optimizer': 'sgd',\n",
    " 'learning_rate': 0.001} \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Grid Search\n",
    "\n",
    "As you can notice, GridSearch evaluates the model in combining every parameter. This of course, can be very inneficient. By contrast, Random Search sets up a grid of hyperparameter values and selects random combinations to train the model and evaluate. In random search you can control the number of hyperparameter combinations that attempted. It is possible that Random search does not find the best possible answer but in cases where the search space and computational time is limited it will work the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:32:46.109234Z",
     "start_time": "2020-10-12T05:32:46.093352Z"
    }
   },
   "outputs": [],
   "source": [
    "def RandomizedGridSearch(train_data, test_data, hyperparameters, num_combinations=5):\n",
    "    best_accuracy = 0\n",
    "    best_parameters = {\n",
    "        \"epoch\": 0,\n",
    "        \"activation\": None,\n",
    "        \"optimizer\": None,\n",
    "        \"learning_rate\": 0.0,\n",
    "    }\n",
    "\n",
    "    # Try different combinations\n",
    "    for i in tqdm(range(num_combinations), desc='RandomizedGridSearch'):\n",
    "        # Pick random hyperparameters\n",
    "        activation = np.random.choice(hyperparameters[\"activations\"])\n",
    "        model = MNISTCNN(activation).to(device)\n",
    "        epochs = np.random.choice(hyperparameters[\"epochs\"])\n",
    "        opt = np.random.choice(hyperparameters[\"optimizers\"])\n",
    "        l_rate = np.random.choice(hyperparameters[\"learning_rate\"])\n",
    "\n",
    "        # Select criterion and optimizer\n",
    "        criterion = get_criterion(\"cross_entropy\")\n",
    "        optimizer = get_optimizer(opt, model, l_rate)\n",
    "\n",
    "        print(\n",
    "            'Training NN...optimizer:{}, activation:{}, epochs:{}, learning rate:{}'.format(opt, activation, epochs, l_rate)\n",
    "        )\n",
    "        # Let's train the model using our custom hyperparameters\n",
    "        clf = train(\n",
    "            model,\n",
    "            train_data,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            n_epochs=epochs,\n",
    "            bs=64,\n",
    "            device=device,\n",
    "        )\n",
    "        # Evaluate\n",
    "        accuracy = evaluation(clf, test_data, device)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            # Update parameters\n",
    "            best_accuracy = accuracy\n",
    "            best_parameters[\"epoch\"] = epochs\n",
    "            best_parameters[\"activation\"] = activation\n",
    "            best_parameters[\"optimizer\"] = opt\n",
    "            best_parameters[\"learning_rate\"] = l_rate\n",
    "\n",
    "        print('Accuracy Testing: {}'.format(accuracy))\n",
    "\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercise we will increase the parameter space search but will limit to a number of 10 random searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b>Exercise 1</b>: <br>\n",
    "    \n",
    "**Warning:** Executing `RandomizedGridSearch` might take around 5 minutes on a GPU.\n",
    "    \n",
    "1. Pass the following hyperparameters to the function `RandomizedGridSearch`.\n",
    "    \n",
    "```python\n",
    "hyperparameters = {\n",
    "    'epochs': [1],\n",
    "    'activations': ['sigmoid','selu','tanh','relu'],\n",
    "    'optimizers': ['adagrad','sgd','adam'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "}\n",
    "```\n",
    "\n",
    "**NOTE:** If we were going to pass the same hyperparameter space to a `GridSearch` algorithm it would perform 108 combinations. This would take significantly more time.\n",
    "    \n",
    "2. Execute the function and find the best hyperparameter for training. Limit the random searches to 5.\n",
    "3. Measure the executation time of the code in minutes. **Hint:** Check solution for exercise 1.\n",
    "    \n",
    "Note it may take 5 minutes on a GPU\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 2\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 2</b></font>\n",
    "</summary>\n",
    "    \n",
    "```python\n",
    "hyperparameters = {\n",
    "    'epochs': [1],\n",
    "    'activations': ['sigmoid','selu','tanh','relu'],\n",
    "    'optimizers': ['adagrad','sgd','adam'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "# This line of code is to measure the executation time\n",
    "start_time = time.time()\n",
    "######################################################\n",
    "# YOU CODE GOES HERE\n",
    "best_params = RandomizedGridSearch(train_loader, test_loader, hyperparameters, num_combinations=5)\n",
    "print(best_params)\n",
    "######################################################\n",
    "print(f'--- {(time.time() - start_time)/60.0} minutes ---')\n",
    "```\n",
    "\n",
    "    \n",
    "Given the stochastic nature of the approach you might get different results everytime you execute this.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto:Test\n",
    "\n",
    "Ignore the below, it's for automatic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T05:33:15.365301Z",
     "start_time": "2020-10-12T05:32:46.112362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efecf6693a8a4ef29980ffabf6294ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='gridsearch', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN...optimizer:adagrad, activation:selu, epochs:1, learning rate:0.01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f178fd0a586404cac9c01bee3e0bece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d72640117b4b14967a5d4c0eefa5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=938.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f6d75db4c6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m######################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# YOU CODE GOES HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m######################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-06be29f76b71>\u001b[0m in \u001b[0;36mGridSearch\u001b[0;34m(train_data, test_data, hyperparameters)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d85b3695878e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, n_epochs, bs, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the prediction here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/jup3.7.3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'epochs': [1],\n",
    "    'activations': ['selu'],\n",
    "    'optimizers': ['adagrad'],\n",
    "    'learning_rate': [1e-2]\n",
    "}\n",
    "\n",
    "import time\n",
    "# This line of code is to measure the executation time\n",
    "start_time = time.time()\n",
    "######################################################\n",
    "# YOU CODE GOES HERE\n",
    "best_params = GridSearch(train_loader, test_loader, hyperparameters)\n",
    "print('best_params', best_params)\n",
    "######################################################\n",
    "print(f'--- {(time.time() - start_time)/60.0} minutes ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-12T05:33:16.142Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85be4dfc9134129905ed3b17b906a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='RandomizedGridSearch', max=1.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN...optimizer:adam, activation:selu, epochs:1, learning rate:0.001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545a4ae347e64fa9b762cf3d5833935e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='train', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb66b13e7b074f13a196f484cd4afed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training', max=938.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'epochs': [1],\n",
    "    'activations': ['sigmoid','selu','tanh','relu'],\n",
    "    'optimizers': ['adagrad','sgd','adam'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "# This line of code is to measure the executation time\n",
    "start_time = time.time()\n",
    "######################################################\n",
    "# YOU CODE GOES HERE\n",
    "best_params = RandomizedGridSearch(train_loader, test_loader, hyperparameters, num_combinations=1)\n",
    "print(best_params)\n",
    "######################################################\n",
    "print(f'--- {(time.time() - start_time)/60.0} minutes ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
