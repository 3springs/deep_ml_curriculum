{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dask\n",
    "There are many occasions when we have to work with datasets that are so big we can't just load all of it into memory. If we have to work with such a data, what is the solution? One of the libraries in python for this type of problems is __Dask__. Dask is a library for parallel computing. It helps us perform common pandas and numpy opperations on large datasets. In this tutorial we will learn about some of the features of Dask and how it can help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:13.885922Z",
     "start_time": "2020-10-01T05:26:12.853775Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassname/.pyenv/versions/jup3.7.3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "import logging\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the beginning to see the difference in the performance of Dask vs Pandas, let's read a 70 MB csv file with both libraries. Of course 70 MB is not considered a large file and we can easily fit it into memory, but it is large enough to see the advantage of using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:13.894060Z",
     "start_time": "2020-10-01T05:26:13.889817Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"../../data/processed/MNIST/train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also defining a function to report memory usage, so we can see how each method affect the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:13.904542Z",
     "start_time": "2020-10-01T05:26:13.896521Z"
    }
   },
   "outputs": [],
   "source": [
    "def memory_usage():\n",
    "    \"\"\"String with current memory usage in MB. Requires `psutil` package.\"\"\"\n",
    "    pid = os.getpid()\n",
    "    mem_bytes = psutil.Process(pid).memory_info().rss\n",
    "    print('[Process {} uses {:.1f}MB]'.format(pid, mem_bytes / 1024 / 1024))\n",
    "    return mem_bytes / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:13.919331Z",
     "start_time": "2020-10-01T05:26:13.908663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Process 3611408 uses 115.3MB]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "115.30078125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see the amount of used memory.<br>\n",
    "Now let's load the data with pandas and see how long it takes to load and how much memory it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:18.775970Z",
     "start_time": "2020-10-01T05:26:13.922759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.8 s, sys: 792 ms, total: 4.6 s\n",
      "Wall time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1 = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:18.785844Z",
     "start_time": "2020-10-01T05:26:18.779273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Process 3611408 uses 623.1MB]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "623.07421875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory usage has gone up by about 250 MB.<br>\n",
    "Now do the same with Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:19.074866Z",
     "start_time": "2020-10-01T05:26:18.788588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 261 ms, sys: 4.19 ms, total: 265 ms\n",
      "Wall time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df2=dd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:19.084088Z",
     "start_time": "2020-10-01T05:26:19.078759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Process 3611408 uses 623.1MB]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "623.07421875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask read the file in a raction of a second and used only about 3 MB of memory. How is that possible?<br> It's because Dask dosn't load the data into memory. The data is still on the disk. It only reads the data when it needs to perform calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the mean for the first 100 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:19.104515Z",
     "start_time": "2020-10-01T05:26:19.087411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.57 ms, sys: 0 ns, total: 9.57 ms\n",
      "Wall time: 8.87 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label       4.456643\n",
       "pixel0      0.000000\n",
       "pixel1      0.000000\n",
       "pixel2      0.000000\n",
       "pixel3      0.000000\n",
       "             ...    \n",
       "pixel94     3.768381\n",
       "pixel95     5.713881\n",
       "pixel96     7.751238\n",
       "pixel97    10.048857\n",
       "pixel98    12.067738\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df1.iloc[:,:100].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see pandas does the calculations in a fraction of a second. <br>\n",
    "Let's try Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:19.171342Z",
     "start_time": "2020-10-01T05:26:19.106698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.8 ms, sys: 158 Âµs, total: 60.9 ms\n",
      "Wall time: 60.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avg = df2.iloc[:,100:200].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is also did the operation very quickly. Let's have a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:19.182053Z",
     "start_time": "2020-10-01T05:26:19.173643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "pixel100    float64\n",
       "pixel99         ...\n",
       "dtype: float64\n",
       "Dask Name: dataframe-mean, 15 tasks"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not returning any numbers. What is happening?<br>\n",
    "The reason is Dask has not calculated the result yet. It only creates a dependency graph (also called Directed Acyclec Graph - DAG), which is basically how the calculations will take place. We need to execute the graph to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.552509Z",
     "start_time": "2020-10-01T05:26:19.185541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.38 s, sys: 783 ms, total: 5.17 s\n",
      "Wall time: 4.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pixel99     13.404952\n",
       "pixel100    13.072714\n",
       "pixel101    11.573810\n",
       "pixel102     9.295500\n",
       "pixel103     6.708333\n",
       "              ...    \n",
       "pixel194     0.345714\n",
       "pixel195     0.025024\n",
       "pixel196     0.000000\n",
       "pixel197     0.017810\n",
       "pixel198     0.112476\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "avg.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the results. Also, we can see that this step is the most time consuming step of all. It's because this is where Dask actually goes to disk and reads the data. If you add up the time for all the steps (reading the file and performing calculations) you will see that both take almost the same amount of time to do the operation, with pandas being slightly faster. This shows that Dask is not doing any magic. It's doing the same steps but it's doing it without using as much memory. However, Dask can perform operations in parallel using multiple cpu cores and even multiple machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned that Dask creates a dependency graph before doing the calculations. We can have a look at this graph and see how it is taking place:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>To see the graph you need a library called <b>GraphViz</b>. If this library is not installed on your system you will not be able to see the graph.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one we prepared earlier:\n",
    "\n",
    "![](img/dask_graphviz.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.958143Z",
     "start_time": "2020-10-01T05:26:23.555225Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Drawing dask graphs requires the `graphviz` python library and the `graphviz` system library to be installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/jup3.7.3/lib/python3.7/site-packages/dask/utils.py\u001b[0m in \u001b[0;36mimport_required\u001b[0;34m(mod_name, error_msg)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2b65b6670508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/jup3.7.3/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(self, filename, format, optimize_graph, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0moptimize_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimize_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/jup3.7.3/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \"\"\"\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdot_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filename\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mydask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/jup3.7.3/lib/python3.7/site-packages/dask/dot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m graphviz = import_required(\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"graphviz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m\"Drawing dask graphs requires the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"`graphviz` python library and the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"`graphviz` system library to be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/jup3.7.3/lib/python3.7/site-packages/dask/utils.py\u001b[0m in \u001b[0;36mimport_required\u001b[0;34m(mod_name, error_msg)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Drawing dask graphs requires the `graphviz` python library and the `graphviz` system library to be installed."
     ]
    }
   ],
   "source": [
    "avg.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a progress bar for each computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.962703Z",
     "start_time": "2020-10-01T05:26:12.925Z"
    }
   },
   "outputs": [],
   "source": [
    "avg = df2.iloc[:, 100:200].mean()\n",
    "task = avg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.964461Z",
     "start_time": "2020-10-01T05:26:12.932Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.966272Z",
     "start_time": "2020-10-01T05:26:12.938Z"
    }
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask dataframe is very similar to pandas dataframe. Even though the functionalities are limited but you will find many methods from pandas dataframe in Dask as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.968007Z",
     "start_time": "2020-10-01T05:26:12.943Z"
    }
   },
   "outputs": [],
   "source": [
    "task = df2.rolling(window=10).mean().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.969774Z",
     "start_time": "2020-10-01T05:26:12.947Z"
    }
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    ma_max = task.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.971389Z",
     "start_time": "2020-10-01T05:26:12.955Z"
    }
   },
   "outputs": [],
   "source": [
    "ma_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.973863Z",
     "start_time": "2020-10-01T05:26:12.959Z"
    }
   },
   "outputs": [],
   "source": [
    "task = df2[[\"label\", \"pixel100\", \"pixel300\", \"pixel500\"]].groupby(\"label\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.975407Z",
     "start_time": "2020-10-01T05:26:12.962Z"
    }
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    result = task.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.978401Z",
     "start_time": "2020-10-01T05:26:12.965Z"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use Dask dataframe of MNIST and follow these steps:\n",
    "1. Add a column to the dataframe which contains sum of all the pixels\n",
    "2. Use Groupby and find the average of __sum__ column for each number (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.980108Z",
     "start_time": "2020-10-01T05:26:12.969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```Python\n",
    "    df2['sum']=df2.values[:,1:].sum(axis=1)\n",
    "    task = df2[['label','sum']].groupby('label').mean()\n",
    "    with ProgressBar():\n",
    "        result=task.compute() \n",
    "    print(result)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array\n",
    "Dask is not just used to replace pandas. There are also multiple numpy functions which can be replaced by Dask. Dask array is Dask equivalent of a numpy array. By doing so, we can perform the computations in parallel and get the results faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.982076Z",
     "start_time": "2020-10-01T05:26:12.976Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.983513Z",
     "start_time": "2020-10-01T05:26:12.979Z"
    }
   },
   "outputs": [],
   "source": [
    "big_array = array.random.normal(size=(10000000, 100), chunks=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.984948Z",
     "start_time": "2020-10-01T05:26:12.984Z"
    }
   },
   "outputs": [],
   "source": [
    "big_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data takes 8 GB if we wanted to store it in RAM. But Dask only generates the numbers in chunks when it needs them. So at each steps it has to deal with a chunk which is ~160 MB in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.987520Z",
     "start_time": "2020-10-01T05:26:12.988Z"
    }
   },
   "outputs": [],
   "source": [
    "task = (big_array * big_array).mean(axis=1)\n",
    "with ProgressBar():\n",
    "    res = task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the chunk size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.989244Z",
     "start_time": "2020-10-01T05:26:12.993Z"
    }
   },
   "outputs": [],
   "source": [
    "big_array = array.random.normal(size=(10000000, 100), chunks=(2 ** 19, 100))\n",
    "big_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.991201Z",
     "start_time": "2020-10-01T05:26:12.997Z"
    }
   },
   "outputs": [],
   "source": [
    "task = (big_array * big_array).mean(axis=1)\n",
    "with ProgressBar():\n",
    "    res = task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply most of common numpy functions to the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.995539Z",
     "start_time": "2020-10-01T05:26:13.001Z"
    }
   },
   "outputs": [],
   "source": [
    "task = np.sin(big_array).mean(axis=0)\n",
    "with ProgressBar():\n",
    "    res = task.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.997045Z",
     "start_time": "2020-10-01T05:26:13.006Z"
    }
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Create two Dask random arrays of size 10,000,000-by-100. Find the difference between the two and pass it to `array.linalg.norm` using argument `axis=1`. Calculate the result and create a histogram of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:23.998540Z",
     "start_time": "2020-10-01T05:26:13.010Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.000074Z",
     "start_time": "2020-10-01T05:26:13.014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```Python\n",
    "    x1 = array.random.random(size=(10000000,100))\n",
    "    x2 = array.random.random(size=(10000000,100))\n",
    "    y = x2-x1\n",
    "    d = array.linalg.norm(y,axis=1)\n",
    "    with ProgressBar():\n",
    "        result = d.compute()\n",
    "    hist(result,bins=100);\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed\n",
    "Dask delayed is a method for parallelising code where you can't write your code directly as dataframe or array operation. `Dask.delayed` is an easy-to-use tool to quickly parallelise these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following functions. The first one takes an input, waits for one second and returns the value. The second function takes two inputs, waits for one second and returns the sum. We are using these functions to represent tasks that are time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.001449Z",
     "start_time": "2020-10-01T05:26:13.020Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "def task1(x):\n",
    "    sleep(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def task2(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we pass two values separately into the first function and then pass the results into the second function, we will have the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.002871Z",
     "start_time": "2020-10-01T05:26:13.025Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = task1(1)\n",
    "x2 = task1(2)\n",
    "y = task2(x1,x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each of these functions are taking one second; therefore, the entire block takes three seconds. But the calculation for `x1` is totally independent of the calculation for `x2`. If we were able to do these operation simultaneously we could save time. This is where `Dask.delayed` comes into play. We need to convert the functions into `delayed` functions so Dask can handle parallelisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.004370Z",
     "start_time": "2020-10-01T05:26:13.032Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_delayed = dask.delayed(task1)\n",
    "task2_delayed = dask.delayed(task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now insteam of the original function we use the delayed functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.006652Z",
     "start_time": "2020-10-01T05:26:13.036Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = task1_delayed(1)\n",
    "x2 = task1_delayed(2)\n",
    "y = task2_delayed(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.008177Z",
     "start_time": "2020-10-01T05:26:13.040Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we saved one second! `x1` and `x2` where calculated in parallel, and then `y` was calculated using `x1` and `x2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly create delayed functions using `dask.delayed` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.009488Z",
     "start_time": "2020-10-01T05:26:13.044Z"
    }
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def task1(x):\n",
    "    sleep(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def task2(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.010696Z",
     "start_time": "2020-10-01T05:26:13.048Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = task1(1)\n",
    "x2 = task1(2)\n",
    "y = task2(x1,x2)\n",
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Numba?\n",
    "\n",
    "Numba is a **just-in-time**, **type-specializing**, **function compiler** for accelerating **numerically-focused** Python.  That's a long list, so let's break down those terms:\n",
    "\n",
    " * **function compiler**: Numba compiles Python functions, not entire applications, and not parts of functions.  Numba does not replace your Python interpreter, but is just another Python module that can turn a function into a (usually) faster function. \n",
    " * **type-specializing**: Numba speeds up your function by generating a specialized implementation for the specific data types you are using.  Python functions are designed to operate on generic data types, which makes them very flexible, but also very slow.  In practice, you only will call a function with a small number of argument types, so Numba will generate a fast implementation for each set of types.\n",
    " * **just-in-time**: Numba translates functions when they are first called.  This ensures the compiler knows what argument types you will be using.  This also allows Numba to be used interactively in a Jupyter notebook just as easily as a traditional application\n",
    " * **numerically-focused**: Currently, Numba is focused on numerical data types, like `int`, `float`, and `complex`.  There is very limited string processing support, and many string use cases are not going to work well on the GPU.  To get best results with Numba, you will likely be using NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Steps\n",
    "\n",
    "Let's write our first Numba function and compile it for the **CPU**.  The Numba compiler is typically enabled by applying a *decorator* to a Python function.  Decorators are functions that transform Python functions.  Here we will use the CPU compilation decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.013479Z",
     "start_time": "2020-10-01T05:26:13.053Z"
    }
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import math\n",
    "\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    # Implementation from https://en.wikipedia.org/wiki/Hypot\n",
    "    x = abs(x)\n",
    "    y = abs(y)\n",
    "    t = min(x, y)\n",
    "    x = max(x, y)\n",
    "    t = t / x\n",
    "    return x * math.sqrt(1 + t * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is equivalent to writing:\n",
    "``` python\n",
    "def hypot(x, y):\n",
    "    x = abs(x);\n",
    "    y = abs(y);\n",
    "    t = min(x, y);\n",
    "    x = max(x, y);\n",
    "    t = t / x;\n",
    "    return x * math.sqrt(1+t*t)\n",
    "    \n",
    "hypot = jit(hypot)\n",
    "```\n",
    "This means that the Numba compiler is just a function you can call whenever you want!\n",
    "\n",
    "Let's try out our hypotenuse calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.015990Z",
     "start_time": "2020-10-01T05:26:13.059Z"
    }
   },
   "outputs": [],
   "source": [
    "hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we call `hypot`, the compiler is triggered and compiles a machine code implementation for float inputs.  Numba also saves the original Python implementation of the function in the `.py_func` attribute, so we can call the original Python code to make sure we get the same answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.017676Z",
     "start_time": "2020-10-01T05:26:13.064Z"
    }
   },
   "outputs": [],
   "source": [
    "hypot.py_func(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "\n",
    "An important part of using Numba is measuring the performance of your new code.  Let's see if we actually sped anything up.  The easiest way to do this in the Jupyter notebook is to use the `%timeit` magic function.  Let's first measure the speed of the original Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.019399Z",
     "start_time": "2020-10-01T05:26:13.069Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit hypot.py_func(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%timeit` magic runs the statement many times to get an accurate estimate of the run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.020700Z",
     "start_time": "2020-10-01T05:26:13.074Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numba did a pretty good job with this function.  It's 3x faster than the pure Python version.\n",
    "\n",
    "Of course, the `hypot` function is already present in the Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.022096Z",
     "start_time": "2020-10-01T05:26:13.079Z"
    }
   },
   "outputs": [],
   "source": [
    "%timeit math.hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's built-in is even faster than Numba!  This is because Numba does introduce some overhead to each function call that is larger than the function call overhead of Python itself.  Extremely fast functions (like the above one) will be hurt by this.\n",
    "\n",
    "(However, if you call one Numba function from another one, there is very little function overhead, sometimes even zero if the compiler inlines the function into the other one.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Numba work?\n",
    "\n",
    "The first time we called our Numba-wrapped `hypot` function, the following process was initiated:\n",
    "\n",
    "![Numba Flowchart](img/numba_flowchart.png \"The compilation process\")\n",
    "\n",
    "We can see the result of type inference by using the `.inspect_types()` method, which prints an annotated version of the source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.023507Z",
     "start_time": "2020-10-01T05:26:13.085Z"
    }
   },
   "outputs": [],
   "source": [
    "hypot.inspect_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Numba's type names tend to mirror the NumPy type names, so a Python `float` is a `float64` (also called \"double precision\" in other languages).  Taking a look at the data types can sometimes be important in GPU code because the performance of `float32` and `float64` computations will be very different on CUDA devices.  An accidental upcast can dramatically slow down a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Things Go Wrong\n",
    "\n",
    "Numba cannot compile all Python code.  Some functions don't have a Numba-translation, and some kinds of Python types can't be efficiently compiled at all (yet).  For example, Numba does not support `FrozenSet` (as of this tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.024790Z",
     "start_time": "2020-10-01T05:26:13.091Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def cannot_compile(x):\n",
    "    return \"a\" in x\n",
    "\n",
    "\n",
    "cannot_compile(frozenset((\"a\", \"b\", \"c\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what happened??  By default, Numba will fall back to a mode, called \"object mode,\" which does not do type-specialization.  Object mode exists to enable other Numba functionality, but in many cases, you want Numba to tell you if type inference fails.  You can force \"nopython mode\" (the other compilation mode) by passing arguments to the decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.026194Z",
     "start_time": "2020-10-01T05:26:13.096Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def cannot_compile(x):\n",
    "    return \"a\" in x\n",
    "\n",
    "try:\n",
    "    cannot_compile(frozenset((\"a\", \"b\", \"c\")))\n",
    "except Exception as e:\n",
    "    logging.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get an exception when Numba tries to compile the function, with an error that says:\n",
    "```\n",
    "- argument 0: cannot determine Numba type of <class 'frozenset'>\n",
    "```\n",
    "which is the underlying problem. Numba doesn't know about frozenset. There are classes that we use regularly in our code but they might not be defined in Numba. An example of a common class that you cannot use in Numba is pandas data frames. <br>Now the question is: what does Numba support? Some of the types/classes that are supported by Numba are listed below:\n",
    "* Numbers (integers, floats, etc)\n",
    "* Numpy arrays\n",
    "* Strings\n",
    "* Lists and tuples (note that a list/tuple of numbers or strings is supported but a list of lists is not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we want the last example to be compiled successfully by Numba jit, we need to use a tuple or a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.027822Z",
     "start_time": "2020-10-01T05:26:13.102Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def can_compile(x):\n",
    "    return \"a\" in x\n",
    "\n",
    "\n",
    "can_compile((\"a\", \"b\", \"c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "GregoryâLeibniz infinite series converges to $\\pi$:\n",
    "$$\\pi = \\frac{4}{1} - \\frac{4}{3} + \\frac{4}{5} - \\frac{4}{7} + \\frac{4}{9} - \\frac{4}{11} + \\frac{4}{13} - \\cdots$$\n",
    "\n",
    "Write a Numba function which calculates the sum of first $n$ terms in this series. Then test its speed agains normal Python function for $ n = 1000000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T05:26:24.029240Z",
     "start_time": "2020-10-01T05:26:13.106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```Python\n",
    "    @jit\n",
    "    def gl_pi(n):\n",
    "        pi = 0\n",
    "        for i in range(n):\n",
    "            if i%2 ==0:\n",
    "                pi += 4/(2*i+1)\n",
    "            else:\n",
    "                pi -= 4/(2*i+1)\n",
    "        return pi \n",
    "```\n",
    "\n",
    "<b>Numba function speed test:</b>\n",
    "```Python\n",
    "    %timeit gl_pi(1000000) \n",
    "```\n",
    "    \n",
    "<b>Normal Python function speed test:</b>\n",
    "```Python\n",
    "    %timeit gl_pi.py_func(1000000) \n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "The following sources where used for creation of this notebook:\n",
    "- https://github.com/NCAR/ncar-python-tutorial\n",
    "- https://github.com/stevesimmons/pydata-ams2017-pandas-and-dask-from-the-inside\n",
    "- https://github.com/numba/euroscipy2019-numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "- [Dask documentation](https://docs.dask.org/en/latest/)\n",
    "- [Why Dask?](https://docs.dask.org/en/latest/why.html)\n",
    "- [Distributed Machine Learning with Python and Dask](https://towardsdatascience.com/distributed-machine-learning-with-python-and-dask-2d6bae91a726)\n",
    "- [Speeding up your Algorithms â Dask](https://towardsdatascience.com/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef)\n",
    "- [Speeding Up your Algorithms â Numba](https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
