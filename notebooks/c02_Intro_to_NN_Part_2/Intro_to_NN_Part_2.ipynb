{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some libraries first\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction to Pytorch \n",
    "There are different architectures for Neural Networks (NNs). Those architectures are defined by blocks called layers. In this notebook we will learn how to use common layers to build a neural networks from scratch. We will also learn how to train the neural network using the `Pytorch` library and evaluate its performance.\n",
    "\n",
    "In Pytorch, we can organise those layers using containers. The most common one that we will use is the `torch.nn.Sequential` layer.\n",
    "\n",
    "In a sequential container, the modules (including activation functions and layers) will be added to it in the order they are passed. Alternatively, an ordered dictionary of modules can also be passed in.\n",
    "\n",
    "To make it easier to understand, here is a small example:\n",
    "\n",
    "Source: [Pytorch Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "\n",
    "```python\n",
    "\n",
    "# Import OrderedDict, which is in the standard collection library for python\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import nn to use basic building blocks\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example of using Sequential\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5), # This is a Convolutional Layer \n",
    "          nn.ReLU(),         # This is an activation function\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "# Example of using Sequential with OrderedDict\n",
    "# This is equivalent to the first model\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "```\n",
    "\n",
    "As you can notice from the examples, `nn` contains the basic building blocks for Neural Networks as well as activiation functions, loss functions and some other useful functions.\n",
    "\n",
    "## 1. Layers in Pytorch\n",
    "\n",
    "\n",
    "### 1.2 Linear (or Dense) Layer\n",
    "\n",
    "Fully connected neural networks (FCNNs) are a type of artificial neural network where  all the neurones, in one layer are connected to the neurones in the next layer. This type of layer is defined in Pytorch by [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear).\n",
    "\n",
    "### 1.3 Convolutional Neural Networks\n",
    "\n",
    "The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels). During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\n",
    "\n",
    "<h4>Architecture in a CNN:</h4>\n",
    "\n",
    "A convolutional neural network consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of convolutional layers that convolve with a multiplication or other dot product. The activation function is commonly a RELU layer, and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final convolution.\n",
    "\n",
    "<h4> CNN Layer: </h4>\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "\n",
    "**NOTE:** <br>\n",
    "Use `nn.Conv2d` in Pytorch for 2d Convolutions.\n",
    "</div>\n",
    "\n",
    "A convolutional layer within a neural network should have the following attributes:\n",
    "- Convolutional kernels defined by a width and height (hyper-parameters).\n",
    "- The number of input channels and output channels (hyper-parameter).\n",
    "- The depth of the Convolution filter (the input channels) must be equal to the number channels (depth) of the input feature map. <br>\n",
    "\n",
    "In a CNNs, feature maps are extracted and the they are downsampled until the last layer where it usually have fully connected layer. Look at the examples in the image below:\n",
    "\n",
    "[Source Image](https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Typical_cnn.png)\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png' width=800 height=300>\n",
    "\n",
    "\n",
    "In neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from every element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer.\n",
    "\n",
    "<h4> Pooling Layers</h4>\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "\n",
    "**NOTE:** <br>\n",
    "Use `nn.maxpoo2d` in Pytorch for 2d Max Pooling.\n",
    "</div>\n",
    "\n",
    "Another important concept of CNNs is pooling, which is a form of non-linear down-sampling. The example below, shows Max pooling with a 2x2 filter and stride = 2. In every sub-region, the max value obtained.\n",
    "\n",
    "[Source Image](https://en.wikipedia.org/wiki/Convolutional_neural_network#/media/File:Max_pooling.png)\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png' width=400 heigh=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN implementation in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: https://dataunderground.org/dataset/landmass-f3\n",
    "\n",
    "Credits to researchers at Georgia Tech, Agile Geoscience\n",
    "License CCbySA\n",
    "\n",
    "In this notebook, we will be using the landmass dataset, which have been preprocessed already. In this dataset, we have images of 4 different types of landmass: 'Chaotic Horizon', 'Fault', 'Horizon', 'Salt Dome'.\n",
    "\n",
    "We will train a CNN to learn how to classify images into those 4 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the Patches\n",
    "from deep_ml_curriculum.data.landmass_f3 import LandmassF3Patches\n",
    "from deep_ml_curriculum.config import project_dir\n",
    "\n",
    "landmassf3_train = LandmassF3Patches(\n",
    "    project_dir / \"data/processed/landmass-f3\", train=True\n",
    ")\n",
    "landmassf3_test = LandmassF3Patches(\n",
    "    project_dir / \"data/processed/landmass-f3\", train=False\n",
    ")\n",
    "print(landmassf3_train)\n",
    "print(landmassf3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the dataset\n",
    "print(landmassf3_train.data.shape)\n",
    "print(landmassf3_test.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LandmassF3Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have a total of 13250 gray-scale images of 99x99 pixels.\n",
    "\n",
    "Let's display the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = landmassf3_train[4]\n",
    "print(\"Class:\", landmassf3_train.classes[y])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmassf3_train.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)\n",
    "\n",
    "Now let's implement our first NN from scratch using Pytorch. A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the networkâ€™s parameters\n",
    "- Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a python class with `nn.Module`\n",
    "2. Let's define two functions:\n",
    "   - The first one `__init__` also called contructor. Here we wil define the blocks that we will use.\n",
    "   - Define a `forward` function. In Pytorch, `forward` is a reserved name for a function that takes the input and returns and output. You can define the flow of the architecture here.\n",
    "   \n",
    "   \n",
    "<h3> Define the network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 23 * 23, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your network\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a random 99x99 input. The input image to follow this convention:\n",
    "\n",
    "(N, C, W, H)\n",
    "- N: Number of images in the batch\n",
    "- C: Number of channels. Use 1 for grayscale or 3 for colored images (RGB)\n",
    "- W: Width\n",
    "- H: Height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(1, 1, 99, 99)\n",
    "out = net(input)\n",
    "\n",
    "# An array with 4 output, each one corresponding to\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function for training our ConvNet.\n",
    "\n",
    "But first, we will define some hyperparameters:\n",
    "\n",
    "- `n_epochs`: is the number of iterations over all dataset\n",
    "- `learning_rate`: is the size of the steps in the optimization process.\n",
    "- `momentum`: helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "- `bs`: batch size corresponds to the number of images evaluated at the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some hyperparameter first\n",
    "n_epochs = 3\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "bs = 64\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Optimizer. In this case, we will use Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Let's disable GPU for this example\n",
    "torch.backends.cudnn.enabled = False\n",
    "# For reproducibility\n",
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = landmassf3_train.data / 255.0\n",
    "y_train = landmassf3_train.targets\n",
    "x_test = landmassf3_test.data / 255.0\n",
    "y_test = landmassf3_test.targets\n",
    "n = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the function for training:\n",
    "\n",
    "[Source of code](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, criterion, optimizer, n_epochs=1, bs=64):\n",
    "    # Set model in train mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range((x_train.shape[0] - 1) // bs + 1):\n",
    "            # Let's divide the data in batches\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            inputs = x_train[start_i:end_i].unsqueeze(1).float()\n",
    "            labels = y_train[start_i:end_i].long()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)  # Get the prediction here\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Do backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x, y):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx, image in enumerate(x):\n",
    "        pred = model(image.unsqueeze(0).unsqueeze(0)).argmax()\n",
    "        if int(pred) == int(y[idx]):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    print(correct, total)\n",
    "    accuracy = 100 * (correct / total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer. In this case, we will use Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# Now let's train the model\n",
    "model = train(net, x_train, y_train, criterion, optimizer)\n",
    "test(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again with 1 more epochs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Net()\n",
    "# Define Optimizer. In this case, we will use Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(net2.parameters(), lr=learning_rate, momentum=momentum)\n",
    "model = train(net, x_train, y_train, criterion, optimizer, n_epochs=2)\n",
    "print(\"Testing accuracy on unseen data...\")\n",
    "test(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Net()\n",
    "# Define Optimizer. In this case, we will use Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(net2.parameters(), lr=learning_rate, momentum=momentum)\n",
    "model = train(net, x_train, y_train, criterion, optimizer, n_epochs=5)\n",
    "print(\"Testing accuracy on unseen data...\")\n",
    "test(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the same model using `SGD` for 1, 2, and 5 epochs. At some point, it seems like the model is not converging in it got stucked in a local minima. To improve the results we will tray a couple of things:\n",
    "\n",
    "1. Create a new model with `Batch Normalization`, it is often used in modern CNN architectures because it helps to create more general models (regularization) preventing overfitting.\n",
    "2. Change `SGD` for `Adam` optimizer. `Adam` is known to converge faster than `SGD`.\n",
    "3. We will train longer (more epochs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BetterCNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 10 * 10, out_features=512)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # print(out.shape)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see first the results training the new model using only 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = BetterCNN()\n",
    "optimizer = torch.optim.Adam(convnet.parameters(), lr=learning_rate)\n",
    "model = train(convnet, x_train, y_train, criterion, optimizer)\n",
    "test(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b>Exercise 1</b>: <br>\n",
    "\n",
    "Modify the previous code to train `BetterCNN` using `Adam` optimizer for a total of 10 `epochs`. Use 1e-3 `learning_rate`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can click in the button below the reveal the solution for exercise 1\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"4\" color=\"darkblue\"><b>See the solution for Exercise 1</b></font>\n",
    "</summary>\n",
    "    \n",
    "If we check the loss, we can notice that Adam is converging faster. However, the model is clearly underfitted. Let's train now the model for 10 epochs more:\n",
    "    \n",
    "```python\n",
    "learning_rate = 1e-3\n",
    "convnet2 = BetterCNN()\n",
    "optimizer = torch.optim.Adam(convnet2.parameters(), lr=learning_rate)\n",
    "model = train(convnet2, x_train, y_train, criterion, optimizer, n_epochs=10)\n",
    "test(model, x_test, y_test)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally ! After changing the optimizer, creating a better CNN architecture and train for a couple of epochs we got an accuracy of over 99% on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we finished the training let's save our best model\n",
    "PATH = \"./landmass_net.pth\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load a new model to check that the performance of the saved model.\n",
    "\n",
    "Check more information about how to save models in Pytorch [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and further reading\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "[Pytorch](https://pytorch.org/)\n",
    "\n",
    "[Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "[Adam](https://arxiv.org/pdf/1412.6980.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
