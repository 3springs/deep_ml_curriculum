{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "## What are autoencoders?\n",
    "Autoencoders are networks which have the same input and output. A set of data is fed to these networks and they are expected to recreate the input. However, what makes autoencoders interesting is that they compress the information into lower number of dimensions (a.k.a latent space) and then recreate the input using those dimensions. They can be used for dimensionality reduction similar to PCA, t-SNE, and Umap. Some of the advantages of using autoencoders compared to some of the other techniques are:\n",
    "- Flexibility: You can design the network based on what the problem demands.\n",
    "- Reversibility: Unlike methods such as t-SNE and UMAP you can convert data back to the initial space.\n",
    "- Non-linearity: Unlike linear methods such as PCA, it is capable of using non-linear transformation.\n",
    "\n",
    "## Structure\n",
    "Autoencoders have two main components:\n",
    "1. Encoder: Converts data to latent space.\n",
    "2. Decoder: Converts the data back from latent space to its initial space.\n",
    "\n",
    "The architecture looks similar to the image below:\n",
    "<img src='./images/nn.svg' style='height:50rem'>\n",
    "\n",
    "We pass the input through the model and it will compress and decompress the input and returns a result. Then we compare the output of the model with the original input. To check how close the output is to the original input we use a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:14.265396Z",
     "start_time": "2020-10-10T11:09:12.925882Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "We are going to start with a simple problem. We will use MNIST dataset which is a collection of hand-written digits as 28x28 pixel images. We are going to use autoencoder to compress each image into only two values and then reconstruct the image. When the model is trained we will have a look at the reconstructed images as well as latent space values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a `Dataset` class. The `Dataset` class reads the data from file and returns data points when we need them. The advantage of using a `Dataset` is that we can adjust it based on what we need for each problem. If we are not dealing with large amount of data we can decide to keep everything in RAM so it is ready use. But if we are dealing with a few gigabytes of data we might need to open the file only when we need them.<br>\n",
    "The MNIST data set is not large so we can easily fit it into memory. In the `Dataset` class we define a few methods:\n",
    "- `__init__`: What information is required to create the object and how this information is saved.\n",
    "- `__len__`: Returns the number of data points (images) when we use `len()` function.\n",
    "- `__getitem__`: We can define how indexing would work for this class.\n",
    "\n",
    "We are going to define a couple of custom functions for convinience:\n",
    "- `show`: to see the image.\n",
    "- `sample`: which returns a random sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:14.278389Z",
     "start_time": "2020-10-10T11:09:14.267745Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path(\"../../data/processed/MNIST/\")\n",
    "\n",
    "\n",
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "\n",
    "        self.root_dir = Path(path)\n",
    "        self.transform = transform\n",
    "        data = pd.read_csv(path)\n",
    "        if \"label\" in data.columns:\n",
    "            self.x = data.drop(columns=[\"label\"]).values\n",
    "            self.y = data[\"label\"].values\n",
    "        else:\n",
    "            self.x = data.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        output = self.x[idx] / 255\n",
    "        if self.transform:\n",
    "            output = self.transform(output)\n",
    "        return output\n",
    "\n",
    "    def show(self, idx):\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(self.x[idx].reshape((28, 28)), \"gray\")\n",
    "\n",
    "    def sample(self, n):\n",
    "        idx = np.random.randint(0, len(self), n)\n",
    "        return self[idx]\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, data):\n",
    "        return torch.FloatTensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ We also defined a class called `ToTensor`. This class takes an input and converts it to pytorch tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a `Dataset` class, we can create a training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:20.612021Z",
     "start_time": "2020-10-10T11:09:14.281258Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = DigitsDataset(path / \"train.csv\", transform=ToTensor())\n",
    "ds_test = DigitsDataset(path / \"test.csv\", transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create a data loaders. The training process takes place at multiple steps. At each step, we choose a few images and feed them to the model. Then we calculate the loss value based on the output. Using the loss value we update the values in the model. We do this over and over until when we think the model is trained. Each of these steps are called a mini-batch and the number of images passed in at each mini-batch is called batch size. Dataloader's job is to go to the dataset and grab a mini-batch of images for training. To create a Dataloader we use a pytorch dataloder object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:20.618251Z",
     "start_time": "2020-10-10T11:09:20.614644Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Shuffle tells the data loader whether the data needs to be shuffled at the end of each epoch. We do it for training to keep the input random. But we don't need to do it for testing since we only use the test dataset for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the model. The architecture we are going to use here is made of two linear layers for the encoder and two linear layers for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:20.631335Z",
     "start_time": "2020-10-10T11:09:20.620667Z"
    }
   },
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc2 = nn.Linear(400, 2)\n",
    "        self.fc3 = nn.Linear(2, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc2(h1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x.view(-1, 784))\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have access to GPU, let's make sure we are using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:20.668879Z",
     "start_time": "2020-10-10T11:09:20.633123Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:23.453096Z",
     "start_time": "2020-10-10T11:09:20.671358Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AE().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to choose an optimiser. The optimiser use the loss value and it's gradients with respect to model parameters and tells us how much each value must be adjusted to have a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:23.459775Z",
     "start_time": "2020-10-10T11:09:23.456455Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final component is the loss function. Here we are going to use Binary Cross Entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:23.469676Z",
     "start_time": "2020-10-10T11:09:23.464452Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_bce(recon_x, x):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
    "\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define two functions one for executing a single epoch of training and one for evaluating the mdel using test data.<br>\n",
    "Notice the following steps in the training loop:\n",
    "1. We make sure the data is in the right device (cpu or gpu)\n",
    "2. We make sure that any saved gradient (derivative) is zeroed.\n",
    "3. We pass a mini-batch of data into the model and grab the predictions.\n",
    "4. We use the loss function to find out how close the model's output is to the actual image.\n",
    "5. We use `loss.backward()` to claculate the derivative of loss with respect to model parameters.\n",
    "6. We ask the optimiser to update model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:09:23.489159Z",
     "start_time": "2020-10-10T11:09:23.472529Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, loss_function, log_interval=50):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in tqdm(enumerate(train_loader), leave=False):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch = model(data)\n",
    "        loss = loss_function(recon_batch, data)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            pct = 100.0 * batch_idx / len(train_loader)\n",
    "            l = loss.item() / len(data)\n",
    "            print(\n",
    "                '#{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}  '.format(epoch, batch_idx * len(data), len(train_loader.dataset), pct, l),\n",
    "                end=\"\\r\",\n",
    "                flush=True,\n",
    "            )\n",
    "    print('#{} Train loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch, loss_function, log_interval=50):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(test_loader), leave=False):\n",
    "            data = data.to(device)\n",
    "            recon_batch = model(data)\n",
    "            test_loss += loss_function(recon_batch, data).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('#{} Test loss: {:.4f}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the components are ready, let's train the model for $10$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:12.998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 Train loss: 178.6645Loss: 162.429794  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 Test loss: 161.9127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#2 Train loss: 158.7694Loss: 155.210556  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#2 Test loss: 155.9821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#3 Train loss: 154.5413Loss: 153.815247  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#3 Test loss: 152.8883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#4 Train loss: 151.8288Loss: 163.013519  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#4 Test loss: 150.7022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#5 Train loss: 149.9034Loss: 155.868347  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#5 Test loss: 149.0420\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fe26bff7d14373adb3c58a9d43bf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#6 [32000/42000 (76%)]\tLoss: 152.886337  \r"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch, loss_bce)\n",
    "    test(epoch, loss_bce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Now let's check out the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.004Z"
    }
   },
   "outputs": [],
   "source": [
    "def cvt2image(tensor):\n",
    "    return tensor.detach().cpu().numpy().reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.009Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(ds_test))\n",
    "\n",
    "model.eval()\n",
    "original = ds_train[[idx]]\n",
    "result = model(original.to(device))\n",
    "img = cvt2image(result[0])\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(img, \"gray\")\n",
    "plt.title(\"Predicted\")\n",
    "ds_train.show(idx)\n",
    "plt.title(\"Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue' size='4rem'>Run the cell above a few times and compare the predicted and actual images.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are certainly some similarities but the predicted (reconstructed) images are not always very clear. We will shortly discuss how we can improve the model. But before that, let's have look at the latent space. The model is converting every image which has 784 values (28x28 pixels) to only 2 values. We can plot these two values for a few numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.015Z"
    }
   },
   "outputs": [],
   "source": [
    "res = model.encode(ds_train[:1000].to(device))\n",
    "res = res.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.019Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    idx = ds_train.y[:1000] == i\n",
    "    plt.scatter(res[idx, 0], res[idx, 1], label=i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each color represents a number. Despite most numbers overlapping, we can still see some distictions, for instance between $1$ and other numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "Obviously the model that we trained needs improvement as it is no recreating the images well enough. There are a few ways we can improve the model. One way is to create a deeper encoder and decoder. In the example above we ued only two layers for encoder and layers for decoder. This doesn't allow the model to comprehend complex relationships, especially in this scenario since we are working with images. By adding more layers we can give the model the opportunity to better differentiate between digits.\n",
    "Another way of making the model is using more dimensions in latent space. For instance, if instead of compressing each image into two values we could use ten values. This will allow the model to extract more features from the input which will make reconstructing the image easier. However, it must be noted that whole point of using autoencoder is to force the model to compress the information into as few dimensions as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "Autoencoders are not only useful for dimensionality reduction. They are often used for other purposes as well, including:\n",
    "1. __Denoising:__ We could add noise to the input and then feed it to the model and then compare the output with the original image (without noise). This approach will create a model which is capable of removing noise from the input.\n",
    "2. __Anomaly Detection:__ When we train a model on specific set of data, the model learns how to recreate the dataset. As a result when there are uncommon instances in the data the model will not be able to recrate them very well. This behaviour is sometimes used as a technique to find anomalous data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "Variational Autoencoders (VAE) are one of the variations of autoencoders. Unlike normal autoencoders which compress the data into a few values, VAEs tries to find the distribution of the data in latent space. As a result, the final model not only has the ability to recreate the input, but can also generate new outputs by sampling from the latent space distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since VAE is a variation of autoencoder, it has a similar architecture. The main difference between the two is an additional layer between encoder and decoder which samples from latent space distribution.\n",
    "In a VAE, the encoder generates two values for each parameter in latent space. One represent the mean and one represents the standard deviation of the parameter. Then sampling layer uses these two numbers and generates random values from the same distribution. These values then are fed to decoder which will create an output similar to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a VAE model. We will use layers with the same size as the previous model. Notice for the second layer we have two linear layers, one to generate the mean and one to generate the log of variance which will be converted into standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.029Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 2)\n",
    "        self.fc22 = nn.Linear(400, 2)\n",
    "        self.fc3 = nn.Linear(2, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.036Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is similar to what we used before, except we have an extra part. the extra equation is Kullback–Leibler divergence which measures difference between probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.042Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_bce_kld(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to slightly adjust the training loop since the loss function now takes four inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.047Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, loss_function, log_interval=50):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            pct = 100.0 * batch_idx / len(train_loader)\n",
    "            l = loss.item() / len(data)\n",
    "            print(\n",
    "                '#{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}  '.format(epoch, batch_idx * len(data), len(train_loader.dataset), pct, l),\n",
    "                end=\"\\r\",\n",
    "                flush=True,\n",
    "            )\n",
    "    print('#{} Train loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch, loss_function, log_interval=50):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('#{} Test loss: {:.4f}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.053Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train(epoch, loss_bce_kld)\n",
    "    test(epoch, loss_bce_kld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.058Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.062Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"VAE.pk\", \"wb\") as fp:\n",
    "    pickle.dump(model.state_dict(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.067Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.071Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "with open(\"VAE.pk\", \"rb\") as fp:\n",
    "    model.load_state_dict(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.077Z"
    }
   },
   "outputs": [],
   "source": [
    "def cvt2image(tensor):\n",
    "    return tensor.detach().numpy().reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.082Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(ds_test))\n",
    "\n",
    "model.eval()\n",
    "original = ds_train[[idx]]\n",
    "result = model(original)\n",
    "img = cvt2image(result[0])\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(img, \"gray\")\n",
    "plt.title(\"Predicted\")\n",
    "ds_train.show(idx)\n",
    "plt.title(\"Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the predicted mean of both parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.087Z"
    }
   },
   "outputs": [],
   "source": [
    "mu, logvar = model.encode(ds_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.090Z"
    }
   },
   "outputs": [],
   "source": [
    "mu = mu.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.094Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    idx = ds_train.y[:1000] == i\n",
    "    plt.scatter(mu[idx, 0], mu[idx, 1], label=i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare this plot with the similar plot for normal autoencoder, we can see that VAE did a better job at creating clusters. The points for each digits are closer together compared to previous model. However, there is still room for improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Create a new VAE but this time use a deeper network. Note, everything else (loss function, dataloaders, training loops, etc.) will stay the same only the model will change. The example above was using these sizes: 784 --> 400 --> 2 --> 400 --> 784\n",
    "<br>Try a new model which uses these size: 784 --> 400 --> 80 --> 2 --> 80 --> 400 --> 784 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert Training loop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder\n",
    "So far we only used __Linear layers__ (also called Dense layers) in our network. The problem with using linear layers is that don't understand the spatial relationship between pixels. For a linear layer all the inputs are the same. But since we know that there is a closer relationship between the pixels located near each other, using a convolutional layer is more appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to apply a Convolutional Variational Autoencoder on 2D DeepRock dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.113Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path(\n",
    "    \"../../data/processed/deep-rock-sr/DeepRockSR-2D/coal2D/coal2D_train_LR_default_X4/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.120Z"
    }
   },
   "outputs": [],
   "source": [
    "img_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have look at a random image from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.126Z"
    }
   },
   "outputs": [],
   "source": [
    "rand_file = np.random.choice(img_list)\n",
    "img = Image.open(path / rand_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.132Z"
    }
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.138Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model. Same as before the model has three main sections:\n",
    "1. __Encoder:__ After each layer in the encoder we decrease the size of output and increase the number of channels. \n",
    "2. __Decoder:__ It acts as opposit of encoder. At each layer we increase the size of output and decrease the number of channels.\n",
    "3. __Reparametrisation (Sampler):__ Gets a mean and log of variance of the distribution and generates a new datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.144Z"
    }
   },
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, n_latent=32):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.fc11 = nn.Linear(1024, n_latent)\n",
    "        self.fc12 = nn.Linear(1024, n_latent)\n",
    "        self.fc2 = nn.Linear(n_latent, 1024)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.randn_like(mu)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "\n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc11(h), self.fc12(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc2(z)\n",
    "        z = z.view(z.size(0), 1024, 1, 1)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a new dataset and dataloader object for this dataset. Note that in this case we are not loading all the data in the memory, but at each mini-batch we only open the images that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.154Z"
    }
   },
   "outputs": [],
   "source": [
    "class RocksDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.image_paths[idx]\n",
    "        image = Image.open(img_name).resize((64, 64))\n",
    "        image = np.array(image) / 255\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image):\n",
    "        return torch.FloatTensor(image).permute((2, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.160Z"
    }
   },
   "outputs": [],
   "source": [
    "image_list = np.array([i for i in path.iterdir()])\n",
    "is_train = np.random.binomial(1, 0.8, len(image_list))\n",
    "list_train = image_list[is_train == 1]\n",
    "list_test = image_list[is_train == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset and dataloader for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.167Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 32\n",
    "ds_train = RocksDataset(list_train, transform=ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(ds_train, bs, shuffle=True)\n",
    "ds_test = RocksDataset(list_test, transform=ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(ds_test, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.172Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_bce_kld(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the model and choose the optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.179Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CVAE()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(lr=0.001, params=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training and test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.186Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, loss_function, log_interval=20):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            pct = 100.0 * batch_idx / len(train_loader)\n",
    "            l = loss.item() / len(data)\n",
    "            print(\n",
    "                '#{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}  '.format(epoch, batch_idx * len(data), len(train_loader.dataset), pct, l),\n",
    "                end=\"\\r\",\n",
    "                flush=True,\n",
    "            )\n",
    "    print('#{} Train loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch, loss_function, log_interval=20):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('#{} Test loss: {:.4f}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "And finally train the model. Since we are training a larger network from scratch we need to let the model train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.193Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>Training can take a long time. Each epoch takes 4 seconds on RTX 2070. Depending on your GPU it might take shorter or longer. Feel free to adjust the number of epochs accordingly.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    t = perf_counter()\n",
    "    train(epoch, loss_bce_kld)\n",
    "    test(epoch, loss_bce_kld)\n",
    "    print('Epoch#{} took {:.1f} seconds'.format(epoch, perf_counter() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "When the model is trained we can have a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.204Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_result(sample):\n",
    "    sample = sample.unsqueeze(0).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output, _, _ = model(sample)  # we only grab the reconstructed image\n",
    "    loss = F.binary_cross_entropy(output, sample, reduction=\"sum\")\n",
    "    print('Loss: {:.2f}'.format(loss.item()))\n",
    "\n",
    "    # return the data to cpu for visualisation\n",
    "    output = output.squeeze(0).permute(1, 2, 0).to(\"cpu\")\n",
    "    output = output.detach().numpy()\n",
    "    sample = sample.squeeze(0).permute(1, 2, 0).to(\"cpu\")\n",
    "    sample = sample.numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(sample)\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output)\n",
    "    plt.title(\"Reconstructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.208Z"
    }
   },
   "outputs": [],
   "source": [
    "# grab an image from test set using the index\n",
    "idx = 0\n",
    "sample = ds_test[idx]\n",
    "show_result(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play with learning rate, batch size, number of variabes in latent space and see if it improves the model.\n",
    "You can also change the size of the input images. However, since the model is designed for the images of size 64 by 64, if you want to use it for other sizes of image, you need to adjust the model as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with Unet\n",
    "Semantic segmentation is very similar to image classification. In image classification we give an image to the model and we expect the model to tell us what category the image belongs to. In Semantic segmentation we ask the model to classify the image, but on pixel level. We expect the model to classify every pixel of the image and tell us whether it belongs to one of the categories or not.<br>\n",
    "Let's say we have the image below and we want to segment humans in the image:<br>\n",
    "<img src='./images/tennis.png' width='50%'>\n",
    "\n",
    "A segmentation model looks at this image and returns a binary image (also called mask) which tells us which pixels in the image above belong to a human.<br>\n",
    "<img src='./images/mask.png' width='50%'>\n",
    "\n",
    "One of the most common architectures for image segmentation is called __U-net__. U-net is very similar to an autoencoder. It has an encoder and a decoder. But what makes U-net so powerful is its skip-connections. In a U-net the middle layers of the encoder is connected to he middle layers of the decoder. This allows the model to remember the details of the original image (using skip connections) and at the same time get a good understanding of the content of the image (using encoder and decoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model\n",
    "We are not going to write a U-net model from scratch like we did with autoencoders. Instead we use the pretrained models from `torch.hub`. This way, we can use the models that are already trained for this task (even though they might be trained on different datasets). If the model is trained on a similar data we can use it out of the box, if not we can retrain it using out own datasets. This methods is usually more effective than training a model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.215Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.hub\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we are using a subset of COCO dataset. This subset has various photos and the goal is to find people in the photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply look at [pytorch hub](https://pytorch.org/hub/) and download the model we need. The code below downloads [DEEPLABV3-RESNET101](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/) which suits the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.221Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"../../data/processed/models/Deeplabv3.pth\")\n",
    "# try:\n",
    "#     model = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "# except Exception as e:\n",
    "#     # The model is also saved in the following path (torch==1.4.0) and we can load it directly.\n",
    "#     model = torch.load(\"../../data/processed/models/Deeplabv3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T07:31:15.917903Z",
     "start_time": "2020-10-05T07:31:15.707865Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "We can test the model on our data. While our data is selected for its human subjects, the model is trained on 20 different categories. Therefore, the output of the model may include subjects other than humans as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.228Z"
    }
   },
   "outputs": [],
   "source": [
    "imgpath = \"../../data/processed/COCO_sample/samplecoco2014\"\n",
    "maskpath = \"../../data/processed/COCO_sample/masks2014/\"\n",
    "filename = np.random.choice(os.listdir(imgpath))\n",
    "maskname = filename.replace(\"jpg\", \"png\")\n",
    "input_image = Image.open(os.path.join(imgpath, filename))\n",
    "mask = Image.open(os.path.join(maskpath, maskname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.232Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_batch = input_batch.to(device)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)[\"out\"][0]\n",
    "output_predictions = output.argmax(0).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.236Z"
    }
   },
   "outputs": [],
   "source": [
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.240Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 15))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(input_image)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title(\"Prediction\")\n",
    "plt.imshow(output_predictions.numpy())\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "But what if the target in our dataset was different from the pretrained model? In that case the model won't be able to make a good prediction as it doesn't know our target object. But this doesn't mean that the pretrained model will be entirely useless. We can still use it, but it needs to be refined. Also the pretrained model is designed for 20 categories. But we might have smaller or larger number of categories.<br>\n",
    "Either way we need to train the model on our dataset. And the steps are similar to before:\n",
    "1. Create a dataset\n",
    "2. Create a data loader\n",
    "3. Create the model\n",
    "4. Select the optimiser\n",
    "5. train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.245Z"
    }
   },
   "outputs": [],
   "source": [
    "class HumanDataset(Dataset):\n",
    "    def __init__(self, path, subdirs, filenames, transform=None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(path, subdirs[0], f) for f in filenames]\n",
    "        self.mask_paths = [\n",
    "            os.path.join(path, subdirs[1], f.replace(\"jpg\", \"png\")) for f in filenames\n",
    "        ]\n",
    "        self.normalizer = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = Image.open(self.image_paths[idx]).resize((128, 128))\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image) / 255\n",
    "\n",
    "        mask = Image.open(self.mask_paths[idx]).resize((128, 128))\n",
    "        mask = np.array(mask) / 255\n",
    "        if self.transform:\n",
    "\n",
    "            image = self.transform(image).permute((2, 0, 1))\n",
    "            image = self.normalizer(image)\n",
    "            mask = self.transform(mask).unsqueeze(0)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image):\n",
    "        return torch.FloatTensor(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.250Z"
    }
   },
   "outputs": [],
   "source": [
    "tsfm = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "filenames = os.listdir(\"../../data/processed/COCO_sample/samplecoco2014/\")\n",
    "ds = HumanDataset(\n",
    "    path=\"../../data/processed/COCO_sample/\",\n",
    "    subdirs=[\"samplecoco2014\", \"masks2014\"],\n",
    "    filenames=filenames,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.255Z"
    }
   },
   "outputs": [],
   "source": [
    "## TODO mike put these offline\n",
    "# torchvision.models.segmentation.segmentation.model_urls = {'fcn_resnet50_coco': None,\n",
    "#  'fcn_resnet101_coco': 'https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth',\n",
    "#  'deeplabv3_resnet50_coco': None,\n",
    "#  'deeplabv3_resnet101_coco': 'https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.261Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision import models\n",
    "\n",
    "def DeepLabv3(outputchannels=1):\n",
    "    model = models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
    "    model.classifier = nn.Sequential(DeepLabHead(2048, outputchannels), nn.Sigmoid())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:03:11.319198Z",
     "start_time": "2020-10-10T11:03:11.313983Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T10:49:14.928537Z",
     "start_time": "2020-10-10T10:49:14.920876Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'><b>When creating an instance of model access to internet is required for downloading the pretrained model. If you don't have access to internet you won't be able to run the rest of the code.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.276Z"
    }
   },
   "outputs": [],
   "source": [
    "# monkey patch to work offline\n",
    "model_path = Path('../../data/processed/models/deeplabv3_resnet101_coco-586e9e4e.pth').absolute().resolve()\n",
    "torchvision.models.segmentation.segmentation.model_urls['deeplabv3_resnet101_coco'] = 'file://{}'.format(model_path)\n",
    "model_path = Path('../../data/processed/models/resnet101-5d3b4d8f.pth').absolute().resolve()\n",
    "torchvision.models.resnet.model_urls['resnet101'] = 'file://{}'.format(model_path)\n",
    "\n",
    "model = DeepLabv3()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:05:55.065823Z",
     "start_time": "2020-10-10T11:05:55.060440Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:04:32.372129Z",
     "start_time": "2020-10-10T11:04:32.328907Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T11:02:55.944054Z",
     "start_time": "2020-10-10T11:02:55.083971Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.294Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.300Z"
    }
   },
   "outputs": [],
   "source": [
    "dl.dataset[40][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T11:09:13.306Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    for data, target in tqdm(dl):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = F.sigmoid(model(data)[\"out\"])\n",
    "        loss = F.binary_cross_entropy(output, target, reduction=\"sum\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        print('loss: {:.2f}   '.format(loss.item()), end=\"\\r\", flush=True)\n",
    "    print('Training Loss epoch#{}: {:.2f}'.format(epoch + 1, np.mean(train_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T07:44:12.217433Z",
     "start_time": "2020-10-05T07:42:32.982512Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is trained you can test it on the data similar to the [example](#Inference) we saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "```Python\n",
    "class VAE2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc2 = nn.Linear(400, 80)\n",
    "        self.fc31 = nn.Linear(80, 2)\n",
    "        self.fc32 = nn.Linear(80, 2)\n",
    "        self.fc4 = nn.Linear(2, 80)\n",
    "        self.fc5 = nn.Linear(80, 400)\n",
    "        self.fc6 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    \n",
    "model = VAE2().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch,loss_bce_kld)\n",
    "    test(epoch,loss_bce_kld)\n",
    "\n",
    "# visualisations\n",
    "\n",
    "model.eval()\n",
    "mu , logvar = model.encode(ds_train[:1000])\n",
    "mu = mu.detach().numpy()\n",
    "for i in range(10):\n",
    "    idx = ds_train.y[:1000]==i\n",
    "    plt.scatter(mu[idx,0],mu[idx,1],label = i)\n",
    "plt.legend()\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Pytorch examples for VAE](https://github.com/pytorch/examples/tree/master/vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "- [Autoencoders Explained](https://www.youtube.com/watch?v=7mRfwaGGAPg)\n",
    "- [Introduction to Variational Autoencoders](https://www.youtube.com/watch?v=9zKuYvjFFS8&t=527s)\n",
    "- [U-net](https://www.youtube.com/watch?v=81AvQQnpG4Q)\n",
    "- [Understanding Semantic Segmentation](https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47)\n",
    "- [Understanding Variation Autoencoders](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "- [Visualizing MNIST using a variational autoencoder](https://www.kaggle.com/rvislaywade/visualizing-mnist-using-a-variational-autoencoder)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "deep_ml_curriculum",
   "language": "python",
   "name": "deep_ml_curriculum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
