{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:02.746278Z",
     "start_time": "2020-08-08T06:56:02.622891Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will learn about Neural Networks and how to implement them using Scikit-learn. Also, we will start exploring some basic concepts for the Pytorch library.\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "- [0. Video. Neural networks in the world around us](#0)\n",
    "- [1. Deep Nueral Networks](#1)\n",
    "    - [1.1 Backpropagation](#1-1)\n",
    "    - [1.2 Important Concepts](#1-2)\n",
    "- [2. Nueral networks in Scikit-learn](#2)\n",
    "- [3. Introduction to Pytorch](#3)\n",
    "    - [3.1 Tensors](#3-1)\n",
    "    - [3.2 Operations](#3-2)\n",
    "    - [3.3 Converting Numpy arrays and Tensors](#3-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"0\"></a>\n",
    "# 0. Neural Networks in the world around us\n",
    "\n",
    "The following video shows a nicely animated video explaining neural networks in the world around us. This video was provided by the Perth Machine Learning Group. Special thanks to Sean Driver for allowing us using this great material in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:02.758596Z",
     "start_time": "2020-08-08T06:56:02.748662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"600\" src=\"https://www.youtube.com/embed/JxZul0fsw1k\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Youtube Video\n",
    "HTML('<iframe width=\"800\" height=\"600\" src=\"https://www.youtube.com/embed/JxZul0fsw1k\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "# 1. Deep Neural Networks\n",
    "\n",
    "Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\n",
    "\n",
    "An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. The \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\n",
    "\n",
    "In the example below, we can observe a neural network with 3 inputs and 2 outputs. The layers in the middle are called hidden layers. The term 'Deep' from Deep Learning is given for the number of hidden layers in a neural net.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1920px-Colored_neural_network.svg.png' width=300 heigh=300/>\n",
    "\n",
    "Example of a simple neural network with two input units (each with a single input) and one output unit (with two inputs).\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/4/42/A_simple_neural_network_with_two_input_units_and_one_output_unit.png' width=300 heigh=300/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The advantages of DNNs are:\n",
    "\n",
    "- Capability to learn non-linear models.\n",
    "\n",
    "- Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "Some disadvantages of DNNs include:\n",
    "\n",
    "- DNNs with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "\n",
    "- DNNs requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "\n",
    "- DNNs are sensitive to feature scaling.\n",
    "\n",
    "In the upcoming notebooks, we will learn how to overcome and deal with some of those problems.\n",
    "\n",
    "Source: [Scikit-learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mlp-tips), [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "<a name=\"1-1\"></a>\n",
    "## 1.1 Backpropagation\n",
    "\n",
    "\n",
    "> In machine learning, backpropagation, is a widely used algorithm in training feedforward neural networks for supervised learning. Generalizations of backpropagation exist for other artificial neural networks (ANNs)... In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single inputâ€“output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. <b>\n",
    "The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.<b/>\n",
    "    \n",
    "<a name=\"1-2\"></a>    \n",
    "## 1.2 Important Concepts\n",
    "    \n",
    "**Optimizers:** are algorithms that updates the neural network parameters such as the weights to reduce the losses. Popular optimizers are : \n",
    "    \n",
    "- `Gradient Descent`: Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient of the function at the current point.\n",
    "\n",
    "- `Stochastic Gradient Descent`: Stochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof.\n",
    "    \n",
    "- `Adam`: an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\n",
    "Source: [Paper](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "    \n",
    "The methods mentioned before are iterative processes that take steps to advance towards a solution. The step size at each iteration towards the minimum of loss is the **learning rate**.\n",
    "    \n",
    "    \n",
    "**Activation Functions:** Activation functions are important to add non-linearity into the models. This will allow the model to represent more complex data.\n",
    "    \n",
    "Source: [Activation Functions](https://en.wikipedia.org/wiki/Activation_function)\n",
    "    \n",
    "**Regularization** is the process of adding information in order to prevent overfitting.\n",
    "    \n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Regularization_(mathematics)#:~:text=In%20mathematics%2C%20statistics%2C%20finance%20%2C,problem%20or%20to%20prevent%20overfitting.)\n",
    "    \n",
    "    \n",
    "    \n",
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b>Exercise 1</b>: <br>\n",
    "In the next example, you will be able to use the tensorflow playground to test some of the concepts of neural networks.\n",
    "    \n",
    "1. Read the concepts below\n",
    "2. Explore in the iterative playground how changing some hyperparameters will affect the prediction, the number of epochs and the test loss in the output.\n",
    "    - Test different values for `learning rate` and different `activations`.\n",
    "    - Add different number of neurons for the hidden layers and compare results.\n",
    "3. Add 6 hidden layers and perform same tests from step 2. \n",
    "4. Use the `Spiral` data, and for every hidden layer (6 in total) add 8 neurons. Compare results of the test loss with and without regularization `L1` and `L2`.\n",
    "4. Add some noise and perform and repeat steps 2 to 4.\n",
    "    \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:02.765553Z",
     "start_time": "2020-08-08T06:56:02.761389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://playground.tensorflow.org\" width=\"1000\" height=\"800\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe src=\"https://playground.tensorflow.org\" width=\"1000\" height=\"800\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "# 2. Neural Networks in Scikit-learn\n",
    "Source: [MLP](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "> Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset, where $m$ is the number of dimensions for input and $o$ is the number of dimensions for output. Given a set of features $X = {x_1, x_2, ..., x_m}$ and a target $y$, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers.\n",
    "\n",
    "\n",
    "Let's try a classification problem with an MLP model. But first let's import the dataset.\n",
    "For this example, we will be using the digits dataset which contains handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.472061Z",
     "start_time": "2020-08-08T06:56:02.768170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import some libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.678312Z",
     "start_time": "2020-08-08T06:56:03.473937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "# Let's first see one of the images\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "print(f'Target: {digits.target[0]}')\n",
    "plt.imshow(digits.images[0],cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.685921Z",
     "start_time": "2020-08-08T06:56:03.680621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "No. Images: 1797\n",
      "Shape: (8, 8)\n",
      "Max value: 15.0\n",
      "Min value: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(digits.images[0])\n",
    "print('No. Images:', len(digits.images))\n",
    "print('Shape:',digits.images[0].shape)\n",
    "print('Max value:',digits.images[0].max())\n",
    "print('Min value:',digits.images[0].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have a total of 1797 images of 8x8 pixels. The values ranges between 0 and 15. As mentioned before, one of the problems with NNs is that they are very sensitive to feature scaling. To solve this problem, we will scale the images in the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.694018Z",
     "start_time": "2020-08-08T06:56:03.690276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MinMaxScaler with a range of [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.809937Z",
     "start_time": "2020-08-08T06:56:03.696925Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0,1))\n",
    "scaler.fit(digits.images[0])\n",
    "images = [scaler.transform(image) for image in digits.images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the max and min of an image to test it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.815684Z",
     "start_time": "2020-08-08T06:56:03.812254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(images[0].max())\n",
    "print(images[0].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's separate features from the target and split them between train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.822630Z",
     "start_time": "2020-08-08T06:56:03.817579Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.828680Z",
     "start_time": "2020-08-08T06:56:03.825006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently `X` has 3 dimensions. However, `MLPClassifier` only accepts arrays with 1 or 2 dimensions. We will need to reshape `X` to have only 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.836659Z",
     "start_time": "2020-08-08T06:56:03.830710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's flatten one dimension\n",
    "X = np.resize(X, (X.shape[0], 8*8))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.851280Z",
     "start_time": "2020-08-08T06:56:03.838842Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T04:06:12.314139Z",
     "start_time": "2020-08-04T04:06:12.309659Z"
    }
   },
   "source": [
    "Now let's pick some hyperparameters before the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:03.875353Z",
     "start_time": "2020-08-08T06:56:03.856344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate is by default 0.001, we will use the default value\n",
    "def train(X_train, y_train, hidden_layers, activation):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layers,\n",
    "                    activation=activation,\n",
    "                    solver=solver,\n",
    "                    random_state=2020, \n",
    "                    max_iter=100).fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.188232Z",
     "start_time": "2020-08-08T06:56:03.879439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2833333333333333"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers = 2\n",
    "activation = 'relu' # ReLU Activation function\n",
    "solver = 'sgd' # Stochastic Gradient Descent optimizer\n",
    "\n",
    "# Let's train the model using our custom hyperparameters\n",
    "clf = train(X_train, y_train, hidden_layers, activation)\n",
    "# Let's evaluate the accuracy of the model using the test data\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy of 28.3%, which is not really great. Let's try again and train a model changing the number of hidden layers to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.464562Z",
     "start_time": "2020-08-08T06:56:04.190614Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7148148148148148"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers = 10\n",
    "\n",
    "# Let's train the model using our custom hyperparameters\n",
    "clf = train(X_train, y_train, hidden_layers, activation)\n",
    "# Let's evaluate the accuracy of the model using the test data\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T04:37:17.737936Z",
     "start_time": "2020-08-04T04:37:17.733227Z"
    }
   },
   "source": [
    "With 10 hidden layers our model's accuracy improved significantly from 28.3% to 71.48%. Much better than before !\n",
    "\n",
    "The more hidden layers a Neural Networks has, it is say that the model is \"Deeper\". Usually deeper models achieve better accuracy, however they are also more prompt to overfitting and regularization techniques are needed to avoid that. The interest of researchers to create models with more layers is what helped created the term \"Deep Learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T04:39:18.918375Z",
     "start_time": "2020-08-04T04:39:18.913922Z"
    }
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b>Exercise 2</b>: <br>\n",
    "Create a function to test all possible combinations of the hyperparameters below and compare the accuracy to select the best hyperparameters. The process of selecting the best hyperparameters is also called hyperparameter optimisation. We will talk more about it in the coming lessons.\n",
    "\n",
    "`Hidden Layers`: 2,4,6,8,10 <br/>\n",
    "`Activation`: identity, logistic, tanh, relu <br/>\n",
    "`Solver`: lbfgs, sgd, adam <br/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the same model using Pytorch ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "# 3. Introduction to Pytorch\n",
    "\n",
    "PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab (FAIR).\n",
    "\n",
    "A number of pieces of Deep Learning software are built on top of PyTorch, including `Tesla Autopilot`, `Uber's Pyro`, `HuggingFace's Transformers`, `PyTorch Lightning`, and `Catalyst`.\n",
    "\n",
    "\n",
    "According to the [Pytorch's official website](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py) Pytorch is:\n",
    "\n",
    "> Itâ€™s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "\n",
    "We will refer to the equivalent of Numpy arrays as tensors when using Pytorch.\n",
    "\n",
    "Source: [Pytorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-1\"></a>\n",
    "## 3.1 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.821486Z",
     "start_time": "2020-08-08T06:56:04.466727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5806, 0.2241, 0.5542],\n",
      "        [0.8945, 0.1796, 0.0272],\n",
      "        [0.3830, 0.9824, 0.4254],\n",
      "        [0.5304, 0.6381, 0.6681],\n",
      "        [0.1760, 0.0085, 0.8561]])\n"
     ]
    }
   ],
   "source": [
    "# Let's import pytorch library\n",
    "import torch\n",
    "# Let's create a random tensor with size [5,3]\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor directly from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.826915Z",
     "start_time": "2020-08-08T06:56:04.823242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's creste a random tensor with the same size as the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.833294Z",
     "start_time": "2020-08-08T06:56:04.828655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8669, -0.2580])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.838073Z",
     "start_time": "2020-08-08T06:56:04.835180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:22:25.816358Z",
     "start_time": "2020-08-04T05:22:25.813680Z"
    }
   },
   "source": [
    "<a name=\"3-2\"></a>\n",
    "## 3.2 Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take look at the addition operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.846478Z",
     "start_time": "2020-08-08T06:56:04.839913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8669, -0.2580]) tensor([0.1419, 0.1048])\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.rand(2)\n",
    "print(x, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.867546Z",
     "start_time": "2020-08-08T06:56:04.861957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0087, -0.1532])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use a different syntax for addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.878005Z",
     "start_time": "2020-08-08T06:56:04.873521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0087, -0.1532])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.886101Z",
     "start_time": "2020-08-08T06:56:04.880025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1230, -0.0270])\n",
      "tensor([ 0.7250, -0.3628])\n",
      "tensor([ 6.1105, -2.4625])\n"
     ]
    }
   ],
   "source": [
    "print(x * x2)\n",
    "print(x - x2)\n",
    "print(x / x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3-3\"></a>\n",
    "## 3.3 Converting Numpy Arrays and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's convert a Pytorch tensor to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.895350Z",
     "start_time": "2020-08-08T06:56:04.888266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.908093Z",
     "start_time": "2020-08-08T06:56:04.900858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the numpy array changed in value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.918156Z",
     "start_time": "2020-08-08T06:56:04.911597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1) # In-place adding\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T05:35:16.877532Z",
     "start_time": "2020-08-04T05:35:16.873727Z"
    }
   },
   "source": [
    "Now let's try to convert a numpy array to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T06:56:04.927323Z",
     "start_time": "2020-08-08T06:56:04.921201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have learnt the basics about Neural Networks and how to implement one NN for classification using Scikit-learn.\n",
    "We also introduced Pytorch and some basic operations with tensors. In the next lesson, we will dive deeper into the some important and concep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References and further reading\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "\n",
    "[Pytorch](https://pytorch.org/)\n",
    "\n",
    "[NN Playground](https://playground.tensorflow.org/)\n",
    "\n",
    "[Multi-layered perceptron](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "[Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "[Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "[Adam](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
